{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "35d0faef-0ef0-4a1b-a7d8-99f59cd72a09",
   "metadata": {},
   "source": [
    "# ğŸ’¡ è¿™èŠ‚è¯¾ä¼šå¸¦ç»™ä½ \n",
    "\n",
    "1. LlamaIndex çš„ç‰¹ç‚¹å’ŒåŸºæœ¬ç”¨æ³•\n",
    "2. äº†è§£ LlamaIndex å†…ç½®çš„å·¥å…·\n",
    "3. å¦‚ä½•ç”¨å¥½ SDK ç®€åŒ–åŸºäº LLM çš„åº”ç”¨å¼€å‘\n",
    "\n",
    "å¼€å§‹ä¸Šè¯¾ï¼\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2207fa56-c6c4-446a-9576-5949d0e6e881",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## ğŸ“ è¿™èŠ‚è¯¾æ€ä¹ˆå­¦\n",
    "\n",
    "ä»£ç èƒ½åŠ›è¦æ±‚ï¼š**ä¸­é«˜**ï¼ŒAI/æ•°å­¦åŸºç¡€è¦æ±‚ï¼š**æ— **\n",
    "\n",
    "1. æœ‰ç¼–ç¨‹ä¸è½¯ä»¶å·¥ç¨‹åŸºç¡€çš„åŒå­¦\n",
    "   - å…³æ³¨æ¥å£ä¸å®ç°ç»†èŠ‚ã€é«˜çº§æŠ€å·§ã€å¯æ‰©å±•æ€§\n",
    "2. æ²¡æœ‰ç¼–ç¨‹æˆ–è½¯ä»¶å·¥ç¨‹åŸºç¡€çš„åŒå­¦\n",
    "   - å°½é‡ç†è§£ SDK çš„æ¦‚å¿µå’Œä»·å€¼ï¼Œå°è¯•ä½“ä¼šä½¿ç”¨ SDK å‰åçš„å·®åˆ«ä¸æ„ä¹‰\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "194427bf-5807-49d4-ab25-fa0556f835f3",
   "metadata": {},
   "source": [
    "## 1ã€å¤§è¯­è¨€æ¨¡å‹å¼€å‘æ¡†æ¶çš„ä»·å€¼æ˜¯ä»€ä¹ˆï¼Ÿ\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb60e6e5-8f2a-4cd5-a4b5-824259afc229",
   "metadata": {},
   "source": [
    "_SDKï¼šSoftware Development Kitï¼Œå®ƒæ˜¯ä¸€ç»„è½¯ä»¶å·¥å…·å’Œèµ„æºçš„é›†åˆï¼Œæ—¨åœ¨å¸®åŠ©å¼€å‘è€…åˆ›å»ºã€æµ‹è¯•ã€éƒ¨ç½²å’Œç»´æŠ¤åº”ç”¨ç¨‹åºæˆ–è½¯ä»¶ã€‚_\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77d08668-4981-4b84-ae07-f74cfa191309",
   "metadata": {},
   "source": [
    "æ‰€æœ‰å¼€å‘æ¡†æ¶ï¼ˆSDKï¼‰çš„æ ¸å¿ƒä»·å€¼ï¼Œéƒ½æ˜¯é™ä½å¼€å‘ã€ç»´æŠ¤æˆæœ¬ã€‚\n",
    "\n",
    "å¤§è¯­è¨€æ¨¡å‹å¼€å‘æ¡†æ¶çš„ä»·å€¼ï¼Œæ˜¯è®©å¼€å‘è€…å¯ä»¥æ›´æ–¹ä¾¿åœ°å¼€å‘åŸºäºå¤§è¯­è¨€æ¨¡å‹çš„åº”ç”¨ã€‚ä¸»è¦æä¾›ä¸¤ç±»å¸®åŠ©ï¼š\n",
    "\n",
    "1. ç¬¬ä¸‰æ–¹èƒ½åŠ›æŠ½è±¡ã€‚æ¯”å¦‚ LLMã€å‘é‡æ•°æ®åº“ã€æœç´¢æ¥å£ç­‰\n",
    "2. å¸¸ç”¨å·¥å…·ã€æ–¹æ¡ˆå°è£…\n",
    "3. åº•å±‚å®ç°å°è£…ã€‚æ¯”å¦‚æµå¼æ¥å£ã€è¶…æ—¶é‡è¿ã€å¼‚æ­¥ä¸å¹¶è¡Œç­‰\n",
    "\n",
    "å¥½çš„å¼€å‘æ¡†æ¶ï¼Œéœ€è¦å…·å¤‡ä»¥ä¸‹ç‰¹ç‚¹ï¼š\n",
    "\n",
    "1. å¯é æ€§ã€é²æ£’æ€§é«˜\n",
    "2. å¯ç»´æŠ¤æ€§é«˜\n",
    "3. å¯æ‰©å±•æ€§é«˜\n",
    "4. å­¦ä¹ æˆæœ¬ä½\n",
    "\n",
    "ä¸¾äº›é€šä¿—çš„ä¾‹å­ï¼š\n",
    "\n",
    "- ä¸å¤–éƒ¨åŠŸèƒ½è§£ä¾èµ–\n",
    "  - æ¯”å¦‚å¯ä»¥éšæ„æ›´æ¢ LLM è€Œä¸ç”¨å¤§é‡é‡æ„ä»£ç \n",
    "  - æ›´æ¢ä¸‰æ–¹å·¥å…·ä¹ŸåŒç†\n",
    "- ç»å¸¸å˜çš„éƒ¨åˆ†è¦åœ¨å¤–éƒ¨ç»´æŠ¤è€Œä¸æ˜¯æ”¾åœ¨ä»£ç é‡Œ\n",
    "  - æ¯”å¦‚ Prompt æ¨¡æ¿\n",
    "- å„ç§ç¯å¢ƒä¸‹éƒ½é€‚ç”¨\n",
    "  - æ¯”å¦‚çº¿ç¨‹å®‰å…¨\n",
    "- æ–¹ä¾¿è°ƒè¯•å’Œæµ‹è¯•\n",
    "  - è‡³å°‘è¦èƒ½æ„Ÿè§‰åˆ°ç”¨äº†æ¯”ä¸ç”¨æ–¹ä¾¿å§\n",
    "  - åˆæ³•çš„è¾“å…¥ä¸ä¼šå¼•å‘æ¡†æ¶å†…éƒ¨çš„æŠ¥é”™\n",
    "\n",
    "<div class=\"alert alert-success\">\n",
    "<b>åˆ’é‡ç‚¹ï¼š</b>é€‰å¯¹äº†æ¡†æ¶ï¼Œäº‹åŠåŠŸå€ï¼›åä¹‹ï¼Œäº‹å€åŠŸåŠã€‚\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "579b589b-5a4e-451e-baf0-d2a409a9cb4b",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "    \n",
    "<b>ä»€ä¹ˆæ˜¯ SDK?</b> https://aws.amazon.com/cn/what-is/sdk/\n",
    "<br/>\n",
    "<b>SDK å’Œ API çš„åŒºåˆ«æ˜¯ä»€ä¹ˆ?</b> https://aws.amazon.com/cn/compare/the-difference-between-sdk-and-api/\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30361cf7-ec31-4c45-871e-28ac4f0db1a9",
   "metadata": {},
   "source": [
    "#### ğŸŒ° ä¸¾ä¸ªä¾‹å­ï¼šä½¿ç”¨ SDKï¼Œ4 è¡Œä»£ç å®ç°ä¸€ä¸ªç®€æ˜“çš„ RAG ç³»ç»Ÿ\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b734d50-a6eb-43f1-879d-fa00c84d1941",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "è¿è¡Œæœ¬è¯¾ä»£ç å‰ï¼Œè¯·å…ˆé‡å¯ä¸€ä¸‹ kernelï¼Œä»¥é‡ç½®æ‰€æœ‰é…ç½®ã€‚<br>\n",
    "    \n",
    "<img src=\"tips.png\" width=400px/>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb03dc82-649d-4fe8-8ba5-023220c8cb22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install --upgrade llama-index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dea32ce7-c7a7-4692-a217-45cf632281ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import VectorStoreIndex, SimpleDirectoryReader\n",
    "\n",
    "documents = SimpleDirectoryReader(\"./data\").load_data()\n",
    "index = VectorStoreIndex.from_documents(documents)\n",
    "\n",
    "query_engine = index.as_query_engine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "565af66f-30b4-41b4-a49f-88b7bbfc6fd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DeepSeek-V3æœ‰671Bæ€»å‚æ•°ï¼Œæ¯ä¸ªtokenæ¿€æ´»äº†37Bå‚æ•°ã€‚\n"
     ]
    }
   ],
   "source": [
    "response = query_engine.query(\"deepseek v3æœ‰å¤šå°‘å‚æ•°\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24e4c617-2fb8-489d-a0d0-34d5ca196f1c",
   "metadata": {},
   "source": [
    "## 2ã€LlamaIndex ä»‹ç»\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "313487e1-a2c7-4d4b-ad41-e3a9f0a0b07e",
   "metadata": {},
   "source": [
    "å®˜ç½‘æ ‡é¢˜ï¼š_ã€Œ Build AI Knowledge Assistants over your enterprise data ã€_\n",
    "\n",
    "LlamaIndex æ˜¯ä¸€ä¸ªä¸ºå¼€å‘ã€ŒçŸ¥è¯†å¢å¼ºã€çš„å¤§è¯­è¨€æ¨¡å‹åº”ç”¨çš„æ¡†æ¶ï¼ˆä¹Ÿå°±æ˜¯ SDKï¼‰ã€‚**çŸ¥è¯†å¢å¼º**ï¼Œæ³›æŒ‡ä»»ä½•åœ¨ç§æœ‰æˆ–ç‰¹å®šé¢†åŸŸæ•°æ®åŸºç¡€ä¸Šåº”ç”¨å¤§è¯­è¨€æ¨¡å‹çš„æƒ…å†µã€‚ä¾‹å¦‚ï¼š\n",
    "\n",
    "<img src=\"basic_rag.png\" width=600px>\n",
    "\n",
    "- Question-Answering Chatbots (ä¹Ÿå°±æ˜¯ RAG)\n",
    "- Document Understanding and Extraction ï¼ˆæ–‡æ¡£ç†è§£ä¸ä¿¡æ¯æŠ½å–ï¼‰\n",
    "\n",
    "- Autonomous Agents that can perform research and take actions ï¼ˆæ™ºèƒ½ä½“åº”ç”¨ï¼‰\n",
    "- Workflow orchestrating single and multi-agent (ç¼–æ’å•ä¸ªæˆ–å¤šä¸ªæ™ºèƒ½ä½“å½¢æˆå·¥ä½œæµï¼‰\n",
    "\n",
    "LlamaIndex æœ‰ Python å’Œ Typescript ä¸¤ä¸ªç‰ˆæœ¬ï¼ŒPython ç‰ˆçš„æ–‡æ¡£ç›¸å¯¹æ›´å®Œå–„ã€‚\n",
    "\n",
    "- Python æ–‡æ¡£åœ°å€ï¼šhttps://docs.llamaindex.ai/en/stable/\n",
    "- Python API æ¥å£æ–‡æ¡£ï¼šhttps://docs.llamaindex.ai/en/stable/api_reference/\n",
    "\n",
    "- TS æ–‡æ¡£åœ°å€ï¼šhttps://ts.llamaindex.ai/\n",
    "\n",
    "LlamaIndex æ˜¯ä¸€ä¸ªå¼€æºæ¡†æ¶ï¼ŒGithub é“¾æ¥ï¼šhttps://github.com/run-llama\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee4822b2-52d5-411c-8244-3432f8733da2",
   "metadata": {},
   "source": [
    "### LlamaIndex çš„æ ¸å¿ƒæ¨¡å—\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c474b43-018a-4687-a51e-54b391a6bbca",
   "metadata": {},
   "source": [
    "<img src=\"llamaindex.png\" alt=\"LlamaIndex æ ¸å¿ƒæ¨¡å—\" width=\"800\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca66bf37-48e4-45eb-9e7e-a24e86108ac1",
   "metadata": {},
   "source": [
    "### å®‰è£… LlamaIndex\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79027d8d-106f-42c8-b7b6-944a46182bda",
   "metadata": {},
   "source": [
    "1. Python\n",
    "\n",
    "```\n",
    "pip install llama-index\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b59179e-c8c8-4730-aa29-01c728c00df4",
   "metadata": {},
   "source": [
    "2. Typescript\n",
    "\n",
    "```\n",
    "# é€šè¿‡ npm å®‰è£…\n",
    "npm install llamaindex\n",
    "\n",
    "# é€šè¿‡ yarn å®‰è£…\n",
    "yarn add llamaindex\n",
    "\n",
    "# é€šè¿‡ pnpm å®‰è£…\n",
    "pnpm add llamaindex\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9a882ce-b03f-43dc-aa4c-b269ff49adfd",
   "metadata": {},
   "source": [
    "æœ¬è¯¾ç¨‹ä»¥ Python ç‰ˆä¸ºä¾‹è¿›è¡Œè®²è§£ã€‚\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74341a28-fb7f-4e5b-9342-cf5cd850654a",
   "metadata": {},
   "source": [
    "## 3ã€æ•°æ®åŠ è½½ï¼ˆLoadingï¼‰\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d134a56-ae9a-41a5-a703-ce9dfb3fc600",
   "metadata": {},
   "source": [
    "### 3.1ã€åŠ è½½æœ¬åœ°æ•°æ®\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d35bfa25-9a09-4c40-afa1-9f9156e56dc2",
   "metadata": {},
   "source": [
    "`SimpleDirectoryReader` æ˜¯ä¸€ä¸ªç®€å•çš„æœ¬åœ°æ–‡ä»¶åŠ è½½å™¨ã€‚å®ƒä¼šéå†æŒ‡å®šç›®å½•ï¼Œå¹¶æ ¹æ®æ–‡ä»¶æ‰©å±•åè‡ªåŠ¨åŠ è½½æ–‡ä»¶ï¼ˆ**æ–‡æœ¬å†…å®¹**ï¼‰ã€‚\n",
    "\n",
    "æ”¯æŒçš„æ–‡ä»¶ç±»å‹ï¼š\n",
    "\n",
    "- `.csv` - comma-separated values\n",
    "- `.docx` - Microsoft Word\n",
    "- `.epub` - EPUB ebook format\n",
    "- `.hwp` - Hangul Word Processor\n",
    "- `.ipynb` - Jupyter Notebook\n",
    "- `.jpeg`, `.jpg` - JPEG image\n",
    "- `.mbox` - MBOX email archive\n",
    "- `.md` - Markdown\n",
    "- `.mp3`, `.mp4` - audio and video\n",
    "- `.pdf` - Portable Document Format\n",
    "- `.png` - Portable Network Graphics\n",
    "- `.ppt`, `.pptm`, `.pptx` - Microsoft PowerPoint\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "05b6a279-3eca-4685-af91-80fb443fca1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pydantic.v1 import BaseModel\n",
    "\n",
    "def show_json(data):\n",
    "    \"\"\"ç”¨äºå±•ç¤ºjsonæ•°æ®\"\"\"\n",
    "    if isinstance(data, str):\n",
    "        obj = json.loads(data)\n",
    "        print(json.dumps(obj, indent=4, ensure_ascii=False))\n",
    "    elif isinstance(data, dict) or isinstance(data, list):\n",
    "        print(json.dumps(data, indent=4, ensure_ascii=False))\n",
    "    elif issubclass(type(data), BaseModel):\n",
    "        print(json.dumps(data.dict(), indent=4, ensure_ascii=False))\n",
    "\n",
    "def show_list_obj(data):\n",
    "    \"\"\"ç”¨äºå±•ç¤ºä¸€ç»„å¯¹è±¡\"\"\"\n",
    "    if isinstance(data, list):\n",
    "        for item in data:\n",
    "            show_json(item)\n",
    "    else:\n",
    "        raise ValueError(\"Input is not a list\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "198ebc20-fab6-46a9-8cca-4b114fe2640f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import SimpleDirectoryReader\n",
    "\n",
    "reader = SimpleDirectoryReader(\n",
    "        input_dir=\"./data\", # ç›®æ ‡ç›®å½•\n",
    "        recursive=False, # æ˜¯å¦é€’å½’éå†å­ç›®å½•\n",
    "        required_exts=[\".pdf\"] # (å¯é€‰)åªè¯»å–æŒ‡å®šåç¼€çš„æ–‡ä»¶\n",
    "    )\n",
    "documents = reader.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dfb3f1ac-75cb-4c5e-b0a1-53621d8d325e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DeepSeek-V3 Technical Report\n",
      "DeepSeek-AI\n",
      "research@deepseek.com\n",
      "Abstract\n",
      "We present DeepSeek-V3, a strong Mixture-of-Experts (MoE) language model with 671B total\n",
      "parameters with 37B activated for each token. To achieve efficient inference and cost-effective\n",
      "training, DeepSeek-V3 adopts Multi-head Latent Attention (MLA) and DeepSeekMoE architec-\n",
      "tures, which were thoroughly validated in DeepSeek-V2. Furthermore, DeepSeek-V3 pioneers\n",
      "an auxiliary-loss-free strategy for load balancing and sets a multi-token prediction training\n",
      "objective for stronger performance. We pre-train DeepSeek-V3 on 14.8 trillion diverse and\n",
      "high-quality tokens, followed by Supervised Fine-Tuning and Reinforcement Learning stages to\n",
      "fully harness its capabilities. Comprehensive evaluations reveal that DeepSeek-V3 outperforms\n",
      "other open-source models and achieves performance comparable to leading closed-source\n",
      "models. Despite its excellent performance, DeepSeek-V3 requires only 2.788M H800 GPU hours\n",
      "for its full training. In addition, its training process is remarkably stable. Throughout the entire\n",
      "training process, we did not experience any irrecoverable loss spikes or perform any rollbacks.\n",
      "The model checkpoints are available at https://github.com/deepseek-ai/DeepSeek-V3.\n",
      "MMLU-Pro\n",
      "(EM)\n",
      "GPQA-Diamond\n",
      "(Pass@1)\n",
      "MATH 500\n",
      "(EM)\n",
      "AIME 2024\n",
      "(Pass@1)\n",
      "Codeforces\n",
      "(Percentile)\n",
      "SWE-bench Verified\n",
      "(Resolved)\n",
      "0\n",
      "20\n",
      "40\n",
      "60\n",
      "80\n",
      "100Accuracy / Percentile (%)\n",
      "75.9\n",
      "59.1\n",
      "90.2\n",
      "39.2\n",
      "51.6\n",
      "42.0\n",
      "66.2\n",
      "41.3\n",
      "74.7\n",
      "16.7\n",
      "35.6\n",
      "22.6\n",
      "71.6\n",
      "49.0\n",
      "80.0\n",
      "23.3 24.8 23.8\n",
      "73.3\n",
      "51.1\n",
      "73.8\n",
      "23.3\n",
      "25.3 24.5\n",
      "72.6\n",
      "49.9\n",
      "74.6\n",
      "9.3\n",
      "23.6\n",
      "38.8\n",
      "78.0\n",
      "65.0\n",
      "78.3\n",
      "16.0\n",
      "20.3\n",
      "50.8\n",
      "DeepSeek-V3 DeepSeek-V2.5 Qwen2.5-72B-Inst Llama-3.1-405B-Inst GPT-4o-0513 Claude-3.5-Sonnet-1022\n",
      "Figure 1 |Benchmark performance of DeepSeek-V3 and its counterparts.\n",
      "arXiv:2412.19437v2  [cs.CL]  18 Feb 2025\n",
      "{\n",
      "    \"id_\": \"268f9401-4266-45fd-9ccf-b1dcb17daaef\",\n",
      "    \"embedding\": null,\n",
      "    \"metadata\": {\n",
      "        \"page_label\": \"1\",\n",
      "        \"file_name\": \"deepseek-v3-1-4.pdf\",\n",
      "        \"file_path\": \"/home/jovyan/dev-notes/10-llamaindex/data/deepseek-v3-1-4.pdf\",\n",
      "        \"file_type\": \"application/pdf\",\n",
      "        \"file_size\": 192218,\n",
      "        \"creation_date\": \"2025-03-12\",\n",
      "        \"last_modified_date\": \"2025-03-12\"\n",
      "    },\n",
      "    \"excluded_embed_metadata_keys\": [\n",
      "        \"file_name\",\n",
      "        \"file_type\",\n",
      "        \"file_size\",\n",
      "        \"creation_date\",\n",
      "        \"last_modified_date\",\n",
      "        \"last_accessed_date\"\n",
      "    ],\n",
      "    \"excluded_llm_metadata_keys\": [\n",
      "        \"file_name\",\n",
      "        \"file_type\",\n",
      "        \"file_size\",\n",
      "        \"creation_date\",\n",
      "        \"last_modified_date\",\n",
      "        \"last_accessed_date\"\n",
      "    ],\n",
      "    \"relationships\": {},\n",
      "    \"metadata_template\": \"{key}: {value}\",\n",
      "    \"metadata_separator\": \"\\n\",\n",
      "    \"text_resource\": {\n",
      "        \"embeddings\": null,\n",
      "        \"text\": \"DeepSeek-V3 Technical Report\\nDeepSeek-AI\\nresearch@deepseek.com\\nAbstract\\nWe present DeepSeek-V3, a strong Mixture-of-Experts (MoE) language model with 671B total\\nparameters with 37B activated for each token. To achieve efficient inference and cost-effective\\ntraining, DeepSeek-V3 adopts Multi-head Latent Attention (MLA) and DeepSeekMoE architec-\\ntures, which were thoroughly validated in DeepSeek-V2. Furthermore, DeepSeek-V3 pioneers\\nan auxiliary-loss-free strategy for load balancing and sets a multi-token prediction training\\nobjective for stronger performance. We pre-train DeepSeek-V3 on 14.8 trillion diverse and\\nhigh-quality tokens, followed by Supervised Fine-Tuning and Reinforcement Learning stages to\\nfully harness its capabilities. Comprehensive evaluations reveal that DeepSeek-V3 outperforms\\nother open-source models and achieves performance comparable to leading closed-source\\nmodels. Despite its excellent performance, DeepSeek-V3 requires only 2.788M H800 GPU hours\\nfor its full training. In addition, its training process is remarkably stable. Throughout the entire\\ntraining process, we did not experience any irrecoverable loss spikes or perform any rollbacks.\\nThe model checkpoints are available at https://github.com/deepseek-ai/DeepSeek-V3.\\nMMLU-Pro\\n(EM)\\nGPQA-Diamond\\n(Pass@1)\\nMATH 500\\n(EM)\\nAIME 2024\\n(Pass@1)\\nCodeforces\\n(Percentile)\\nSWE-bench Verified\\n(Resolved)\\n0\\n20\\n40\\n60\\n80\\n100Accuracy / Percentile (%)\\n75.9\\n59.1\\n90.2\\n39.2\\n51.6\\n42.0\\n66.2\\n41.3\\n74.7\\n16.7\\n35.6\\n22.6\\n71.6\\n49.0\\n80.0\\n23.3 24.8 23.8\\n73.3\\n51.1\\n73.8\\n23.3\\n25.3 24.5\\n72.6\\n49.9\\n74.6\\n9.3\\n23.6\\n38.8\\n78.0\\n65.0\\n78.3\\n16.0\\n20.3\\n50.8\\nDeepSeek-V3 DeepSeek-V2.5 Qwen2.5-72B-Inst Llama-3.1-405B-Inst GPT-4o-0513 Claude-3.5-Sonnet-1022\\nFigure 1 |Benchmark performance of DeepSeek-V3 and its counterparts.\\narXiv:2412.19437v2  [cs.CL]  18 Feb 2025\",\n",
      "        \"path\": null,\n",
      "        \"url\": null,\n",
      "        \"mimetype\": null\n",
      "    },\n",
      "    \"image_resource\": null,\n",
      "    \"audio_resource\": null,\n",
      "    \"video_resource\": null,\n",
      "    \"text_template\": \"{metadata_str}\\n\\n{content}\",\n",
      "    \"class_name\": \"Document\",\n",
      "    \"text\": \"DeepSeek-V3 Technical Report\\nDeepSeek-AI\\nresearch@deepseek.com\\nAbstract\\nWe present DeepSeek-V3, a strong Mixture-of-Experts (MoE) language model with 671B total\\nparameters with 37B activated for each token. To achieve efficient inference and cost-effective\\ntraining, DeepSeek-V3 adopts Multi-head Latent Attention (MLA) and DeepSeekMoE architec-\\ntures, which were thoroughly validated in DeepSeek-V2. Furthermore, DeepSeek-V3 pioneers\\nan auxiliary-loss-free strategy for load balancing and sets a multi-token prediction training\\nobjective for stronger performance. We pre-train DeepSeek-V3 on 14.8 trillion diverse and\\nhigh-quality tokens, followed by Supervised Fine-Tuning and Reinforcement Learning stages to\\nfully harness its capabilities. Comprehensive evaluations reveal that DeepSeek-V3 outperforms\\nother open-source models and achieves performance comparable to leading closed-source\\nmodels. Despite its excellent performance, DeepSeek-V3 requires only 2.788M H800 GPU hours\\nfor its full training. In addition, its training process is remarkably stable. Throughout the entire\\ntraining process, we did not experience any irrecoverable loss spikes or perform any rollbacks.\\nThe model checkpoints are available at https://github.com/deepseek-ai/DeepSeek-V3.\\nMMLU-Pro\\n(EM)\\nGPQA-Diamond\\n(Pass@1)\\nMATH 500\\n(EM)\\nAIME 2024\\n(Pass@1)\\nCodeforces\\n(Percentile)\\nSWE-bench Verified\\n(Resolved)\\n0\\n20\\n40\\n60\\n80\\n100Accuracy / Percentile (%)\\n75.9\\n59.1\\n90.2\\n39.2\\n51.6\\n42.0\\n66.2\\n41.3\\n74.7\\n16.7\\n35.6\\n22.6\\n71.6\\n49.0\\n80.0\\n23.3 24.8 23.8\\n73.3\\n51.1\\n73.8\\n23.3\\n25.3 24.5\\n72.6\\n49.9\\n74.6\\n9.3\\n23.6\\n38.8\\n78.0\\n65.0\\n78.3\\n16.0\\n20.3\\n50.8\\nDeepSeek-V3 DeepSeek-V2.5 Qwen2.5-72B-Inst Llama-3.1-405B-Inst GPT-4o-0513 Claude-3.5-Sonnet-1022\\nFigure 1 |Benchmark performance of DeepSeek-V3 and its counterparts.\\narXiv:2412.19437v2  [cs.CL]  18 Feb 2025\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "print(documents[0].text)\n",
    "show_json(documents[0].json())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "277be7f4-1993-4b74-bd3e-f9d8cb5a827d",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "<b>æ³¨æ„ï¼š</b>å¯¹å›¾åƒã€è§†é¢‘ã€è¯­éŸ³ç±»æ–‡ä»¶ï¼Œé»˜è®¤ä¸ä¼šè‡ªåŠ¨æå–å…¶ä¸­æ–‡å­—ã€‚å¦‚éœ€æå–ï¼Œå‚è€ƒä¸‹é¢ä»‹ç»çš„ <code>Data Connectors</code>ã€‚\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2c98851-0858-4215-858d-bea70e310d5f",
   "metadata": {},
   "source": [
    "é»˜è®¤çš„ `PDFReader` æ•ˆæœå¹¶ä¸ç†æƒ³ï¼Œæˆ‘ä»¬å¯ä»¥æ›´æ¢æ–‡ä»¶åŠ è½½å™¨\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b093a88b-5d4a-423b-bfe7-fa28b34744e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started parsing the file under job_id 45c4b2cf-963a-4abc-99be-38b82ce8eef3\n",
      "# DeepSeek-V3 Technical Report\n",
      "\n",
      "# DeepSeek-AI\n",
      "\n",
      "research@deepseek.com\n",
      "\n",
      "# Abstract\n",
      "\n",
      "We present DeepSeek-V3, a strong Mixture-of-Experts (MoE) language model with 671B total parameters with 37B activated for each token. To achieve efficient inference and cost-effective training, DeepSeek-V3 adopts Multi-head Latent Attention (MLA) and DeepSeekMoE architectures, which were thoroughly validated in DeepSeek-V2. Furthermore, DeepSeek-V3 pioneers an auxiliary-loss-free strategy for load balancing and sets a multi-token prediction training objective for stronger performance. We pre-train DeepSeek-V3 on 14.8 trillion diverse and high-quality tokens, followed by Supervised Fine-Tuning and Reinforcement Learning stages to fully harness its capabilities. Comprehensive evaluations reveal that DeepSeek-V3 outperforms other open-source models and achieves performance comparable to leading closed-source models. Despite its excellent performance, DeepSeek-V3 requires only 2.788M H800 GPU hours for its full training. In addition, its training process is remarkably stable. Throughout the entire training process, we did not experience any irrecoverable loss spikes or perform any rollbacks. The model checkpoints are available at https://github.com/deepseek-ai/DeepSeek-V3.\n",
      "\n",
      "|DeepSeek-V3|DeepSeek-V2.5|Qwen2.5-72B-Inst|Llama-3.1-405B-Inst|GPT-4o-0513|Claude-3.5-Sonnet-1022| |\n",
      "|---|---|---|---|---|---|---|\n",
      "|100|90.2| | | | | |\n",
      "|80|75.9|71.6|73.3|72.6|78.0| |\n",
      "| |74.7|80.0|73.8|74.6|78.3| |\n",
      "|66.2| |65.0| | | | |\n",
      "|60| |59.1| | | | |\n",
      "|49.0|51.1|49.9| |51.6|50.8| |\n",
      "|40|41.3|39.2| |42.0|38.8| |\n",
      "| | | |35.6| | | |\n",
      "|20| | | |20.3| | |\n",
      "| |16.7|16.0| |9.3| | |\n",
      "|0|MMLU-Pro|GPQA-Diamond|MATH 500|AIME 2024|Codeforces|SWE-bench Verified|\n",
      "\n",
      "Figure 1 | Benchmark performance of DeepSeek-V3 and its counterparts.\n"
     ]
    }
   ],
   "source": [
    "# åœ¨ç¯å¢ƒå˜é‡ä¸­é…ç½® LLAMA_CLOUD_API_KEY=llx-8uobXJdi12aGhJduv2iTpbpXnZPbMFEMvDoKzj1JLV4ZjKDP\n",
    "import os\n",
    "os.environ[\"LLAMA_CLOUD_API_KEY\"] = \"llx-8uobXJdi12aGhJduv2iTpbpXnZPbMFEMvDoKzj1JLV4ZjKDP\"\n",
    "\n",
    "from llama_cloud_services import LlamaParse\n",
    "from llama_index.core import SimpleDirectoryReader\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply() # åªåœ¨Jupyterç¬”è®°ç¯å¢ƒä¸­éœ€è¦æ­¤æ“ä½œï¼Œå¦åˆ™ä¼šæŠ¥é”™\n",
    "\n",
    "# set up parser\n",
    "parser = LlamaParse(\n",
    "    result_type=\"markdown\"  # \"markdown\" and \"text\" are available\n",
    ")\n",
    "file_extractor = {\".pdf\": parser}\n",
    "\n",
    "documents = SimpleDirectoryReader(input_dir=\"./data\", required_exts=[\".pdf\"], file_extractor=file_extractor).load_data()\n",
    "print(documents[0].text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a116d296-e4a4-4710-a41a-318bbab8ec24",
   "metadata": {},
   "source": [
    "### 3.2ã€Data Connectors\n",
    "\n",
    "ç”¨äºå¤„ç†æ›´ä¸°å¯Œçš„æ•°æ®ç±»å‹ï¼Œå¹¶å°†å…¶è¯»å–ä¸º `Document` çš„å½¢å¼ã€‚\n",
    "\n",
    "ä¾‹å¦‚ï¼šç›´æ¥è¯»å–ç½‘é¡µ\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19471210-6cb3-4822-8bad-dc701b6ab335",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install llama-index-readers-web"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2d73abf3-91df-488c-a5f3-c7de0c6d4c44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ç½‘é¡µ](https://www.baidu.com/s?ie=utf-8&fr=bks0000&wd=çŸ¥ä¹çŸ¥å­¦å ‚)[æ–°é—»](http://news.baidu.com/ns?tn=news&cl=2&rn=20&ct=1&fr=bks0000&ie=utf-8&word=çŸ¥ä¹çŸ¥å­¦å ‚)[è´´å§](https://tieba.baidu.com/f?ie=utf-8&fr=bks0000&kw=çŸ¥ä¹çŸ¥å­¦å ‚)[çŸ¥é“](https://zhidao.baidu.com/search?pn=0&&rn=10&lm=0&fr=bks0000&word=çŸ¥ä¹çŸ¥å­¦å ‚)[ç½‘ç›˜](https://pan.baidu.com/disk/home#/search?from=1027327l&key=çŸ¥ä¹çŸ¥å­¦å ‚)[å›¾ç‰‡](http://image.baidu.com/search/index?tn=baiduimage&ct=201326592&lm=-1&cl=2&nc=1&ie=utf-8&word=çŸ¥ä¹çŸ¥å­¦å ‚)[è§†é¢‘](https://www.baidu.com/sf/vsearch?pd=video&tn=vsearch&ie=utf-8&rsv_spt=17&wd=çŸ¥ä¹çŸ¥å­¦å ‚)[åœ°å›¾](http://map.baidu.com/m?ie=utf-8&fr=bks0000&word=çŸ¥ä¹çŸ¥å­¦å ‚)[æ–‡åº“](https://wenku.baidu.com/search?lm=0&od=0&ie=utf-8&fr=bks0000&word=çŸ¥ä¹çŸ¥å­¦å ‚)[èµ„è®¯](https://www.baidu.com/s?rtt=1&bsst=1&cl=2&tn=news&fr=baike&word=çŸ¥ä¹çŸ¥å­¦å ‚)[é‡‡è´­](https://b2b.baidu.com/s?fr=bks0000&q=çŸ¥ä¹çŸ¥å­¦å ‚)ç™¾ç§‘\n",
      "\n",
      "[ç™¾åº¦é¦–é¡µ](http://www.baidu.com)\n",
      "\n",
      "ç™»å½•\n",
      "\n",
      "[æ³¨å†Œ](https://passport.baidu.com/v2/?reg&regType=1&tpl=wk)\n",
      "\n",
      "[![ç™¾åº¦ç™¾ç§‘](https://baikebcs.bdimg.com/baike-react/common/logo-\n",
      "baike.svg)](/#home)\n",
      "\n",
      "è¿›å…¥è¯æ¡å…¨ç«™æœç´¢[å¸®åŠ©](/help)\n",
      "\n",
      "[é¦–é¡µ](/)\n",
      "\n",
      "    \n",
      "\n",
      "ç§’æ‡‚ç™¾ç§‘\n",
      "\n",
      "    \n",
      "\n",
      "ç‰¹è‰²ç™¾ç§‘\n",
      "\n",
      "    \n",
      "\n",
      "çŸ¥è¯†ä¸“é¢˜\n",
      "\n",
      "    \n",
      "\n",
      "åŠ å…¥ç™¾ç§‘\n",
      "\n",
      "    \n",
      "\n",
      "ç™¾ç§‘å›¢é˜Ÿ\n",
      "\n",
      "    \n",
      "\n",
      "æƒå¨åˆä½œ\n",
      "\n",
      "    \n",
      "\n",
      "[ä¸ªäººä¸­å¿ƒ](/usercenter)\n",
      "\n",
      "æ”¶è—\n",
      "\n",
      "æŸ¥çœ‹[æˆ‘çš„æ”¶è—](/uc/favolemma)\n",
      "\n",
      "0æœ‰ç”¨+1\n",
      "\n",
      "0\n",
      "\n",
      "# çŸ¥ä¹çŸ¥å­¦å ‚\n",
      "\n",
      "æ’­æŠ¥[è®¨è®º](/planet/talk?lemmaId=64028760&fromModule=lemma_right-issue-btn)ä¸Šä¼ è§†é¢‘\n",
      "\n",
      "çŸ¥ä¹æ——ä¸‹èŒä¸šæ•™è‚²å“ç‰Œ\n",
      "\n",
      "çŸ¥ä¹çŸ¥å­¦å ‚æ˜¯çŸ¥ä¹æ——ä¸‹çš„èŒä¸šæ•™è‚²å“ç‰Œï¼Œäº2022å¹´12æœˆ30æ—¥æ­£å¼å‘å¸ƒã€‚çŸ¥ä¹çŸ¥å­¦å ‚ä¾æ‰˜è‡ªèº«ç§‘æŠ€å®åŠ›ï¼Œä¸“æ³¨äºç”¨æˆ·èŒä¸šå‘å±•ï¼Œèšé›†å„è¡Œä¸šä¼˜è´¨æ•™è‚²èµ„æºï¼Œæ‰“é€ äº†ä¸€ç«™å¼åœ¨çº¿èŒä¸šæ•™è‚²å¹³å°ã€‚\n",
      "[1]__æˆªè‡³2023å¹´åº•\n",
      "ï¼ŒçŸ¥ä¹çŸ¥å­¦å ‚é›†åˆäº†å“èŒæ•™è‚²ã€è¶´è¶´æ•™è‚²ã€MBAå¤§å¸ˆã€ä¸€èµ·å…¬è€ƒã€ä¸€èµ·è€ƒæ•™å¸ˆã€AGIè¯¾å ‚6ä¸ªå­å“ç‰Œï¼Œå®Œæˆäº†6å¤§é¢†åŸŸ30å¤šä¸ªå“ç±»è¯¾ç¨‹çš„è¦†ç›–ï¼Œç´¯è®¡ä¸ºè¶…è¿‡2000ä¸‡å­¦å‘˜æä¾›èŒä¸šåŸ¹è®­æœåŠ¡ã€‚\n",
      "[2]__\n",
      "\n",
      "2024å¹´1æœˆ11æ—¥ï¼ŒçŸ¥ä¹çŸ¥å­¦å ‚å®£å¸ƒæ­£å¼ç‹¬ç«‹è¿è¥ï¼Œå¹¶å…¬å¸ƒäº†æœªæ¥ä¸¤å¤§æ ¸å¿ƒæˆ˜ç•¥ï¼šç¬¬ä¸€ï¼Œé€šè¿‡æ•°å­—åŒ–èµ‹èƒ½ï¼Œæ”¹å–„èŒä¸šæ•™è‚²æœåŠ¡ä½“ç³»å’Œäº¤ä»˜æ ‡å‡†ï¼›ç¬¬äºŒï¼Œé€šè¿‡å¼€æ”¾åä½œå’Œäº§æ•™èåˆï¼ŒåŠ©åŠ›èŒä¸šæ•™è‚²å®ç°é«˜è´¨é‡å°±ä¸šã€‚\n",
      "[2]__\n",
      "\n",
      "ä¸­æ–‡å\n",
      "\n",
      "     çŸ¥ä¹çŸ¥å­¦å ‚\n",
      "\n",
      "æ‰€å±è¡Œä¸š\n",
      "\n",
      "    èŒä¸šæ•™è‚²\n",
      "\n",
      "åˆ›ç«‹æ—¶é—´\n",
      "\n",
      "    2022å¹´12æœˆ30æ—¥\n",
      "\n",
      "æ‰€å±å…¬å¸\n",
      "\n",
      "    çŸ¥ä¹\n",
      "\n",
      "å“ç‰Œå£å·\n",
      "\n",
      "    æœªæ¥èŒä¸šä¹‹è·¯\n",
      "\n",
      "å•†æ ‡æ³¨å†Œå·\n",
      "\n",
      "    59736985 [3]__\n",
      "\n",
      "## ç›®å½•\n",
      "\n",
      "  1. 1å‘å±•å†ç¨‹\n",
      "  2. 2ä¸šåŠ¡çŸ©é˜µ\n",
      "  3. 3æ——ä¸‹å“ç‰Œ\n",
      "  4. 4éƒ¨åˆ†å¸ˆèµ„\n",
      "  5. 5å“ç‰ŒåŠ¨æ€\n",
      "\n",
      "## å‘å±•å†ç¨‹\n",
      "\n",
      "æ’­æŠ¥\n",
      "\n",
      "ç¼–è¾‘\n",
      "\n",
      "2022å¹´6æœˆ29æ—¥ï¼Œæ¢ç»„æ±‚æ±‚å˜±ä¹å‡¶çŸ¥ä¹æ¨å‡ºâ€œå­¦ä¹ â€ä¸“åŒºï¼Œä¸Šçº¿æ•°ç™¾é—¨å…è´¹æ­£ç‰ˆæˆæƒè¯¾ç¨‹ã€‚ [4]__\n",
      "\n",
      "2022å¹´11æœˆ1æ—¥ï¼ŒçŸ¥ä¹ä¸Šçº¿AIå¤§å¸ˆå…è´¹è¯¾ç¨‹ï¼Œå´æ©è¾¾ã€æ²ˆå‘æ´‹é¢†è¡”æ­ç§˜äººå·¥æ™ºèƒ½ã€‚ [5]__\n",
      "\n",
      "2022å¹´12æœˆ30æ—¥ï¼ŒçŸ¥ä¹å‘å¸ƒèŒä¸šæ•™è‚²å“ç‰Œâ€”â€”åœ¨çº¿èŒä¸šæ•™è‚²å¹³å°ã€ŒçŸ¥ä¹çŸ¥å­¦å ‚ã€ã€‚ [6]__\n",
      "\n",
      "2023å¹´2æœˆ21æ—¥ï¼ŒçŸ¥ä¹è”åˆä¸­å›½æ•™å‚¬ç­¾å¯’è‚²åœ¨çº¿å‘å¸ƒè€ƒç ”ç¦è®²åšæ•°æ®æŠ¥å‘Šï¼Œç«™å†…è€ƒç ”æ ¼é¡µåºœå…´è¶£ç”¨æˆ·1å˜±æˆ’594ä¸‡ã€‚ [7]__\n",
      "\n",
      "2023å¹´7æœˆ15æ—¥ï¼ŒçŸ¥ä¹å¹´åº¦æ´»åŠ¨ã€Œ2023å¹´æ–°çŸ¥é’å¹´å¤§ä¼šã€åœ¨åŒ—äº¬ä¸¾è¡Œã€‚çŸ¥ä¹åˆ›å§‹äººã€è‘£äº‹é•¿å…¼ CEO\n",
      "å‘¨æºå‡ºå¸­å¹¶å‘è¡¨æ¼”å©†æ—‹è®²å°†èŒä¸šæ•™è‚²ä½œä¸ºç¬¬äºŒå¢é•¿æ›²çº¿ï¼ŒåŒæ—¶ä¹Ÿå®£å¸ƒã€ŒçŸ¥å­¦è®¡åˆ’ ã€å·²è¿›å…¥å†…æµ‹é˜¶æ®µï¼Œå³å°†å¯¹æ›´å¤šæœ‰å¿—äºæä¾›ä¸“ä¸šèŒä¸šæ•™è‚²æœåŠ¡çš„æœºæ„å’Œä¸ªäººå¼€æ”¾ã€‚ [8]__\n",
      "\n",
      "2023å¹´11æœˆ29æ—¥ï¼ŒçŸ¥ä¹å‘å¸ƒ2023å¹´Q3è´¢ç¢‘æ—¬ç…§æŠ¥ï¼ŒèŒä¸šæ•™è‚²ä¸šåŠ¡è¥æ”¶è¾¾åˆ°1.45äº¿å…ƒï¼Œå®ç°åŒæ¯”å¢é•¿85.6%ã€‚ [9]__\n",
      "\n",
      "2024å¹´1æœˆ11æ—¥æ¶ˆæ¯ï¼ŒçŸ¥ä¹åœ¨åŒ—äº¬ä¸¾åŠã€Œ2024çŸ¥ä¹æ•™è‚²å¤§ä¼šã€ï¼Œå®£å¸ƒèŒä¸šæ•™è‚²å“ç‰Œã€ŒçŸ¥ä¹çŸ¥å­¦å ‚ã€æ­£å¼ç‹¬ç«‹è¿è¥ã€‚ [2]__\n",
      "\n",
      "## ä¸šåŠ¡çŸ©é˜µ\n",
      "\n",
      "æ’­æŠ¥\n",
      "\n",
      "ç¼–è¾‘\n",
      "\n",
      "çŸ¥ä¹çŸ¥å­¦å ‚å·²ç»é›†åˆäº†å“èŒæ•™è‚²ã€è¶´è¶´æ•™è‚²ã€MBAå¤§å¸ˆã€ä¸€èµ·å…¬è€ƒã€ä¸€èµ·è€ƒæ•™å¸ˆã€AGIè¯¾å ‚6ä¸ªå­å“ç‰Œï¼Œå®Œæˆäº†6å¤§é¢†åŸŸ30å¤šä¸ªå“ç±»çš„è¦†ç›–ï¼Œç´¯è®¡ä¸ºè¶…è¿‡2000ä¸‡å­¦å‘˜æä¾›èŒä¸šåŸ¹è®­æœåŠ¡ï¼Œæˆä¸ºå›½å†…è¦†ç›–å“ç±»æœ€å…¨çš„èŒä¸šæ•™è‚²å†…å®¹ä½“ç³»ä¹‹ä¸€ã€‚åœ¨è€ƒç ”ã€å…¬è€ƒã€æ•™èµ„ã€CPAã€CFAã€MBAã€èŒåœºæŠ€èƒ½ç­‰è¯¸å¤šè¯¾ç¨‹èµ›é“ä¸­ï¼ŒçŸ¥ä¹éƒ½å½¢æˆäº†ä¸€å®šçš„ç”¨æˆ·å£ç¢‘å’Œå†…å®¹å½±å“åŠ›ã€‚\n",
      "[2]__\n",
      "\n",
      "[![](https://bkimg.cdn.bcebos.com/pic/d31b0ef41bd5ad6eddc4d047bd932edbb6fd5266965c?x-bce-\n",
      "process=image/format,f_auto/resize,m_lfit,limit_1,h_268)](/pic/%E7%9F%A5%E4%B9%8E%E7%9F%A5%E5%AD%A6%E5%A0%82/64028760/0/d31b0ef41bd5ad6eddc4d047bd932edbb6fd5266965c?fr=lemma&fromModule=lemma_content-\n",
      "image \"çŸ¥ä¹çŸ¥å­¦å ‚å­¦ç§‘å“ç±»\")çŸ¥ä¹çŸ¥å­¦å ‚å­¦ç§‘å“ç±» [1]__\n",
      "\n",
      "## æ——ä¸‹å“ç‰Œ\n",
      "\n",
      "æ’­æŠ¥\n",
      "\n",
      "ç¼–è¾‘\n",
      "\n",
      "è¶´è¶´æ•™è‚²ï¼šçŸ¥ä¹çŸ¥å­¦å ‚æ——ä¸‹ä¸“æ³¨ç•™å­¦è€ƒè¯•åŸ¹è®­ä»¥åŠç•™å­¦ç”³è¯·çš„ä¸€ç«™å¼æœåŠ¡æœºæ„ï¼Œå¼€è®¾æœ‰é›…æ€ã€æ‰˜ç¦ã€GREã€GMAT ç­‰è¯¾ç¨‹äº§å“ï¼Œç´¯è®¡ä¸ºç™¾ä¸‡å­¦å‘˜æä¾›è¿‡ç•™å­¦è¯­åŸ¹æœåŠ¡ã€‚\n",
      "[1]__\n",
      "\n",
      "å“èŒæ•™è‚²ï¼šçŸ¥ä¹çŸ¥å­¦å ‚æ——ä¸‹ä¸“æ³¨é‡‘èèŒä¸šåŸ¹è®­çš„å­å“ç‰Œï¼Œæä¾›åŒ…æ‹¬ CFAã€ESGã€FRMã€CPAã€é‡‘èè€ƒç ”ç­‰å¤šæ¬¾è´¢ç»æ•™è‚²ç±»è¯¾ç¨‹ï¼Œå·²ç´¯è®¡æœåŠ¡è€ƒç”Ÿ\n",
      "13000+ã€CFA æŒè¯å­¦å‘˜ 2000+ï¼Œå¹¶ä¸º ESG é¢†åŸŸè¾“é€äººæ‰ 4000+ã€‚ [1]__\n",
      "\n",
      "MBAå¤§å¸ˆï¼šçŸ¥ä¹çŸ¥å­¦å ‚æ——ä¸‹ä¸“æ³¨ç®¡ç†ç±»è”è€ƒåŸ¹è®­çš„åœ¨çº¿æ•™è‚²å“ç‰Œï¼Œä¸ºè€ƒç”Ÿæä¾›ç²¾å“è¯¾ç¨‹ã€ä¸“å±æ•™æåŠä¸€ç«™å¼å¤‡è€ƒæœåŠ¡ã€‚ã€ŒMBAå¤§å¸ˆã€å§‹ç»ˆè‡´åŠ›äºä¸ºç”¨æˆ·æä¾›æ™ºèƒ½åŒ–ã€ä¸ªæ€§åŒ–ã€ç³»ç»ŸåŒ–çš„åœ¨çº¿å­¦ä¹ ä½“éªŒï¼Œç´¯è®¡å·²æœåŠ¡ç®¡ç†ç±»è”è€ƒè€ƒç”Ÿè¶…è¿‡\n",
      "300 ä¸‡äººã€‚ [1]__\n",
      "\n",
      "ä¸€èµ·è€ƒæ•™å¸ˆï¼šçŸ¥ä¹çŸ¥å­¦å ‚æ——ä¸‹ä¸“æ³¨æ•™å¸ˆèµ„æ ¼è¯å’Œæ•™å¸ˆæ‹›è˜è€ƒè¯•åŸ¹è®­çš„æ•™è‚²å“ç‰Œï¼Œç´¯è®¡ä¸ºè¶…è¿‡ 1500 ä¸‡å¤‡è€ƒæ•™å¸ˆèµ„æ ¼è¯å’Œæ•™å¸ˆæ‹›è˜çš„è€ƒç”Ÿæä¾›è¿‡æœåŠ¡ã€‚ [1]__\n",
      "\n",
      "ä¸€èµ·å…¬è€ƒï¼šçŸ¥ä¹çŸ¥å­¦å ‚æ——ä¸‹ä¸“æ³¨äºå…¬åŠ¡å‘˜å’Œäº‹ä¸šå•ä½è€ƒè¯•åŸ¹è®­çš„æ•™è‚²å“ç‰Œï¼Œç´¯è®¡ä¸ºè¶…è¿‡ 200 ä¸‡å¤‡è€ƒå…¬åŠ¡å‘˜åŠäº‹ä¸šå•ä½è€ƒè¯•çš„ç”¨æˆ·æä¾›è¿‡æœåŠ¡ã€‚ [1]__\n",
      "\n",
      "AGIè¯¾å ‚ï¼šçŸ¥ä¹çŸ¥å­¦å ‚æ——ä¸‹ä¸“æ³¨äººå·¥æ™ºèƒ½æŠ€æœ¯åŸ¹è®­çš„æ•™è‚²å“ç‰Œï¼Œæä¾›å¤§æ¨¡å‹ç®—æ³•åŸç†ã€å¤§æ¨¡å‹æŠ€æœ¯åº”ç”¨ã€å¤§æ¨¡å‹ä¸šåŠ¡è§£å†³æ–¹æ¡ˆç­‰å‰æ²¿ AI\n",
      "è¯¾ç¨‹ã€‚ã€ŒAGIè¯¾å ‚ã€å§‹ç»ˆè‡´åŠ›äºç»“åˆæ—¶ä¸‹çƒ­é—¨æŠ€æœ¯è·¯çº¿åŠé«˜é¢‘åº”ç”¨åœºæ™¯ï¼Œä¸ºç”¨æˆ·æä¾›ä¸€ç«™å¼äººå·¥æ™ºèƒ½æ•™è‚²è§£å†³æ–¹æ¡ˆã€‚ [1]__\n",
      "\n",
      "[![](https://bkimg.cdn.bcebos.com/pic/dcc451da81cb39dbb6fd2dd3924e1e24ab18962b9aa4?x-bce-\n",
      "process=image/format,f_auto/resize,m_lfit,limit_1,h_240)](/pic/%E7%9F%A5%E4%B9%8E%E7%9F%A5%E5%AD%A6%E5%A0%82/64028760/0/dcc451da81cb39dbb6fd2dd3924e1e24ab18962b9aa4?fr=lemma&fromModule=lemma_content-\n",
      "image \"çŸ¥ä¹çŸ¥å­¦å ‚æ——ä¸‹å“ç‰Œ\")çŸ¥ä¹çŸ¥å­¦å ‚æ——ä¸‹å“ç‰Œ [2]__\n",
      "\n",
      "## éƒ¨åˆ†å¸ˆèµ„\n",
      "\n",
      "æ’­æŠ¥\n",
      "\n",
      "ç¼–è¾‘\n",
      "\n",
      "è´¾å²š è€ƒç ”è‹±è¯­èµ„æ·±åå¸ˆã€ä¸­å›½äººæ°‘å¤§å­¦è‹±è¯­è¯­è¨€æ–‡å­¦ç¡•å£«ã€‚ [1]__\n",
      "\n",
      "ç‹ç‘æ© ä¸­ç¾ä¸¤å›½æ³•å¾‹èŒä¸šèµ„æ ¼è¯æŒè¯äººã€æœ¬ç§‘æ¯•ä¸šäºæ¸…åå¤§å­¦ã€‚ [1]__\n",
      "\n",
      "è”¡é‡‘é¾™ å…¬è€ƒèµ„æ·±åå¸ˆã€17å¹´æ‰§æ•™è¡Œä¸šç»éªŒã€æ¸…åå¤§å­¦æœ¬ç¡•ã€‚ [1]__\n",
      "\n",
      "[ææ–¯å…‹](/item/%E6%9D%8E%E6%96%AF%E5%85%8B/58536549?fromModule=lemma_inlink)\n",
      "CFA/FRMèµ„æ·±åå¸ˆã€ä¸Šæµ·äº¤é€šå¤§å­¦æœ¬ç¡•ã€‚ [1]__\n",
      "\n",
      "Paul Winterbottom å‰é›…æ€å£è¯­å†™ä½œè‹±ç±è€ƒå®˜ã€ç‰›æ´¥å¤§å­¦æœ¬ç¡•ã€‚ [1]__\n",
      "\n",
      "[è–›ç¿](/item/%E8%96%9B%E7%9D%BF/54369978?fromModule=lemma_inlink)\n",
      "ç®¡ç†ç±»è”è€ƒèµ„æ·±åå¸ˆã€è¥¿å®‰äº¤å¤§æœ¬ç§‘åŠMBAã€åˆ©å…¹ç¡•å£«ã€‚ [1]__\n",
      "\n",
      "å¼ å€© å‰çº¢åœˆæ‰€å¾‹å¸ˆã€æ¯•ä¸šäºåŒ—äº¬å¤§å­¦æ³•å­¦é™¢ã€‚ [1]__\n",
      "\n",
      "## å“ç‰ŒåŠ¨æ€\n",
      "\n",
      "æ’­æŠ¥\n",
      "\n",
      "ç¼–è¾‘\n",
      "\n",
      "2024å¹´1æœˆ11æ—¥ï¼ŒçŸ¥ä¹åœ¨åŒ—äº¬ä¸¾åŠâ€œ2024çŸ¥ä¹æ•™è‚²å¤§ä¼šâ€ã€‚ä¼šä¸Šï¼ŒçŸ¥ä¹åˆ›å§‹äººã€CEO[å‘¨æº](/item/%E5%91%A8%E6%BA%90/15897920?fromModule=lemma_inlink)è¡¨ç¤ºï¼Œä»Šå¤©çš„çŸ¥ä¹ï¼Œä¸æ­¢äºç¤¾åŒºï¼Œæ­£è½¬å˜ä¸ºä¸€ä¸ªä¸ºæ–°èŒäººæ­å»ºå¹³å°ã€æä¾›æœåŠ¡çš„å…¬å¸ï¼Œæ»¡è¶³æ–°èŒäººåŒ…å«èŒä¸šæ•™è‚²åœ¨å†…çš„å„ç±»éœ€æ±‚ï¼›çŸ¥ä¹é«˜çº§å‰¯æ€»è£ã€çŸ¥ä¹çŸ¥å­¦å ‚CEOå¼ è£ä¹è¡¨ç¤ºï¼ŒçŸ¥ä¹çŸ¥å­¦å ‚å·²ç»æˆä¸ºå›½å†…è¦†ç›–å“ç±»æœ€å…¨çš„èŒä¸šæ•™è‚²å†…å®¹ä½“ç³»ä¹‹ä¸€ï¼Œæœªæ¥å°†æŒç»­æ¨åŠ¨æ•°å­—åŒ–èµ‹èƒ½ï¼Œç”¨ç§‘æŠ€é‡å¡‘èŒä¸šæ•™è‚²è¡Œä¸šï¼Œå¹¶é€šè¿‡å¼€æ”¾åä½œå’Œäº§æ•™èåˆï¼ŒåŠ©åŠ›èŒä¸šæ•™è‚²å®ç°é«˜è´¨é‡å°±ä¸šã€‚\n",
      "[2]__\n",
      "\n",
      "[![](https://bkimg.cdn.bcebos.com/pic/8ad4b31c8701a18b87d6e56ba777100828381f3050c4?x-bce-\n",
      "process=image/format,f_auto/resize,m_lfit,limit_1,w_440)å¤§ä¼šå˜‰å®¾è§‚ç‚¹(5å¼ )](/pic/%E7%9F%A5%E4%B9%8E%E7%9F%A5%E5%AD%A6%E5%A0%82/64028760/5168228231/8ad4b31c8701a18b87d6e56ba777100828381f3050c4?fr=lemma&fromModule=lemma_content-\n",
      "image \"å¤§ä¼šå˜‰å®¾è§‚ç‚¹\")\n",
      "\n",
      "æ–°æ‰‹ä¸Šè·¯\n",
      "\n",
      "[æˆé•¿ä»»åŠ¡](/usercenter/tasks#guide)[ç¼–è¾‘å…¥é—¨](/help#main01)[ç¼–è¾‘è§„åˆ™](/help#main06)[æœ¬äººç¼–è¾‘![new](https://baikebcs.bdimg.com/baike-\n",
      "react/common/new.png)](/item/ç™¾åº¦ç™¾ç§‘ï¼šæœ¬äººè¯æ¡ç¼–è¾‘æœåŠ¡/22442459?bk_fr=pcFooter)\n",
      "\n",
      "æˆ‘æœ‰ç–‘é—®\n",
      "\n",
      "å†…å®¹è´¨ç–‘[åœ¨çº¿å®¢æœ](https://ufosdk.baidu.com/bailingPC/getEntryPath/aKo-\n",
      "PBP84zlnBbgvj2STxzZNwlrmWT8XVd-\n",
      "PzIQ3C-c=)[å®˜æ–¹è´´å§](http://tieba.baidu.com/f?ie=utf-8&fr=bks0000&kw=%E7%99%BE%E5%BA%A6%E7%99%BE%E7%A7%91)æ„è§åé¦ˆ\n",
      "\n",
      "æŠ•è¯‰å»ºè®®\n",
      "\n",
      "[ä¸¾æŠ¥ä¸è‰¯ä¿¡æ¯](http://help.baidu.com/newadd?prod_id=10&category=1)[æœªé€šè¿‡è¯æ¡ç”³è¯‰](http://help.baidu.com/newadd?prod_id=10&category=2)[æŠ•è¯‰ä¾µæƒä¿¡æ¯](http://help.baidu.com/newadd?prod_id=10&category=6)[å°ç¦æŸ¥è¯¢ä¸è§£å°](http://help.baidu.com/newadd?prod_id=10&category=5)\n",
      "\n",
      "Â©2025 Baidu [ä½¿ç”¨ç™¾åº¦å‰å¿…è¯»](http://www.baidu.com/duty/) | [ç™¾ç§‘åè®®](http://help.baidu.com/question?prod_en=baike&class=89&id=1637) | [éšç§æ”¿ç­–](http://help.baidu.com/question?prod_id=10&class=690&id=1001779) | [ç™¾åº¦ç™¾ç§‘åˆä½œå¹³å°](/operation/cooperation) | [äº¬ICPè¯030173å· ](https://beian.miit.gov.cn/)![](https://ss0.bdstatic.com/5aV1bjqh_Q23odCf/static/superman/img/copy_rignt_24.png)\n",
      "\n",
      "[\n",
      "__äº¬å…¬ç½‘å®‰å¤‡11000002000001å·](http://www.beian.gov.cn/portal/registerSystemInfo?recordcode=11000002000001)\n"
     ]
    }
   ],
   "source": [
    "from llama_index.readers.web import SimpleWebPageReader\n",
    "\n",
    "documents = SimpleWebPageReader(html_to_text=True).load_data(\n",
    "    [\"https://baike.baidu.com/item/%E7%9F%A5%E4%B9%8E%E7%9F%A5%E5%AD%A6%E5%A0%82/64028760\"]\n",
    ")\n",
    "\n",
    "print(documents[0].text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02a88893-01b8-44a1-9bed-e93e60cec328",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "<b>æ›´å¤š Data Connectors</b>\n",
    "    <ul>\n",
    "        <li>å†…ç½®çš„<a href=\"https://llamahub.ai/l/readers/llama-index-readers-file\">æ–‡ä»¶åŠ è½½å™¨</a></li>\n",
    "        <li>è¿æ¥ä¸‰æ–¹æœåŠ¡çš„<a href=\"https://docs.llamaindex.ai/en/stable/module_guides/loading/connector/modules/\">æ•°æ®åŠ è½½å™¨</a>ï¼Œä¾‹å¦‚æ•°æ®åº“</li>\n",
    "        <li>æ›´å¤šåŠ è½½å™¨å¯ä»¥åœ¨ <a href=\"https://llamahub.ai/\">LlamaHub</a> ä¸Šæ‰¾åˆ°</li>\n",
    "    </ul>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36ed8a42-f6fc-430f-9e92-07736e7f359c",
   "metadata": {},
   "source": [
    "## 4ã€æ–‡æœ¬åˆ‡åˆ†ä¸è§£æï¼ˆChunkingï¼‰\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39ad7ebd-9e56-47f9-8136-2e1098139c01",
   "metadata": {},
   "source": [
    "ä¸ºæ–¹ä¾¿æ£€ç´¢ï¼Œæˆ‘ä»¬é€šå¸¸æŠŠ `Document` åˆ‡åˆ†ä¸º `Node`ã€‚\n",
    "\n",
    "åœ¨ LlamaIndex ä¸­ï¼Œ`Node` è¢«å®šä¹‰ä¸ºä¸€ä¸ªæ–‡æœ¬çš„ã€Œchunkã€ã€‚\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4881c7d9-9107-4704-b322-d3bc34af96f2",
   "metadata": {},
   "source": [
    "### 4.1ã€ä½¿ç”¨ TextSplitters å¯¹æ–‡æœ¬åšåˆ‡åˆ†\n",
    "\n",
    "ä¾‹å¦‚ï¼š`TokenTextSplitter` æŒ‰æŒ‡å®š token æ•°åˆ‡åˆ†æ–‡æœ¬\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "69a083f7-cda9-45d9-be3e-397ce866e440",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import Document\n",
    "from llama_index.core.node_parser import TokenTextSplitter\n",
    "\n",
    "node_parser = TokenTextSplitter(\n",
    "    chunk_size=1000,  # æ¯ä¸ª chunk çš„æœ€å¤§é•¿åº¦\n",
    "    chunk_overlap=500  # chunk ä¹‹é—´é‡å é•¿åº¦ \n",
    ")\n",
    "\n",
    "nodes = node_parser.get_nodes_from_documents(\n",
    "    documents, show_progress=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "81b363de-c5ef-4e20-ad4d-39bccabb3369",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"id_\": \"33ac9624-6d4b-4894-afbf-67c7bc1d0257\",\n",
      "    \"embedding\": null,\n",
      "    \"metadata\": {},\n",
      "    \"excluded_embed_metadata_keys\": [],\n",
      "    \"excluded_llm_metadata_keys\": [],\n",
      "    \"relationships\": {\n",
      "        \"1\": {\n",
      "            \"node_id\": \"https://baike.baidu.com/item/%E7%9F%A5%E4%B9%8E%E7%9F%A5%E5%AD%A6%E5%A0%82/64028760\",\n",
      "            \"node_type\": \"4\",\n",
      "            \"metadata\": {},\n",
      "            \"hash\": \"eacbba43d1c8799964f5b2ac4b82de6fdc0571d0bf2e0c21fe0bf27649ada42c\",\n",
      "            \"class_name\": \"RelatedNodeInfo\"\n",
      "        },\n",
      "        \"2\": {\n",
      "            \"node_id\": \"7704db4e-fa23-4289-a1a5-5ea1fb024fe4\",\n",
      "            \"node_type\": \"1\",\n",
      "            \"metadata\": {},\n",
      "            \"hash\": \"facf5469a06bfabcff6e70288290aee614bf116a9eb4e33b85bfc70e14fcb2bd\",\n",
      "            \"class_name\": \"RelatedNodeInfo\"\n",
      "        },\n",
      "        \"3\": {\n",
      "            \"node_id\": \"d9a63cad-8b9e-442b-819e-df1a25155ef3\",\n",
      "            \"node_type\": \"1\",\n",
      "            \"metadata\": {},\n",
      "            \"hash\": \"90219c2d62580d8e83fcb60370b640246a46d6e4817ffe622c1289dbfe1e97ae\",\n",
      "            \"class_name\": \"RelatedNodeInfo\"\n",
      "        }\n",
      "    },\n",
      "    \"metadata_template\": \"{key}: {value}\",\n",
      "    \"metadata_separator\": \"\\n\",\n",
      "    \"text\": \"ç§’æ‡‚ç™¾ç§‘\\n\\n    \\n\\nç‰¹è‰²ç™¾ç§‘\\n\\n    \\n\\nçŸ¥è¯†ä¸“é¢˜\\n\\n    \\n\\nåŠ å…¥ç™¾ç§‘\\n\\n    \\n\\nç™¾ç§‘å›¢é˜Ÿ\\n\\n    \\n\\næƒå¨åˆä½œ\\n\\n    \\n\\n[ä¸ªäººä¸­å¿ƒ](/usercenter)\\n\\næ”¶è—\\n\\næŸ¥çœ‹[æˆ‘çš„æ”¶è—](/uc/favolemma)\\n\\n0æœ‰ç”¨+1\\n\\n0\\n\\n# çŸ¥ä¹çŸ¥å­¦å ‚\\n\\næ’­æŠ¥[è®¨è®º](/planet/talk?lemmaId=64028760&fromModule=lemma_right-issue-btn)ä¸Šä¼ è§†é¢‘\\n\\nçŸ¥ä¹æ——ä¸‹èŒä¸šæ•™è‚²å“ç‰Œ\\n\\nçŸ¥ä¹çŸ¥å­¦å ‚æ˜¯çŸ¥ä¹æ——ä¸‹çš„èŒä¸šæ•™è‚²å“ç‰Œï¼Œäº2022å¹´12æœˆ30æ—¥æ­£å¼å‘å¸ƒã€‚çŸ¥ä¹çŸ¥å­¦å ‚ä¾æ‰˜è‡ªèº«ç§‘æŠ€å®åŠ›ï¼Œä¸“æ³¨äºç”¨æˆ·èŒä¸šå‘å±•ï¼Œèšé›†å„è¡Œä¸šä¼˜è´¨æ•™è‚²èµ„æºï¼Œæ‰“é€ äº†ä¸€ç«™å¼åœ¨çº¿èŒä¸šæ•™è‚²å¹³å°ã€‚\\n[1]__æˆªè‡³2023å¹´åº•\\nï¼ŒçŸ¥ä¹çŸ¥å­¦å ‚é›†åˆäº†å“èŒæ•™è‚²ã€è¶´è¶´æ•™è‚²ã€MBAå¤§å¸ˆã€ä¸€èµ·å…¬è€ƒã€ä¸€èµ·è€ƒæ•™å¸ˆã€AGIè¯¾å ‚6ä¸ªå­å“ç‰Œï¼Œå®Œæˆäº†6å¤§é¢†åŸŸ30å¤šä¸ªå“ç±»è¯¾ç¨‹çš„è¦†ç›–ï¼Œç´¯è®¡ä¸ºè¶…è¿‡2000ä¸‡å­¦å‘˜æä¾›èŒä¸šåŸ¹è®­æœåŠ¡ã€‚\\n[2]__\\n\\n2024å¹´1æœˆ11æ—¥ï¼ŒçŸ¥ä¹çŸ¥å­¦å ‚å®£å¸ƒæ­£å¼ç‹¬ç«‹è¿è¥ï¼Œå¹¶å…¬å¸ƒäº†æœªæ¥ä¸¤å¤§æ ¸å¿ƒæˆ˜ç•¥ï¼šç¬¬ä¸€ï¼Œé€šè¿‡æ•°å­—åŒ–èµ‹èƒ½ï¼Œæ”¹å–„èŒä¸šæ•™è‚²æœåŠ¡ä½“ç³»å’Œäº¤ä»˜æ ‡å‡†ï¼›ç¬¬äºŒï¼Œé€šè¿‡å¼€æ”¾åä½œå’Œäº§æ•™èåˆï¼ŒåŠ©åŠ›èŒä¸šæ•™è‚²å®ç°é«˜è´¨é‡å°±ä¸šã€‚\\n[2]__\\n\\nä¸­æ–‡å\\n\\n     çŸ¥ä¹çŸ¥å­¦å ‚\\n\\næ‰€å±è¡Œä¸š\\n\\n    èŒä¸šæ•™è‚²\\n\\nåˆ›ç«‹æ—¶é—´\\n\\n    2022å¹´12æœˆ30æ—¥\\n\\næ‰€å±å…¬å¸\\n\\n    çŸ¥ä¹\\n\\nå“ç‰Œå£å·\\n\\n    æœªæ¥èŒä¸šä¹‹è·¯\\n\\nå•†æ ‡æ³¨å†Œå·\\n\\n    59736985 [3]__\\n\\n## ç›®å½•\\n\\n  1. 1å‘å±•å†ç¨‹\\n  2. 2ä¸šåŠ¡çŸ©é˜µ\\n  3. 3æ——ä¸‹å“ç‰Œ\\n  4. 4éƒ¨åˆ†å¸ˆèµ„\\n  5. 5å“ç‰ŒåŠ¨æ€\\n\\n## å‘å±•å†ç¨‹\\n\\næ’­æŠ¥\\n\\nç¼–è¾‘\\n\\n2022å¹´6æœˆ29æ—¥ï¼ŒçŸ¥ä¹æ¨å‡ºâ€œå­¦ä¹ â€ä¸“åŒºï¼Œä¸Šçº¿æ•°ç™¾é—¨å…ç²¾ç½ªå¢“è´¹æ­£ç‰ˆæˆæƒè¯¾ç¨‹ã€‚ [4]__\\n\\nå·´è¾¾æ‰2022å¹´11æœˆ1æ—¥ï¼ŒçŸ¥ä¹ä¸Šçº¿AIå¤§å¸ˆå…è´¹è¯¾ç¨‹ï¼Œå´æ©è¾¾ã€æ²ˆå‘å©†æ—‹æ´‹é¢†è¡”æ­ç§˜äººå·¥æ™ºèƒ½ã€‚ [5]__\\n\\n2022å¹´12æœˆ30æ—¥ï¼ŒçŸ¥ä¹å‘å¸ƒèŒä¸šæ•™è‚²å“ç‰Œâ€”â€”åœ¨çº¿èŒä¸šæ•™è‚²å¹³å°ã€ŒçŸ¥ä¹çŸ¥å­¦å ‚ã€ã€‚ [6]__\\n\\n2023å¹´2æœˆ21æ—¥ï¼ŒçŸ¥ä¹å¦ç³Šè”åˆä¸­å›½æ•™è‚²åœ¨çº¿å‘å¸ƒè€ƒç ”æ•°æ®æŠ¥å‘Šï¼Œç«™å†…è€ƒç ”å…´è¶£ç”¨æˆ·1594ä¸‡ã€‚ [7]__\\n\\n2023å¹´7æœˆ15æ—¥ï¼ŒçŸ¥ä¹å¹´åº¦æ´»åŠ¨ã€Œ2023å¹´æ–°çŸ¥é’å¹´å¤§ä¼šã€åœ¨åŒ—äº¬ä¸¾è¡Œã€‚çŸ¥ä¹åˆ›å§‹äººã€è‘£äº‹é•¿å…¼ CEO\\nå‘¨æºå‡ºå¸­å¹¶å‘è¡¨æ¼”è®²å°†èŒä¸šæ•™è‚²ä½œç¢‘æ—¬ç…§ä¸ºç¬¬äºŒå¢é•¿æ›²çº¿ï¼ŒåŒæ—¶ä¹Ÿå®£å¸ƒã€ŒçŸ¥å­¦è®¡åˆ’\",\n",
      "    \"mimetype\": \"text/plain\",\n",
      "    \"start_char_idx\": 1013,\n",
      "    \"end_char_idx\": 2024,\n",
      "    \"metadata_seperator\": \"\\n\",\n",
      "    \"text_template\": \"{metadata_str}\\n\\n{content}\",\n",
      "    \"class_name\": \"TextNode\"\n",
      "}\n",
      "{\n",
      "    \"id_\": \"d9a63cad-8b9e-442b-819e-df1a25155ef3\",\n",
      "    \"embedding\": null,\n",
      "    \"metadata\": {},\n",
      "    \"excluded_embed_metadata_keys\": [],\n",
      "    \"excluded_llm_metadata_keys\": [],\n",
      "    \"relationships\": {\n",
      "        \"1\": {\n",
      "            \"node_id\": \"https://baike.baidu.com/item/%E7%9F%A5%E4%B9%8E%E7%9F%A5%E5%AD%A6%E5%A0%82/64028760\",\n",
      "            \"node_type\": \"4\",\n",
      "            \"metadata\": {},\n",
      "            \"hash\": \"eacbba43d1c8799964f5b2ac4b82de6fdc0571d0bf2e0c21fe0bf27649ada42c\",\n",
      "            \"class_name\": \"RelatedNodeInfo\"\n",
      "        },\n",
      "        \"2\": {\n",
      "            \"node_id\": \"33ac9624-6d4b-4894-afbf-67c7bc1d0257\",\n",
      "            \"node_type\": \"1\",\n",
      "            \"metadata\": {},\n",
      "            \"hash\": \"3d70a560f1015648e7840183cc71f72e028dffed6a22f33e0b3cd52c9a27febd\",\n",
      "            \"class_name\": \"RelatedNodeInfo\"\n",
      "        },\n",
      "        \"3\": {\n",
      "            \"node_id\": \"d46243fb-c175-4f0c-a6f6-ab38723a14f7\",\n",
      "            \"node_type\": \"1\",\n",
      "            \"metadata\": {},\n",
      "            \"hash\": \"54e597ca35d3611ff0085643b5a8f41aa88be906e08b05b8da443c0d9086eae3\",\n",
      "            \"class_name\": \"RelatedNodeInfo\"\n",
      "        }\n",
      "    },\n",
      "    \"metadata_template\": \"{key}: {value}\",\n",
      "    \"metadata_separator\": \"\\n\",\n",
      "    \"text\": \"çŸ¥ä¹çŸ¥å­¦å ‚\\n\\næ‰€å±è¡Œä¸š\\n\\n    èŒä¸šæ•™è‚²\\n\\nåˆ›ç«‹æ—¶é—´\\n\\n    2022å¹´12æœˆ30æ—¥\\n\\næ‰€å±å…¬å¸\\n\\n    çŸ¥ä¹\\n\\nå“ç‰Œå£å·\\n\\n    æœªæ¥èŒä¸šä¹‹è·¯\\n\\nå•†æ ‡æ³¨å†Œå·\\n\\n    59736985 [3]__\\n\\n## ç›®å½•\\n\\n  1. 1å‘å±•å†ç¨‹\\n  2. 2ä¸šåŠ¡çŸ©é˜µ\\n  3. 3æ——ä¸‹å“ç‰Œ\\n  4. 4éƒ¨åˆ†å¸ˆèµ„\\n  5. 5å“ç‰ŒåŠ¨æ€\\n\\n## å‘å±•å†ç¨‹\\n\\næ’­æŠ¥\\n\\nç¼–è¾‘\\n\\n2022å¹´6æœˆ29æ—¥ï¼ŒçŸ¥ä¹æ¨å‡ºâ€œå­¦ä¹ â€ä¸“åŒºï¼Œä¸Šçº¿æ•°ç™¾é—¨å…ç²¾ç½ªå¢“è´¹æ­£ç‰ˆæˆæƒè¯¾ç¨‹ã€‚ [4]__\\n\\nå·´è¾¾æ‰2022å¹´11æœˆ1æ—¥ï¼ŒçŸ¥ä¹ä¸Šçº¿AIå¤§å¸ˆå…è´¹è¯¾ç¨‹ï¼Œå´æ©è¾¾ã€æ²ˆå‘å©†æ—‹æ´‹é¢†è¡”æ­ç§˜äººå·¥æ™ºèƒ½ã€‚ [5]__\\n\\n2022å¹´12æœˆ30æ—¥ï¼ŒçŸ¥ä¹å‘å¸ƒèŒä¸šæ•™è‚²å“ç‰Œâ€”â€”åœ¨çº¿èŒä¸šæ•™è‚²å¹³å°ã€ŒçŸ¥ä¹çŸ¥å­¦å ‚ã€ã€‚ [6]__\\n\\n2023å¹´2æœˆ21æ—¥ï¼ŒçŸ¥ä¹å¦ç³Šè”åˆä¸­å›½æ•™è‚²åœ¨çº¿å‘å¸ƒè€ƒç ”æ•°æ®æŠ¥å‘Šï¼Œç«™å†…è€ƒç ”å…´è¶£ç”¨æˆ·1594ä¸‡ã€‚ [7]__\\n\\n2023å¹´7æœˆ15æ—¥ï¼ŒçŸ¥ä¹å¹´åº¦æ´»åŠ¨ã€Œ2023å¹´æ–°çŸ¥é’å¹´å¤§ä¼šã€åœ¨åŒ—äº¬ä¸¾è¡Œã€‚çŸ¥ä¹åˆ›å§‹äººã€è‘£äº‹é•¿å…¼ CEO\\nå‘¨æºå‡ºå¸­å¹¶å‘è¡¨æ¼”è®²å°†èŒä¸šæ•™è‚²ä½œç¢‘æ—¬ç…§ä¸ºç¬¬äºŒå¢é•¿æ›²çº¿ï¼ŒåŒæ—¶ä¹Ÿå®£å¸ƒã€ŒçŸ¥å­¦è®¡åˆ’ ã€å·²è¿›å…¥å†…æµ‹é˜¶æ®µï¼Œå³å°†å¯¹æ›´å¤šæœ‰å¿—äºæä¾›ä¸“ä¸šèŒä¸šæ•™è‚²æœåŠ¡çš„æœºæ„å’Œä¸ªäººçŠè½¿è¾£å¼€æ”¾ã€‚\\n[8]__\\n\\n2023å¹´11æœˆ29æ—¥ï¼ŒçŸ¥ä¹å‘å¸ƒ2023å¹´Q3è´¢æŠ¥ï¼ŒèŒä¸šæ•™è‚²ä¸šåŠ¡è¥æ”¶è¾¾åˆ°1.æ¢ç»„æ±‚45äº¿å…ƒï¼Œå®ç°åŒæ¯”å¢é•¿85.6%ã€‚ [9]__\\n\\n2024å¹´1æœˆ11æ—¥æ¶ˆæ¯ï¼ŒçŸ¥ä¹åœ¨åŒ—äº¬ä¸¾åŠã€Œ2024çŸ¥ä¹æ•™è‚²å¤§ä¼šã€ï¼Œå®£å¸ƒèŒä¸šæ•™è‚²å“ç‰Œã€ŒçŸ¥ä¹çŸ¥å­¦å ‚è…¿é¡µæ±‚ä¹Œã€æ­£å¼ç‹¬ç«‹è¿è¥ã€‚ [2]__\\n\\n##\",\n",
      "    \"mimetype\": \"text/plain\",\n",
      "    \"start_char_idx\": 1550,\n",
      "    \"end_char_idx\": 2207,\n",
      "    \"metadata_seperator\": \"\\n\",\n",
      "    \"text_template\": \"{metadata_str}\\n\\n{content}\",\n",
      "    \"class_name\": \"TextNode\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "show_json(nodes[1].json())\n",
    "show_json(nodes[2].json())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1f56dbf-4247-4a06-a127-2fa1929455f0",
   "metadata": {},
   "source": [
    "LlamaIndex æä¾›äº†ä¸°å¯Œçš„ `TextSplitter`ï¼Œä¾‹å¦‚ï¼š\n",
    "\n",
    "- [`SentenceSplitter`](https://docs.llamaindex.ai/en/stable/api_reference/node_parsers/sentence_splitter/)ï¼šåœ¨åˆ‡åˆ†æŒ‡å®šé•¿åº¦çš„ chunk åŒæ—¶å°½é‡ä¿è¯å¥å­è¾¹ç•Œä¸è¢«åˆ‡æ–­ï¼›\n",
    "- [`CodeSplitter`](https://docs.llamaindex.ai/en/stable/api_reference/node_parsers/code/)ï¼šæ ¹æ® ASTï¼ˆç¼–è¯‘å™¨çš„æŠ½è±¡å¥æ³•æ ‘ï¼‰åˆ‡åˆ†ä»£ç ï¼Œä¿è¯ä»£ç åŠŸèƒ½ç‰‡æ®µå®Œæ•´ï¼›\n",
    "- [`SemanticSplitterNodeParser`](https://docs.llamaindex.ai/en/stable/api_reference/node_parsers/semantic_splitter/)ï¼šæ ¹æ®è¯­ä¹‰ç›¸å…³æ€§å¯¹å°†æ–‡æœ¬åˆ‡åˆ†ä¸ºç‰‡æ®µã€‚\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b692f9fb-af99-4bf5-9d4e-c745438173d6",
   "metadata": {},
   "source": [
    "### 4.2ã€ä½¿ç”¨ NodeParsers å¯¹æœ‰ç»“æ„çš„æ–‡æ¡£åšè§£æ\n",
    "\n",
    "ä¾‹å¦‚ï¼š`HTMLNodeParser`è§£æ HTML æ–‡æ¡£\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b7c6f533-a203-42a2-b312-3d89e0cbf22a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "æ”¶è—\n",
      "\n",
      "0\n",
      "æœ‰ç”¨+1\n",
      "\n",
      "\n",
      "æ’­æŠ¥\n",
      "çŸ¥ä¹çŸ¥å­¦å ‚æ˜¯çŸ¥ä¹æ——ä¸‹çš„èŒä¸šæ•™è‚²å“ç‰Œï¼Œäº2022å¹´12æœˆ30æ—¥æ­£å¼å‘å¸ƒã€‚çŸ¥ä¹çŸ¥å­¦å ‚ä¾æ‰˜è‡ªèº«ç§‘æŠ€å®åŠ›ï¼Œä¸“æ³¨äºç”¨æˆ·èŒä¸šå‘å±•ï¼Œèšé›†å„è¡Œä¸šä¼˜è´¨æ•™è‚²èµ„æºï¼Œæ‰“é€ äº†ä¸€ç«™å¼åœ¨çº¿èŒä¸šæ•™è‚²å¹³å°ã€‚\n",
      "[1]\n",
      "æˆªè‡³2023å¹´åº•\n",
      "ï¼ŒçŸ¥ä¹çŸ¥å­¦å ‚é›†åˆäº†å“èŒæ•™è‚²ã€è¶´è¶´æ•™è‚²ã€MBAå¤§å¸ˆã€ä¸€èµ·å…¬è€ƒã€ä¸€èµ·è€ƒæ•™å¸ˆã€AGIè¯¾å ‚6ä¸ªå­å“ç‰Œï¼Œå®Œæˆäº†6å¤§é¢†åŸŸ30å¤šä¸ªå“ç±»è¯¾ç¨‹çš„è¦†ç›–ï¼Œç´¯è®¡ä¸ºè¶…è¿‡2000ä¸‡å­¦å‘˜æä¾›èŒä¸šåŸ¹è®­æœåŠ¡ã€‚\n",
      "[2]\n",
      "2024å¹´1æœˆ11æ—¥ï¼ŒçŸ¥ä¹çŸ¥å­¦å ‚å®£å¸ƒæ­£å¼ç‹¬ç«‹è¿è¥ï¼Œå¹¶å…¬å¸ƒäº†æœªæ¥ä¸¤å¤§æ ¸å¿ƒæˆ˜ç•¥ï¼šç¬¬ä¸€ï¼Œé€šè¿‡æ•°å­—åŒ–èµ‹èƒ½ï¼Œæ”¹å–„èŒä¸šæ•™è‚²æœåŠ¡ä½“ç³»å’Œäº¤ä»˜æ ‡å‡†ï¼›ç¬¬äºŒï¼Œé€šè¿‡å¼€æ”¾åä½œå’Œäº§æ•™èåˆï¼ŒåŠ©åŠ›èŒä¸šæ•™è‚²å®ç°é«˜è´¨é‡å°±ä¸šã€‚\n",
      "[2]\n",
      "çŸ¥ä¹çŸ¥å­¦å ‚\n",
      "èŒä¸šæ•™è‚²\n",
      "2022å¹´12æœˆ30æ—¥\n",
      "çŸ¥ä¹\n",
      "æœªæ¥èŒä¸šä¹‹è·¯\n",
      "59736985\n",
      "[3]\n",
      "1\n",
      "å‘å±•å†ç¨‹\n",
      "2\n",
      "ä¸šåŠ¡çŸ©é˜µ\n",
      "3\n",
      "æ——ä¸‹å“ç‰Œ\n",
      "4\n",
      "éƒ¨åˆ†å¸ˆèµ„\n",
      "5\n",
      "å“ç‰ŒåŠ¨æ€\n",
      "\n",
      "\n",
      "æ’­æŠ¥\n",
      "ç¼–è¾‘\n",
      "2022å¹´6æœˆ29æ—¥ï¼ŒçŸ¥ä¹æ¨å‡ºâ€œå­¦ä¹ â€ä¸“åŒºï¼Œä¸Šçº¿æ•°ç™¾ç¢‘æ—¬ç…§é—¨å…è´¹æ­£ç‰ˆæˆæƒè¯¾ç¨‹ã€‚\n",
      "[4]\n",
      "2022å¹´11æœˆ1æ—¥ï¼ŒçŸ¥ä¹ä¸Šçº¿AIå¤§å¸ˆå…è´¹è¯¾ç¨‹ï¼Œå´æ©è¾¾å©†æ—‹ã€æ²ˆå‘æ´‹é¢†è¡”æ­ç§˜äººå·¥æ™ºèƒ½ã€‚\n",
      "[5]\n",
      "2022å¹´12æœˆ30æ—¥ï¼ŒçŸ¥ä¹å‘å¸ƒèŒä¸šæ•™è‚²å“ç‰Œâ€”â€”åœ¨çº¿èŒä¸šæ•™è‚²å¹³å°ã€ŒçŸ¥æŒ¨ç½ªä¹çŸ¥å­¦å ‚ã€ã€‚\n",
      "[6]\n",
      "2023å¹´2æœˆ21æ—¥ï¼ŒçŸ¥ä¹è”åˆä¸­å›½æ•™è‚²åœ¨çº¿å‘å¸ƒè€ƒç ”æ•°æ®æŠ¥å‘Šï¼Œç«™å†…è€ƒç ”å…´è¶£ç”¨æˆ·1594ä¸‡ã€‚\n",
      "[7]\n",
      "2023å¹´7æœˆ15æ—¥ï¼ŒçŸ¥ä¹å¹´åº¦æ´»åŠ¨ã€Œ2023å¹´æ–°çŸ¥é’å¹´å¤§ä¼šæ•…å…†æ°‘ã€åœ¨åŒ—äº¬ä¸¾è¡Œã€‚çŸ¥ä¹åˆ›å§‹äººã€è‘£äº‹é•¿å…¼ CEO å‘¨æºå‡ºå¸­å¹¶å‘è¡¨æ¼”è®²å°†èŒä¸šæ•™è‚²ä½œä¸ºç¬¬äºŒæ¢ç»„æ±‚å¢é•¿æ›²çº¿é¡¹èˆ¹è…Šï¼ŒåŒæ—¶ä¹Ÿå®£å¸ƒã€ŒçŸ¥å­¦è®¡åˆ’ ã€å·²è¿›å…¥å†…æµ‹é˜¶æ®µï¼Œå³å°†å¯¹æ›´å¤šæœ‰å¿—äºæä¾›ä¸“ä¸šèŒä¸šæ•™è‚²æœåŠ¡çš„æœºæ„å’Œä¸ªäººå¼€æ”¾ã€‚\n",
      "[8]\n",
      "2023å¹´11æœˆ29æ—¥ï¼ŒçŸ¥ä¹å‘å¸ƒ2023å¹´Q3è´¢æŠ¥ï¼ŒèŒä¸šæ•™è‚²ä¸šåŠ¡è¥æ”¶è¾¾ç¯®åŸ‹è…Šåˆ°1.45äº¿å…ƒï¼Œå®ç°åŒæ¯”å¢é•¿85.6%ã€‚\n",
      "[9]\n",
      "2024å¹´1æœˆ11æ—¥æ¶ˆæ¯ï¼ŒçŸ¥ä¹åœ¨åŒ—äº¬ä¸¾åŠã€Œ2024çŸ¥ä¹æ•™è‚²å¤§ä¼šã€ï¼Œå®£å¸ƒèŒä¸šæ•™è‚²å“ç‰Œã€ŒçŸ¥ä¹çŸ¥å­¦å ‚ã€æ­£å¼ç‹¬ç«‹è¿ç…§å°è‰°ä¹è¥ã€‚\n",
      "[2]\n",
      "\n",
      "\n",
      "æ’­æŠ¥\n",
      "ç¼–è¾‘\n",
      "çŸ¥ä¹çŸ¥å­¦å ‚å·²ç»é›†åˆäº†å“èŒæ•™è‚²ã€è¶´è¶´æ•™è‚²ã€MBAå¤§å¸ˆã€ä¸€èµ·å…¬è€ƒã€ä¸€èµ·è€ƒæ•™å¸ˆã€AGIè¯¾å ‚6ä¸ªå­å“ç‰Œï¼Œå®Œæˆäº†6å¤§é¢†åŸŸ30å¤šä¸ªå“ç±»çš„è¦†ç›–ï¼Œç´¯è®¡ä¸ºè¶…è¿‡2000ä¸‡å­¦å‘˜æä¾›èŒä¸šåŸ¹è®­æœåŠ¡ï¼Œæˆä¸ºå›½å†…è¦†ç›–å“ç±»æœ€å…¨çš„èŒä¸šæ•™è‚²å†…å®¹ä½“ç³»ä¹‹ä¸€ã€‚åœ¨è€ƒç ”ã€å…¬è€ƒã€æ•™èµ„ã€CPAã€CFAã€MBAã€èŒåœºæŠ€èƒ½ç­‰è¯¸å¤šè¯¾ç¨‹èµ›é“ä¸­ï¼ŒçŸ¥ä¹éƒ½å½¢æˆäº†ä¸€å®šçš„ç”¨æˆ·å£ç¢‘å’Œå†…å®¹å½±å“åŠ›ã€‚\n",
      "[2]\n",
      "\n",
      "çŸ¥ä¹çŸ¥å­¦å ‚å­¦ç§‘å“ç±»\n",
      "[1]\n",
      "\n",
      "\n",
      "æ’­æŠ¥\n",
      "ç¼–è¾‘\n",
      "è¶´è¶´æ•™è‚²ï¼šçŸ¥ä¹çŸ¥å­¦å ‚æ——ä¸‹ä¸“æ³¨ç•™å­¦è€ƒè¯•åŸ¹è®­ä»¥åŠç•™å­¦ç”³è¯·çš„ä¸€ç«™å¼æœåŠ¡æœºæ„ï¼Œå¼€è®¾æœ‰é›…æ€ã€æ‰˜ç¦ã€GREã€GMAT ç­‰è¯¾ç¨‹äº§å“ï¼Œç´¯è®¡ä¸ºç™¾ä¸‡å­¦å‘˜æä¾›è¿‡ç•™å­¦è¯­åŸ¹æœåŠ¡ã€‚\n",
      "[1]\n",
      "å“èŒæ•™è‚²ï¼šçŸ¥ä¹çŸ¥å­¦å ‚æ——ä¸‹ä¸“æ³¨é‡‘èèŒä¸šåŸ¹è®­çš„å­å“ç‰Œï¼Œæä¾›åŒ…æ‹¬ CFAã€ESGã€FRMã€CPAã€é‡‘èè€ƒç ”ç­‰å¤šæ¬¾è´¢ç»æ•™è‚²ç±»è¯¾ç¨‹ï¼Œå·²ç´¯è®¡æœåŠ¡è€ƒç”Ÿ 13000+ã€CFA æŒè¯å­¦å‘˜ 2000+ï¼Œå¹¶ä¸º ESG é¢†åŸŸè¾“é€äººæ‰ 4000+ã€‚\n",
      "[1]\n",
      "MBAå¤§å¸ˆï¼šçŸ¥ä¹çŸ¥å­¦å ‚æ——ä¸‹ä¸“æ³¨ç®¡ç†ç±»è”è€ƒåŸ¹è®­çš„åœ¨çº¿æ•™è‚²å“ç‰Œï¼Œä¸ºè€ƒç”Ÿæä¾›ç²¾å“è¯¾ç¨‹ã€ä¸“å±æ•™æåŠä¸€ç«™å¼å¤‡è€ƒæœåŠ¡ã€‚ã€ŒMBAå¤§å¸ˆã€å§‹ç»ˆè‡´åŠ›äºä¸ºç”¨æˆ·æä¾›æ™ºèƒ½åŒ–ã€ä¸ªæ€§åŒ–ã€ç³»ç»ŸåŒ–çš„åœ¨çº¿å­¦ä¹ ä½“éªŒï¼Œç´¯è®¡å·²æœåŠ¡ç®¡ç†ç±»è”è€ƒè€ƒç”Ÿè¶…è¿‡ 300 ä¸‡äººã€‚\n",
      "[1]\n",
      "ä¸€èµ·è€ƒæ•™å¸ˆï¼šçŸ¥ä¹çŸ¥å­¦å ‚æ——ä¸‹ä¸“æ³¨æ•™å¸ˆèµ„æ ¼è¯å’Œæ•™å¸ˆæ‹›è˜è€ƒè¯•åŸ¹è®­çš„æ•™è‚²å“ç‰Œï¼Œç´¯è®¡ä¸ºè¶…è¿‡ 1500 ä¸‡å¤‡è€ƒæ•™å¸ˆèµ„æ ¼è¯å’Œæ•™å¸ˆæ‹›è˜çš„è€ƒç”Ÿæä¾›è¿‡æœåŠ¡ã€‚\n",
      "[1]\n",
      "ä¸€èµ·å…¬è€ƒï¼šçŸ¥ä¹çŸ¥å­¦å ‚æ——ä¸‹ä¸“æ³¨äºå…¬åŠ¡å‘˜å’Œäº‹ä¸šå•ä½è€ƒè¯•åŸ¹è®­çš„æ•™è‚²å“ç‰Œï¼Œç´¯è®¡ä¸ºè¶…è¿‡ 200 ä¸‡å¤‡è€ƒå…¬åŠ¡å‘˜åŠäº‹ä¸šå•ä½è€ƒè¯•çš„ç”¨æˆ·æä¾›è¿‡æœåŠ¡ã€‚\n",
      "[1]\n",
      "AGIè¯¾å ‚ï¼šçŸ¥ä¹çŸ¥å­¦å ‚æ——ä¸‹ä¸“æ³¨äººå·¥æ™ºèƒ½æŠ€æœ¯åŸ¹è®­çš„æ•™è‚²å“ç‰Œï¼Œæä¾›å¤§æ¨¡å‹ç®—æ³•åŸç†ã€å¤§æ¨¡å‹æŠ€æœ¯åº”ç”¨ã€å¤§æ¨¡å‹ä¸šåŠ¡è§£å†³æ–¹æ¡ˆç­‰å‰æ²¿ AI è¯¾ç¨‹ã€‚ã€ŒAGIè¯¾å ‚ã€å§‹ç»ˆè‡´åŠ›äºç»“åˆæ—¶ä¸‹çƒ­é—¨æŠ€æœ¯è·¯çº¿åŠé«˜é¢‘åº”ç”¨åœºæ™¯ï¼Œä¸ºç”¨æˆ·æä¾›ä¸€ç«™å¼äººå·¥æ™ºèƒ½æ•™è‚²è§£å†³æ–¹æ¡ˆã€‚\n",
      "[1]\n",
      "\n",
      "çŸ¥ä¹çŸ¥å­¦å ‚æ——ä¸‹å“ç‰Œ\n",
      "[2]\n",
      "\n",
      "\n",
      "æ’­æŠ¥\n",
      "ç¼–è¾‘\n",
      "è´¾å²š     è€ƒç ”è‹±è¯­èµ„æ·±åå¸ˆã€ä¸­å›½äººæ°‘å¤§å­¦è‹±è¯­è¯­è¨€æ–‡å­¦ç¡•å£«ã€‚\n",
      "[1]\n",
      "ç‹ç‘æ©  ä¸­ç¾ä¸¤å›½æ³•å¾‹èŒä¸šèµ„æ ¼è¯æŒè¯äººã€æœ¬ç§‘æ¯•ä¸šäºæ¸…åå¤§å­¦ã€‚\n",
      "[1]\n",
      "è”¡é‡‘é¾™  å…¬è€ƒèµ„æ·±åå¸ˆã€17å¹´æ‰§æ•™è¡Œä¸šç»éªŒã€æ¸…åå¤§å­¦æœ¬ç¡•ã€‚\n",
      "[1]\n",
      "ææ–¯å…‹\n",
      "CFA/FRMèµ„æ·±åå¸ˆã€ä¸Šæµ·äº¤é€šå¤§å­¦æœ¬ç¡•ã€‚\n",
      "[1]\n",
      "Paul Winterbottom   å‰é›…æ€å£è¯­å†™ä½œè‹±ç±è€ƒå®˜ã€ç‰›æ´¥å¤§å­¦æœ¬ç¡•ã€‚\n",
      "[1]\n",
      "è–›ç¿\n",
      "ç®¡ç†ç±»è”è€ƒèµ„æ·±åå¸ˆã€è¥¿å®‰äº¤å¤§æœ¬ç§‘åŠMBAã€åˆ©å…¹ç¡•å£«ã€‚\n",
      "[1]\n",
      "å¼ å€©     å‰çº¢åœˆæ‰€å¾‹å¸ˆã€æ¯•ä¸šäºåŒ—äº¬å¤§å­¦æ³•å­¦é™¢ã€‚\n",
      "[1]\n",
      "\n",
      "\n",
      "æ’­æŠ¥\n",
      "ç¼–è¾‘\n",
      "2024å¹´1æœˆ11æ—¥ï¼ŒçŸ¥ä¹åœ¨åŒ—äº¬ä¸¾åŠâ€œ2024çŸ¥ä¹æ•™è‚²å¤§ä¼šâ€ã€‚ä¼šä¸Šï¼ŒçŸ¥ä¹åˆ›å§‹äººã€CEO\n",
      "å‘¨æº\n",
      "è¡¨ç¤ºï¼Œä»Šå¤©çš„çŸ¥ä¹ï¼Œä¸æ­¢äºç¤¾åŒºï¼Œæ­£è½¬å˜ä¸ºä¸€ä¸ªä¸ºæ–°èŒäººæ­å»ºå¹³å°ã€æä¾›æœåŠ¡çš„å…¬å¸ï¼Œæ»¡è¶³æ–°èŒäººåŒ…å«èŒä¸šæ•™è‚²åœ¨å†…çš„å„ç±»éœ€æ±‚ï¼›çŸ¥ä¹é«˜çº§å‰¯æ€»è£ã€çŸ¥ä¹çŸ¥å­¦å ‚CEOå¼ è£ä¹è¡¨ç¤ºï¼ŒçŸ¥ä¹çŸ¥å­¦å ‚å·²ç»æˆä¸ºå›½å†…è¦†ç›–å“ç±»æœ€å…¨çš„èŒä¸šæ•™è‚²å†…å®¹ä½“ç³»ä¹‹ä¸€ï¼Œæœªæ¥å°†æŒç»­æ¨åŠ¨æ•°å­—åŒ–èµ‹èƒ½ï¼Œç”¨ç§‘æŠ€é‡å¡‘èŒä¸šæ•™è‚²è¡Œä¸šï¼Œå¹¶é€šè¿‡å¼€æ”¾åä½œå’Œäº§æ•™èåˆï¼ŒåŠ©åŠ›èŒä¸šæ•™è‚²å®ç°é«˜è´¨é‡å°±ä¸šã€‚\n",
      "[2]\n",
      "å¤§ä¼šå˜‰å®¾è§‚ç‚¹\n",
      "(5å¼ )\n",
      "æ–°æ‰‹ä¸Šè·¯\n",
      "æˆ‘æœ‰ç–‘é—®\n",
      "æŠ•è¯‰å»ºè®®\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core.node_parser import HTMLNodeParser\n",
    "from llama_index.readers.web import SimpleWebPageReader\n",
    "\n",
    "documents = SimpleWebPageReader(html_to_text=False).load_data(\n",
    "    [\"https://baike.baidu.com/item/%E7%9F%A5%E4%B9%8E%E7%9F%A5%E5%AD%A6%E5%A0%82/64028760\"]\n",
    ")\n",
    "\n",
    "# é»˜è®¤è§£æ [\"p\", \"h1\", \"h2\", \"h3\", \"h4\", \"h5\", \"h6\", \"li\", \"b\", \"i\", \"u\", \"section\"]\n",
    "parser = HTMLNodeParser(tags=[\"span\"])  # å¯ä»¥è‡ªå®šä¹‰è§£æå“ªäº›æ ‡ç­¾\n",
    "nodes = parser.get_nodes_from_documents(documents)\n",
    "\n",
    "for node in nodes:\n",
    "    print(node.text+\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bdc613c-36a8-4afb-871b-097496703eaf",
   "metadata": {},
   "source": [
    "æ›´å¤šçš„ `NodeParser` åŒ…æ‹¬ [`MarkdownNodeParser`](https://docs.llamaindex.ai/en/stable/api_reference/node_parsers/markdown/)ï¼Œ[`JSONNodeParser`](https://docs.llamaindex.ai/en/stable/api_reference/node_parsers/json/)ç­‰ç­‰ã€‚\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8b8557f-20af-477d-918d-14761a9c986d",
   "metadata": {},
   "source": [
    "## 5ã€ç´¢å¼•ï¼ˆIndexingï¼‰ä¸æ£€ç´¢ï¼ˆRetrievalï¼‰\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df338b16-df37-412d-a385-2d1f4b681112",
   "metadata": {},
   "source": [
    "**åŸºç¡€æ¦‚å¿µ**ï¼šåœ¨ã€Œæ£€ç´¢ã€ç›¸å…³çš„ä¸Šä¸‹æ–‡ä¸­ï¼Œã€Œç´¢å¼•ã€å³`index`ï¼Œ é€šå¸¸æ˜¯æŒ‡ä¸ºäº†å®ç°å¿«é€Ÿæ£€ç´¢è€Œè®¾è®¡çš„ç‰¹å®šã€Œæ•°æ®ç»“æ„ã€ã€‚\n",
    "\n",
    "ç´¢å¼•çš„å…·ä½“åŸç†ä¸å®ç°ä¸æ˜¯æœ¬è¯¾ç¨‹çš„æ•™å­¦é‡ç‚¹ï¼Œæ„Ÿå…´è¶£çš„åŒå­¦å¯ä»¥å‚è€ƒï¼š[ä¼ ç»Ÿç´¢å¼•](https://en.wikipedia.org/wiki/Search_engine_indexing)ã€[å‘é‡ç´¢å¼•](https://medium.com/kx-systems/vector-indexing-a-roadmap-for-vector-databases-65866f07daf5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dd397fe-8932-49de-ac37-bed0b585205c",
   "metadata": {},
   "source": [
    "### 5.1ã€å‘é‡æ£€ç´¢\n",
    "\n",
    "1. `SimpleVectorStore` ç›´æ¥åœ¨å†…å­˜ä¸­æ„å»ºä¸€ä¸ª Vector Store å¹¶å»ºç´¢å¼•\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "4ea17e80-d25c-43ac-b9b5-983c6acb3adb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "it surpasses these models in Chinese factual knowledge (Chinese\n",
      "SimpleQA), highlighting its strength in Chinese factual knowledge.\n",
      "â€¢ Code, Math, and Reasoning: (1) DeepSeek-V3 achieves state-of-the-art performance on\n",
      "math-related benchmarks among all non-long-CoT open-source and closed-source models.\n",
      "Notably, it even outperforms o1-preview on specific benchmarks, such as MATH-500,\n",
      "demonstrating its robust mathematical reasoning capabilities. (2) On coding-related tasks,\n",
      "DeepSeek-V3 emerges as the top-performing model for coding competition benchmarks,\n",
      "such as LiveCodeBench, solidifying its position as the leading model in this domain. For\n",
      "engineering-related tasks, while DeepSeek-V3 performs slightly below Claude-Sonnet-3.5,\n",
      "it still outpaces all other models by a significant margin, demonstrating its competitiveness\n",
      "across diverse technical benchmarks.\n",
      "In the remainder of this paper, we first present a detailed exposition of our DeepSeek-V3\n",
      "model architecture (Section 2). Subsequently, we introduce our infrastructures, encompassing\n",
      "our compute clusters, the training framework, the support for FP8 training, the inference\n",
      "deployment strategy, and our suggestions on future hardware design. Next, we describe our\n",
      "pre-training process, including the construction of training data, hyper-parameter settings, long-\n",
      "context extension techniques, the associated evaluations, as well as some discussions (Section 4).\n",
      "Thereafter, we discuss our efforts on post-training, which include Supervised Fine-Tuning (SFT),\n",
      "Reinforcement Learning (RL), the corresponding evaluations, and discussions (Section 5). Lastly,\n",
      "we conclude this work, discuss existing limitations of DeepSeek-V3, and propose potential\n",
      "directions for future research (Section 6).\n",
      "2. Architecture\n",
      "We first introduce the basic architecture of DeepSeek-V3, featured by Multi-head Latent Atten-\n",
      "tion (MLA) (DeepSeek-AI, 2024c) for efficient inference and DeepSeekMoE (Dai et al., 2024)\n",
      "for economical training. Then, we present a Multi-Token Prediction (MTP) training objective,\n",
      "which we have observed to enhance the overall performance on evaluation benchmarks. For\n",
      "other minor details not explicitly mentioned,\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core import VectorStoreIndex, SimpleDirectoryReader\n",
    "from llama_index.core.node_parser import TokenTextSplitter\n",
    "\n",
    "# åŠ è½½ pdf æ–‡æ¡£\n",
    "documents = SimpleDirectoryReader(\n",
    "    \"./data\", \n",
    "    required_exts=[\".pdf\"],\n",
    ").load_data()\n",
    "\n",
    "# å®šä¹‰ Node Parser\n",
    "node_parser = TokenTextSplitter(chunk_size=500, chunk_overlap=250)\n",
    "\n",
    "# åˆ‡åˆ†æ–‡æ¡£\n",
    "nodes = node_parser.get_nodes_from_documents(documents)\n",
    "\n",
    "# æ„å»º index\n",
    "index = VectorStoreIndex(nodes)\n",
    "\n",
    "# è·å– retriever\n",
    "vector_retriever = index.as_retriever(\n",
    "    similarity_top_k=2 # è¿”å›2ä¸ªç»“æœ\n",
    ")\n",
    "\n",
    "# æ£€ç´¢\n",
    "results = vector_retriever.retrieve(\"deepseek v3æ•°å­¦èƒ½åŠ›æ€ä¹ˆæ ·\")\n",
    "\n",
    "print(results[0].text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaeca7fd-5606-45a4-8443-ddab6b9e413f",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "<p>LlamaIndex é»˜è®¤çš„ Embedding æ¨¡å‹æ˜¯ <code>OpenAIEmbedding(model=\"text-embedding-ada-002\")</code>ã€‚</p>\n",
    "<p>å¦‚ä½•æ›¿æ¢æŒ‡å®šçš„ Embedding æ¨¡å‹è§åé¢ç« èŠ‚è¯¦è§£ã€‚</p>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90ebec20-7c12-4d2d-a4f4-5cb2abcc5f32",
   "metadata": {},
   "source": [
    "2. ä½¿ç”¨è‡ªå®šä¹‰çš„ Vector Storeï¼Œä»¥ `Qdrant` ä¸ºä¾‹ï¼š\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b5e7648-0598-4df7-923f-42fe2f172da3",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install llama-index-vector-stores-qdrant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "a65a38da-3e71-4fa8-872d-eff6b4f3856b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Node ID: c6f4bdf6-ee82-4a19-b238-ab43cc8f53f3\n",
      "Text: it surpasses these models in Chinese factual knowledge (Chinese\n",
      "SimpleQA), highlighting its strength in Chinese factual knowledge. â€¢\n",
      "Code, Math, and Reasoning: (1) DeepSeek-V3 achieves state-of-the-art\n",
      "performance on math-related benchmarks among all non-long-CoT open-\n",
      "source and closed-source models. Notably, it even outperforms\n",
      "o1-preview on sp...\n",
      "Score:  0.834\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core.indices.vector_store.base import VectorStoreIndex\n",
    "from llama_index.vector_stores.qdrant import QdrantVectorStore\n",
    "from llama_index.core import StorageContext\n",
    "\n",
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.models import VectorParams, Distance\n",
    "\n",
    "client = QdrantClient(location=\":memory:\")\n",
    "collection_name = \"demo\"\n",
    "collection = client.create_collection(\n",
    "    collection_name=collection_name,\n",
    "    vectors_config=VectorParams(size=1536, distance=Distance.COSINE)\n",
    ")\n",
    "\n",
    "vector_store = QdrantVectorStore(client=client, collection_name=collection_name)\n",
    "# storage: æŒ‡å®šå­˜å‚¨ç©ºé—´\n",
    "storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
    "\n",
    "# åˆ›å»º indexï¼šé€šè¿‡ Storage Context å…³è”åˆ°è‡ªå®šä¹‰çš„ Vector Store\n",
    "index = VectorStoreIndex(nodes, storage_context=storage_context)\n",
    "\n",
    "# è·å– retriever\n",
    "vector_retriever = index.as_retriever(similarity_top_k=1)\n",
    "\n",
    "# æ£€ç´¢\n",
    "results = vector_retriever.retrieve(\"deepseek v3æ•°å­¦èƒ½åŠ›æ€ä¹ˆæ ·\")\n",
    "\n",
    "print(results[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9913e5d5-7c84-4833-b21d-2fe440749a63",
   "metadata": {},
   "source": [
    "### 5.2ã€æ›´å¤šç´¢å¼•ä¸æ£€ç´¢æ–¹å¼\n",
    "\n",
    "LlamaIndex å†…ç½®äº†ä¸°å¯Œçš„æ£€ç´¢æœºåˆ¶ï¼Œä¾‹å¦‚ï¼š\n",
    "\n",
    "- å…³é”®å­—æ£€ç´¢\n",
    "\n",
    "  - [`BM25Retriever`](https://docs.llamaindex.ai/en/stable/api_reference/retrievers/bm25/)ï¼šåŸºäº tokenizer å®ç°çš„ BM25 ç»å…¸æ£€ç´¢ç®—æ³•\n",
    "  - [`KeywordTableGPTRetriever`](https://docs.llamaindex.ai/en/stable/api_reference/retrievers/keyword/#llama_index.core.indices.keyword_table.retrievers.KeywordTableGPTRetriever)ï¼šä½¿ç”¨ GPT æå–æ£€ç´¢å…³é”®å­—\n",
    "  - [`KeywordTableSimpleRetriever`](https://docs.llamaindex.ai/en/stable/api_reference/retrievers/keyword/#llama_index.core.indices.keyword_table.retrievers.KeywordTableSimpleRetriever)ï¼šä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼æå–æ£€ç´¢å…³é”®å­—\n",
    "  - [`KeywordTableRAKERetriever`](https://docs.llamaindex.ai/en/stable/api_reference/retrievers/keyword/#llama_index.core.indices.keyword_table.retrievers.KeywordTableRAKERetriever)ï¼šä½¿ç”¨[`RAKE`](https://pypi.org/project/rake-nltk/)ç®—æ³•æå–æ£€ç´¢å…³é”®å­—ï¼ˆæœ‰è¯­è¨€é™åˆ¶ï¼‰\n",
    "\n",
    "- RAG-Fusion [`QueryFusionRetriever`](https://docs.llamaindex.ai/en/stable/api_reference/retrievers/query_fusion/)\n",
    "\n",
    "- è¿˜æ”¯æŒ [KnowledgeGraph](https://docs.llamaindex.ai/en/stable/api_reference/retrievers/knowledge_graph/)ã€[SQL](https://docs.llamaindex.ai/en/stable/api_reference/retrievers/sql/#llama_index.core.retrievers.SQLRetriever)ã€[Text-to-SQL](https://docs.llamaindex.ai/en/stable/api_reference/retrievers/sql/#llama_index.core.retrievers.NLSQLRetriever) ç­‰ç­‰\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af53e99b-2ab5-4520-ba96-4a9979a94480",
   "metadata": {},
   "source": [
    "### 5.3ã€æ£€ç´¢åå¤„ç†\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13c53c7c-9077-42bc-a5c0-832688b352b8",
   "metadata": {},
   "source": [
    "LlamaIndex çš„ `Node Postprocessors` æä¾›äº†ä¸€ç³»åˆ—æ£€ç´¢åå¤„ç†æ¨¡å—ã€‚\n",
    "\n",
    "ä¾‹å¦‚ï¼šæˆ‘ä»¬å¯ä»¥ç”¨ä¸åŒæ¨¡å‹å¯¹æ£€ç´¢åçš„ `Nodes` åšé‡æ’åº\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "7176f29c-be6b-491c-b2e8-6e604ad201b4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0] pipeline bubbles and hides most of the communication during training through\n",
      "computation-communication overlap. This overlap ensures that, as the model further scales up,\n",
      "as long as we maintain a constant computation-to-communication ratio, we can still employ\n",
      "fine-grained experts across nodes while achieving a near-zero all-to-all communication overhead.\n",
      "In addition, we also develop efficient cross-node all-to-all communication kernels to fully utilize\n",
      "InfiniBand (IB) and NVLink bandwidths. Furthermore, we meticulously optimize the memory\n",
      "footprint, making it possible to train DeepSeek-V3 without using costly tensor parallelism.\n",
      "Combining these efforts, we achieve high training efficiency.\n",
      "During pre-training, we train DeepSeek-V3 on 14.8T high-quality and diverse tokens. The\n",
      "pre-training process is remarkably stable. Throughout the entire training process, we did not\n",
      "encounter any irrecoverable loss spikes or have to roll back. Next, we conduct a two-stage\n",
      "context length extension for DeepSeek-V3. In the first stage, the maximum context length is\n",
      "extended to 32K, and in the second stage, it is further extended to 128K. Following this, we\n",
      "conduct post-training, including Supervised Fine-Tuning (SFT) and Reinforcement Learning (RL)\n",
      "on the base model of DeepSeek-V3, to align it with human preferences and further unlock its\n",
      "potential. During the post-training stage, we distill the reasoning capability from the DeepSeek-\n",
      "R1 series of models, and meanwhile carefully maintain the balance between model accuracy\n",
      "4\n",
      "\n",
      "[1] Training Costs Pre-Training Context Extension Post-Training Total\n",
      "in H800 GPU Hours 2664K 119K 5K 2788K\n",
      "in USD $5.328M $0.238M $0.01M $5.576M\n",
      "Table 1 |Training costs of DeepSeek-V3, assuming the rental price of H800 is $2 per GPU hour.\n",
      "and generation length.\n",
      "We evaluate DeepSeek-V3 on a comprehensive array of benchmarks. Despite its economical\n",
      "training costs, comprehensive evaluations reveal that DeepSeek-V3-Base has emerged as the\n",
      "strongest open-source base model currently available, especially in code and math. Its chat\n",
      "version also outperforms other open-source models and achieves performance comparable to\n",
      "leading closed-source models, including GPT-4o and Claude-3.5-Sonnet, on a series of standard\n",
      "and open-ended benchmarks.\n",
      "Lastly, we emphasize again the economical training costs of DeepSeek-V3, summarized in\n",
      "Table 1, achieved through our optimized co-design of algorithms, frameworks, and hardware.\n",
      "During the pre-training stage, training DeepSeek-V3 on each trillion tokens requires only 180K\n",
      "H800 GPU hours, i.e., 3.7 days on our cluster with 2048 H800 GPUs. Consequently, our pre-\n",
      "training stage is completed in less than two months and costs 2664K GPU hours. Combined\n",
      "with 119K GPU hours for the context length extension and 5K GPU hours for post-training,\n",
      "DeepSeek-V3 costs only 2.788M GPU hours for its full training. Assuming the rental price of\n",
      "the H800 GPU is $2 per GPU hour, our total training costs amount to only $5.576M. Note that\n",
      "the aforementioned costs include only the official training of DeepSeek-V3, excluding the costs\n",
      "associated with prior research and ablation experiments on architectures, algorithms, or data.\n",
      "Our main contribution includes:\n",
      "Architecture: Innovative Load Balancing Strategy and Training Objective\n",
      "â€¢ On top of the efficient architecture of DeepSeek-V2, we pioneer an auxiliary-loss-free\n",
      "strategy for load balancing, which minimizes the performance degradation that arises\n",
      "from encouraging load balancing.\n",
      "â€¢ We investigate a Multi-Token Prediction (MTP) objective\n",
      "\n",
      "[2] algorithms, frameworks, and hardware.\n",
      "During the pre-training stage, training DeepSeek-V3 on each trillion tokens requires only 180K\n",
      "H800 GPU hours, i.e., 3.7 days on our cluster with 2048 H800 GPUs. Consequently, our pre-\n",
      "training stage is completed in less than two months and costs 2664K GPU hours. Combined\n",
      "with 119K GPU hours for the context length extension and 5K GPU hours for post-training,\n",
      "DeepSeek-V3 costs only 2.788M GPU hours for its full training. Assuming the rental price of\n",
      "the H800 GPU is $2 per GPU hour, our total training costs amount to only $5.576M. Note that\n",
      "the aforementioned costs include only the official training of DeepSeek-V3, excluding the costs\n",
      "associated with prior research and ablation experiments on architectures, algorithms, or data.\n",
      "Our main contribution includes:\n",
      "Architecture: Innovative Load Balancing Strategy and Training Objective\n",
      "â€¢ On top of the efficient architecture of DeepSeek-V2, we pioneer an auxiliary-loss-free\n",
      "strategy for load balancing, which minimizes the performance degradation that arises\n",
      "from encouraging load balancing.\n",
      "â€¢ We investigate a Multi-Token Prediction (MTP) objective and prove it beneficial to model\n",
      "performance. It can also be used for speculative decoding for inference acceleration.\n",
      "Pre-Training: Towards Ultimate Training Efficiency\n",
      "â€¢ We design an FP8 mixed precision training framework and, for the first time, validate the\n",
      "feasibility and effectiveness of FP8 training on an extremely large-scale model.\n",
      "â€¢ Through the co-design of algorithms, frameworks, and hardware, we overcome the\n",
      "communication bottleneck in cross-node MoE training, achieving near-full computation-\n",
      "communication overlap. This significantly enhances our training efficiency and reduces the\n",
      "training costs, enabling us to further scale up the model size without additional overhead.\n",
      "â€¢ At an economical cost of only 2.664M H800 GPU hours, we complete the pre-training of\n",
      "DeepSeek-V3 on 14.8T tokens, producing the currently strongest open-source base model.\n",
      "The subsequent training stages after pre-training require only 0.1M GPU hours.\n",
      "Post-Training: Knowledge Distillation from DeepSeek-R1\n",
      "â€¢ We introduce an innovative methodology to distill reasoning capabilities from the\n",
      "\n",
      "[3] it surpasses these models in Chinese factual knowledge (Chinese\n",
      "SimpleQA), highlighting its strength in Chinese factual knowledge.\n",
      "â€¢ Code, Math, and Reasoning: (1) DeepSeek-V3 achieves state-of-the-art performance on\n",
      "math-related benchmarks among all non-long-CoT open-source and closed-source models.\n",
      "Notably, it even outperforms o1-preview on specific benchmarks, such as MATH-500,\n",
      "demonstrating its robust mathematical reasoning capabilities. (2) On coding-related tasks,\n",
      "DeepSeek-V3 emerges as the top-performing model for coding competition benchmarks,\n",
      "such as LiveCodeBench, solidifying its position as the leading model in this domain. For\n",
      "engineering-related tasks, while DeepSeek-V3 performs slightly below Claude-Sonnet-3.5,\n",
      "it still outpaces all other models by a significant margin, demonstrating its competitiveness\n",
      "across diverse technical benchmarks.\n",
      "In the remainder of this paper, we first present a detailed exposition of our DeepSeek-V3\n",
      "model architecture (Section 2). Subsequently, we introduce our infrastructures, encompassing\n",
      "our compute clusters, the training framework, the support for FP8 training, the inference\n",
      "deployment strategy, and our suggestions on future hardware design. Next, we describe our\n",
      "pre-training process, including the construction of training data, hyper-parameter settings, long-\n",
      "context extension techniques, the associated evaluations, as well as some discussions (Section 4).\n",
      "Thereafter, we discuss our efforts on post-training, which include Supervised Fine-Tuning (SFT),\n",
      "Reinforcement Learning (RL), the corresponding evaluations, and discussions (Section 5). Lastly,\n",
      "we conclude this work, discuss existing limitations of DeepSeek-V3, and propose potential\n",
      "directions for future research (Section 6).\n",
      "2. Architecture\n",
      "We first introduce the basic architecture of DeepSeek-V3, featured by Multi-head Latent Atten-\n",
      "tion (MLA) (DeepSeek-AI, 2024c) for efficient inference and DeepSeekMoE (Dai et al., 2024)\n",
      "for economical training. Then, we present a Multi-Token Prediction (MTP) training objective,\n",
      "which we have observed to enhance the overall performance on evaluation benchmarks. For\n",
      "other minor details not explicitly mentioned,\n",
      "\n",
      "[4] performance comparable to leading closed-source\n",
      "models. Despite its excellent performance, DeepSeek-V3 requires only 2.788M H800 GPU hours\n",
      "for its full training. In addition, its training process is remarkably stable. Throughout the entire\n",
      "training process, we did not experience any irrecoverable loss spikes or perform any rollbacks.\n",
      "The model checkpoints are available at https://github.com/deepseek-ai/DeepSeek-V3.\n",
      "MMLU-Pro\n",
      "(EM)\n",
      "GPQA-Diamond\n",
      "(Pass@1)\n",
      "MATH 500\n",
      "(EM)\n",
      "AIME 2024\n",
      "(Pass@1)\n",
      "Codeforces\n",
      "(Percentile)\n",
      "SWE-bench Verified\n",
      "(Resolved)\n",
      "0\n",
      "20\n",
      "40\n",
      "60\n",
      "80\n",
      "100Accuracy / Percentile (%)\n",
      "75.9\n",
      "59.1\n",
      "90.2\n",
      "39.2\n",
      "51.6\n",
      "42.0\n",
      "66.2\n",
      "41.3\n",
      "74.7\n",
      "16.7\n",
      "35.6\n",
      "22.6\n",
      "71.6\n",
      "49.0\n",
      "80.0\n",
      "23.3 24.8 23.8\n",
      "73.3\n",
      "51.1\n",
      "73.8\n",
      "23.3\n",
      "25.3 24.5\n",
      "72.6\n",
      "49.9\n",
      "74.6\n",
      "9.3\n",
      "23.6\n",
      "38.8\n",
      "78.0\n",
      "65.0\n",
      "78.3\n",
      "16.0\n",
      "20.3\n",
      "50.8\n",
      "DeepSeek-V3 DeepSeek-V2.5 Qwen2.5-72B-Inst Llama-3.1-405B-Inst GPT-4o-0513 Claude-3.5-Sonnet-1022\n",
      "Figure 1 |Benchmark performance of DeepSeek-V3 and its counterparts.\n",
      "arXiv:2412.19437v2  [cs.CL]  18 Feb 2025\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# è·å– retriever\n",
    "vector_retriever = index.as_retriever(similarity_top_k=5)\n",
    "\n",
    "# æ£€ç´¢\n",
    "nodes = vector_retriever.retrieve(\"deepseek v3éœ€è¦å¤šå°‘è®­ç»ƒèµ„æº?\")\n",
    "\n",
    "for i, node in enumerate(nodes):\n",
    "    print(f\"[{i}] {node.text}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "65bae2b4-7c36-44fa-9566-0d1b3f34972a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0] Training Costs Pre-Training Context Extension Post-Training Total\n",
      "in H800 GPU Hours 2664K 119K 5K 2788K\n",
      "in USD $5.328M $0.238M $0.01M $5.576M\n",
      "Table 1 |Training costs of DeepSeek-V3, assuming the rental price of H800 is $2 per GPU hour.\n",
      "and generation length.\n",
      "We evaluate DeepSeek-V3 on a comprehensive array of benchmarks. Despite its economical\n",
      "training costs, comprehensive evaluations reveal that DeepSeek-V3-Base has emerged as the\n",
      "strongest open-source base model currently available, especially in code and math. Its chat\n",
      "version also outperforms other open-source models and achieves performance comparable to\n",
      "leading closed-source models, including GPT-4o and Claude-3.5-Sonnet, on a series of standard\n",
      "and open-ended benchmarks.\n",
      "Lastly, we emphasize again the economical training costs of DeepSeek-V3, summarized in\n",
      "Table 1, achieved through our optimized co-design of algorithms, frameworks, and hardware.\n",
      "During the pre-training stage, training DeepSeek-V3 on each trillion tokens requires only 180K\n",
      "H800 GPU hours, i.e., 3.7 days on our cluster with 2048 H800 GPUs. Consequently, our pre-\n",
      "training stage is completed in less than two months and costs 2664K GPU hours. Combined\n",
      "with 119K GPU hours for the context length extension and 5K GPU hours for post-training,\n",
      "DeepSeek-V3 costs only 2.788M GPU hours for its full training. Assuming the rental price of\n",
      "the H800 GPU is $2 per GPU hour, our total training costs amount to only $5.576M. Note that\n",
      "the aforementioned costs include only the official training of DeepSeek-V3, excluding the costs\n",
      "associated with prior research and ablation experiments on architectures, algorithms, or data.\n",
      "Our main contribution includes:\n",
      "Architecture: Innovative Load Balancing Strategy and Training Objective\n",
      "â€¢ On top of the efficient architecture of DeepSeek-V2, we pioneer an auxiliary-loss-free\n",
      "strategy for load balancing, which minimizes the performance degradation that arises\n",
      "from encouraging load balancing.\n",
      "â€¢ We investigate a Multi-Token Prediction (MTP) objective\n",
      "[1] algorithms, frameworks, and hardware.\n",
      "During the pre-training stage, training DeepSeek-V3 on each trillion tokens requires only 180K\n",
      "H800 GPU hours, i.e., 3.7 days on our cluster with 2048 H800 GPUs. Consequently, our pre-\n",
      "training stage is completed in less than two months and costs 2664K GPU hours. Combined\n",
      "with 119K GPU hours for the context length extension and 5K GPU hours for post-training,\n",
      "DeepSeek-V3 costs only 2.788M GPU hours for its full training. Assuming the rental price of\n",
      "the H800 GPU is $2 per GPU hour, our total training costs amount to only $5.576M. Note that\n",
      "the aforementioned costs include only the official training of DeepSeek-V3, excluding the costs\n",
      "associated with prior research and ablation experiments on architectures, algorithms, or data.\n",
      "Our main contribution includes:\n",
      "Architecture: Innovative Load Balancing Strategy and Training Objective\n",
      "â€¢ On top of the efficient architecture of DeepSeek-V2, we pioneer an auxiliary-loss-free\n",
      "strategy for load balancing, which minimizes the performance degradation that arises\n",
      "from encouraging load balancing.\n",
      "â€¢ We investigate a Multi-Token Prediction (MTP) objective and prove it beneficial to model\n",
      "performance. It can also be used for speculative decoding for inference acceleration.\n",
      "Pre-Training: Towards Ultimate Training Efficiency\n",
      "â€¢ We design an FP8 mixed precision training framework and, for the first time, validate the\n",
      "feasibility and effectiveness of FP8 training on an extremely large-scale model.\n",
      "â€¢ Through the co-design of algorithms, frameworks, and hardware, we overcome the\n",
      "communication bottleneck in cross-node MoE training, achieving near-full computation-\n",
      "communication overlap. This significantly enhances our training efficiency and reduces the\n",
      "training costs, enabling us to further scale up the model size without additional overhead.\n",
      "â€¢ At an economical cost of only 2.664M H800 GPU hours, we complete the pre-training of\n",
      "DeepSeek-V3 on 14.8T tokens, producing the currently strongest open-source base model.\n",
      "The subsequent training stages after pre-training require only 0.1M GPU hours.\n",
      "Post-Training: Knowledge Distillation from DeepSeek-R1\n",
      "â€¢ We introduce an innovative methodology to distill reasoning capabilities from the\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core.postprocessor import LLMRerank\n",
    "\n",
    "postprocessor = LLMRerank(top_n=2)\n",
    "\n",
    "nodes = postprocessor.postprocess_nodes(nodes, query_str=\"deepseek v3éœ€è¦å¤šå°‘è®­ç»ƒèµ„æº?\")\n",
    "\n",
    "for i, node in enumerate(nodes):\n",
    "    print(f\"[{i}] {node.text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69ca41ea-3112-491a-a1a1-5ec8db295b61",
   "metadata": {},
   "source": [
    "æ›´å¤šçš„ Rerank åŠå…¶å®ƒåå¤„ç†æ–¹æ³•ï¼Œå‚è€ƒå®˜æ–¹æ–‡æ¡£ï¼š[Node Postprocessor Modules](https://docs.llamaindex.ai/en/stable/module_guides/querying/node_postprocessors/node_postprocessors/)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "392d8d69-cd42-445c-a0cd-0dd7c66d07bc",
   "metadata": {},
   "source": [
    "## 6ã€ç”Ÿæˆå›å¤ï¼ˆQA & Chatï¼‰\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d4248c1-602c-4df6-bd6a-72e994faf1ed",
   "metadata": {},
   "source": [
    "### 6.1ã€å•è½®é—®ç­”ï¼ˆQuery Engineï¼‰\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "261d35c3-8b54-4840-ab88-c04b348b0fb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DeepSeek-V3æœ‰671Bæ€»å‚æ•°ï¼Œæ¯ä¸ªtokenæ¿€æ´»äº†37Bå‚æ•°ã€‚\n"
     ]
    }
   ],
   "source": [
    "qa_engine = index.as_query_engine()\n",
    "response = qa_engine.query(\"deepseek v3æœ‰å¤šå°‘å‚æ•°?\")\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44d55521-5b67-4c26-a8de-440d2e1046a7",
   "metadata": {},
   "source": [
    "#### æµå¼è¾“å‡º\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "1817400e-9311-4161-89e4-507267862524",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DeepSeek-V3æœ‰671Bæ€»å‚æ•°ï¼Œæ¯ä¸ªtokenæ¿€æ´»äº†37Bå‚æ•°ã€‚"
     ]
    }
   ],
   "source": [
    "qa_engine = index.as_query_engine(streaming=True)\n",
    "response = qa_engine.query(\"deepseek v3æœ‰å¤šå°‘å‚æ•°?\")\n",
    "response.print_response_stream()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cf53205-beb2-4571-81aa-c5f8e12142f5",
   "metadata": {},
   "source": [
    "### 6.2ã€å¤šè½®å¯¹è¯ï¼ˆChat Engineï¼‰\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "81a73a48-469e-4d40-adf3-e0e5ec44305a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DeepSeek-V3 has a total of 671 billion parameters, with 37 billion parameters activated for each token.\n"
     ]
    }
   ],
   "source": [
    "chat_engine = index.as_chat_engine()\n",
    "response = chat_engine.chat(\"deepseek v3æœ‰å¤šå°‘å‚æ•°?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "712c8c71-6420-4c6f-821c-1bd1c31dc8bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "æ¯ä¸ªä»¤ç‰Œæ¿€æ´»äº†37äº¿ä¸ªå‚æ•°ã€‚\n"
     ]
    }
   ],
   "source": [
    "response = chat_engine.chat(\"æ¯æ¬¡æ¿€æ´»å¤šå°‘?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3094d188-2241-4995-9704-1d2aeb878e1e",
   "metadata": {},
   "source": [
    "#### æµå¼è¾“å‡º\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "bf33effa-ad36-4eea-b1d1-926ca5cc8dff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DeepSeek-V3æœ‰6710äº¿ä¸ªæ€»å‚æ•°ï¼Œæ¯ä¸ªæ ‡è®°æ¿€æ´»äº†370äº¿ä¸ªå‚æ•°ã€‚"
     ]
    }
   ],
   "source": [
    "chat_engine = index.as_chat_engine()\n",
    "streaming_response = chat_engine.stream_chat(\"deepseek v3æœ‰å¤šå°‘å‚æ•°?\")\n",
    "# streaming_response.print_response_stream()\n",
    "for token in streaming_response.response_gen:\n",
    "    print(token, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7eb76fa-e4ba-4c47-9785-53eadddf478e",
   "metadata": {},
   "source": [
    "## 7ã€åº•å±‚æ¥å£ï¼šPromptã€LLM ä¸ Embedding\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da0cce84-9f79-41ef-a6c4-caaeb70d029e",
   "metadata": {},
   "source": [
    "### 7.1ã€Prompt æ¨¡æ¿\n",
    "\n",
    "#### `PromptTemplate` å®šä¹‰æç¤ºè¯æ¨¡æ¿\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "41358d83-4fcb-430c-9c2e-c1ac0839cbfe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'å†™ä¸€ä¸ªå…³äºå°æ˜çš„ç¬‘è¯'"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from llama_index.core import PromptTemplate\n",
    "\n",
    "prompt = PromptTemplate(\"å†™ä¸€ä¸ªå…³äº{topic}çš„ç¬‘è¯\")\n",
    "\n",
    "prompt.format(topic=\"å°æ˜\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4a94232-e8d5-4bf2-8751-8851d924fcee",
   "metadata": {},
   "source": [
    "#### `ChatPromptTemplate` å®šä¹‰å¤šè½®æ¶ˆæ¯æ¨¡æ¿\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "edb05592-835f-4cb2-bfa0-862796c36fdf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "system: ä½ å«å°æ˜ï¼Œä½ å¿…é¡»æ ¹æ®ç”¨æˆ·æä¾›çš„ä¸Šä¸‹æ–‡å›ç­”é—®é¢˜ã€‚\n",
      "user: å·²çŸ¥ä¸Šä¸‹æ–‡ï¼š\n",
      "è¿™æ˜¯ä¸€ä¸ªæµ‹è¯•\n",
      "\n",
      "é—®é¢˜ï¼šè¿™æ˜¯ä»€ä¹ˆ\n",
      "assistant: \n"
     ]
    }
   ],
   "source": [
    "from llama_index.core.llms import ChatMessage, MessageRole\n",
    "from llama_index.core import ChatPromptTemplate\n",
    "\n",
    "chat_text_qa_msgs = [\n",
    "    ChatMessage(\n",
    "        role=MessageRole.SYSTEM,\n",
    "        content=\"ä½ å«{name}ï¼Œä½ å¿…é¡»æ ¹æ®ç”¨æˆ·æä¾›çš„ä¸Šä¸‹æ–‡å›ç­”é—®é¢˜ã€‚\",\n",
    "    ),\n",
    "    ChatMessage(\n",
    "        role=MessageRole.USER, \n",
    "        content=(\n",
    "            \"å·²çŸ¥ä¸Šä¸‹æ–‡ï¼š\\n\" \\\n",
    "            \"{context}\\n\\n\" \\\n",
    "            \"é—®é¢˜ï¼š{question}\"\n",
    "        )\n",
    "    ),\n",
    "]\n",
    "text_qa_template = ChatPromptTemplate(chat_text_qa_msgs)\n",
    "\n",
    "print(\n",
    "    text_qa_template.format(\n",
    "        name=\"å°æ˜\",\n",
    "        context=\"è¿™æ˜¯ä¸€ä¸ªæµ‹è¯•\",\n",
    "        question=\"è¿™æ˜¯ä»€ä¹ˆ\"\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c98931d9-f411-4433-9900-649f74a40b6e",
   "metadata": {},
   "source": [
    "### 7.2ã€è¯­è¨€æ¨¡å‹\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "1febbfc2-06c1-4692-9ac1-8843e5554098",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.llms.openai import OpenAI\n",
    "\n",
    "llm = OpenAI(temperature=0, model=\"gpt-4o\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "0c0d3842-32ff-4510-9853-47e7282b56e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "å°æ˜æœ‰ä¸€å¤©å»å‚åŠ ä¸€ä¸ªæ™ºåŠ›ç«èµ›ï¼Œä¸»æŒäººé—®ä»–ï¼šâ€œå°æ˜ï¼Œè¯·ç”¨â€˜å› ä¸ºâ€™å’Œâ€˜æ‰€ä»¥â€™é€ ä¸€ä¸ªå¥å­ã€‚â€\n",
      "\n",
      "å°æ˜æƒ³äº†æƒ³ï¼Œè¯´ï¼šâ€œå› ä¸ºä»Šå¤©æˆ‘æ²¡å¸¦ä½œä¸šï¼Œæ‰€ä»¥æˆ‘åªèƒ½ç«™åœ¨è¿™é‡Œå›ç­”é—®é¢˜ã€‚â€\n"
     ]
    }
   ],
   "source": [
    "response = llm.complete(prompt.format(topic=\"å°æ˜\"))\n",
    "\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d0cbe1cc-9dfc-4274-8cdd-b4dd6d3eb5bb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "æˆ‘æ˜¯ç“œç“œï¼Œä½ çš„æ™ºèƒ½åŠ©æ‰‹ã€‚æ ¹æ®ä½ æä¾›çš„ä¸Šä¸‹æ–‡ï¼Œæˆ‘ä»¬æ­£åœ¨è¿›è¡Œä¸€ä¸ªæµ‹è¯•ã€‚è¯·è®©æˆ‘çŸ¥é“å¦‚æœä½ æœ‰å…¶ä»–é—®é¢˜æˆ–éœ€è¦è¿›ä¸€æ­¥çš„å¸®åŠ©ï¼\n"
     ]
    }
   ],
   "source": [
    "response = llm.complete(\n",
    "    text_qa_template.format(\n",
    "        name=\"å°æ˜\",\n",
    "        context=\"è¿™æ˜¯ä¸€ä¸ªæµ‹è¯•\",\n",
    "        question=\"ä½ æ˜¯è°ï¼Œæˆ‘ä»¬åœ¨å¹²å˜›\"\n",
    "    )\n",
    ")\n",
    "\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4234c43f-d46c-4755-afb3-ade10c308bf8",
   "metadata": {},
   "source": [
    "#### è¿æ¥DeepSeek"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad92e176-6153-4059-a01b-e2957a052a6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install llama-index-llms-deepseek"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "511a86bd-c5eb-45e7-8e0a-7e56b2fe7f7d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CompletionResponse(text='ğŸ˜‚ ç†ŠçŒ«å»æ‰‹æœºåº—ä¹°æ‰‹æœºï¼Œåº—å‘˜çƒ­æƒ…æ¨èæœ€æ–°æ¬¾ï¼Œç†ŠçŒ«å´æ‘†æ‘†æ‰‹è¯´ï¼šâ€œç®—äº†ï¼Œè¿™æ‰‹æœºä¸é€‚åˆæˆ‘ã€‚â€  \\nåº—å‘˜ä¸è§£ï¼šâ€œä¸ºå•¥å‘€ï¼ŸåŠŸèƒ½è¶…å¼ºçš„ï¼â€  \\nç†ŠçŒ«æŒ‡äº†æŒ‡é»‘çœ¼åœˆï¼šâ€œç¾é¢œå¼€åˆ°æœ€å¤§éƒ½æ•‘ä¸äº†æˆ‘çš„é»‘çœ¼åœˆï¼Œä¹°äº†æœ‰å•¥ç”¨ï¼Ÿâ€  \\n\\nï¼ˆåº—å‘˜é»˜é»˜æŠŠç¾ç™½æ»¤é•œå¹¿å‘Šç‰Œè—åˆ°äº†èº«åï¼‰', additional_kwargs={}, raw=ChatCompletion(id='375b81e2-08c2-4601-9ae4-eea7d1bb2a45', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='ğŸ˜‚ ç†ŠçŒ«å»æ‰‹æœºåº—ä¹°æ‰‹æœºï¼Œåº—å‘˜çƒ­æƒ…æ¨èæœ€æ–°æ¬¾ï¼Œç†ŠçŒ«å´æ‘†æ‘†æ‰‹è¯´ï¼šâ€œç®—äº†ï¼Œè¿™æ‰‹æœºä¸é€‚åˆæˆ‘ã€‚â€  \\nåº—å‘˜ä¸è§£ï¼šâ€œä¸ºå•¥å‘€ï¼ŸåŠŸèƒ½è¶…å¼ºçš„ï¼â€  \\nç†ŠçŒ«æŒ‡äº†æŒ‡é»‘çœ¼åœˆï¼šâ€œç¾é¢œå¼€åˆ°æœ€å¤§éƒ½æ•‘ä¸äº†æˆ‘çš„é»‘çœ¼åœˆï¼Œä¹°äº†æœ‰å•¥ç”¨ï¼Ÿâ€  \\n\\nï¼ˆåº—å‘˜é»˜é»˜æŠŠç¾ç™½æ»¤é•œå¹¿å‘Šç‰Œè—åˆ°äº†èº«åï¼‰', refusal=None, role='assistant', audio=None, function_call=None, tool_calls=None, reasoning_content='å¥½çš„ï¼Œç”¨æˆ·è®©æˆ‘å†™ä¸ªç¬‘è¯ã€‚é¦–å…ˆï¼Œæˆ‘éœ€è¦ç¡®å®šç”¨æˆ·çš„éœ€æ±‚æ˜¯ä»€ä¹ˆã€‚ä»–ä»¬å¯èƒ½åªæ˜¯æƒ³è½»æ¾ä¸€ä¸‹ï¼Œæˆ–è€…éœ€è¦ç”¨åœ¨æŸä¸ªåœºåˆï¼Œæ¯”å¦‚æ¼”è®²ã€èšä¼šï¼Œæˆ–è€…åªæ˜¯æ—¥å¸¸å¨±ä¹ã€‚æ¥ä¸‹æ¥ï¼Œæˆ‘å¾—è€ƒè™‘ç¬‘è¯çš„ç±»å‹ï¼Œæ˜¯å†·ç¬‘è¯ã€åŒå…³è¯­ã€è¿˜æ˜¯æƒ…æ™¯å–œå‰§ç±»å‹çš„ã€‚\\n\\nç”¨æˆ·å¯èƒ½æ²¡æœ‰è¯´æ˜å…·ä½“ç±»å‹ï¼Œæ‰€ä»¥æˆ‘è¦é€‰ä¸€ä¸ªæ™®éå—æ¬¢è¿çš„ç±»å‹ã€‚åŒå…³è¯­æˆ–è€…æ—¥å¸¸ç”Ÿæ´»ç›¸å…³çš„ç¬‘è¯é€šå¸¸æ¯”è¾ƒå®‰å…¨ï¼Œå®¹æ˜“å¼•èµ·å…±é¸£ã€‚ç„¶åï¼Œæˆ‘éœ€è¦ç¡®ä¿ç¬‘è¯ä¸ä¼šæ¶‰åŠæ•æ„Ÿè¯é¢˜ï¼Œæ¯”å¦‚ç§æ—ã€å®—æ•™ã€æ”¿æ²»ç­‰ï¼Œé¿å…å†’çŠ¯ä»»ä½•äººã€‚\\n\\næ¥ä¸‹æ¥ï¼Œæ„æ€ç¬‘è¯çš„ç»“æ„ã€‚é€šå¸¸ç¬‘è¯æœ‰è®¾å®šå’Œç¬‘ç‚¹ä¸¤éƒ¨åˆ†ã€‚è®¾å®šéœ€è¦ç®€æ´æ˜äº†ï¼Œè®©å¬ä¼—èƒ½è¿…é€Ÿè¿›å…¥æƒ…å¢ƒã€‚ç¬‘ç‚¹è¦å‡ºä¹æ„æ–™ï¼Œä½†åˆåœ¨æƒ…ç†ä¹‹ä¸­ã€‚æ¯”å¦‚ï¼Œåˆ©ç”¨è°éŸ³å­—æˆ–è€…åè½¬ã€‚\\n\\næ¯”å¦‚ï¼Œå¯ä»¥æƒ³åˆ°ä¸€ä¸ªå…³äºåŠ¨ç‰©çš„ç¬‘è¯ï¼Œå› ä¸ºå®ƒä»¬æ¯”è¾ƒä¸­æ€§ï¼Œå®¹æ˜“æ¥å—ã€‚æ¯”å¦‚ç†ŠçŒ«å’ŒåŒ—æç†Šçš„å¯¹æ¯”ã€‚ç†ŠçŒ«ä¸ºä»€ä¹ˆä¸ç”¨æ™ºèƒ½æ‰‹æœºï¼Ÿç„¶ååè½¬ç­”æ¡ˆï¼Œå¯èƒ½åˆ©ç”¨å®ƒä»¬çš„é¢œè‰²ç‰¹ç‚¹ï¼Œæ¯”å¦‚é»‘çœ¼åœˆå’Œç™½æ¯›ï¼Œå¯¼è‡´è‡ªæ‹é—®é¢˜ã€‚è¿™æ ·æ—¢æœ‰è¶£åˆæ— å®³ã€‚\\n\\nç„¶åæ£€æŸ¥ç¬‘è¯æ˜¯å¦åŸåˆ›ï¼Œæœ‰æ²¡æœ‰å¬è¿‡ç±»ä¼¼çš„ã€‚è™½ç„¶å¾ˆå¤šç¬‘è¯ç»“æ„ç›¸ä¼¼ï¼Œä½†å†…å®¹ä¸Šè¦å°½é‡åˆ›æ–°ã€‚åŒæ—¶ï¼Œè¯­è¨€è¦å£è¯­åŒ–ï¼Œé¿å…å¤æ‚å¥å­ï¼Œè®©ç¬‘è¯æ›´å®¹æ˜“ç†è§£ã€‚æœ€åï¼Œç¡®è®¤ç¬‘ç‚¹æ˜¯å¦æ˜æ˜¾ï¼Œæœ‰æ²¡æœ‰å¯èƒ½è®©äººå¬ä¸æ‡‚ï¼Œå¯èƒ½éœ€è¦è°ƒæ•´ç”¨è¯æˆ–ç»“æ„ï¼Œç¡®ä¿ç¬‘ç‚¹æ¸…æ™°ã€‚\\n\\næ€»ç»“ä¸€ä¸‹ï¼Œå…ˆç¡®å®šç±»å‹ï¼Œæ„æ€æƒ…å¢ƒï¼Œåˆ¶é€ åè½¬ï¼Œæ£€æŸ¥ appropriateness å’ŒåŸåˆ›æ€§ï¼Œè°ƒæ•´è¯­è¨€è¡¨è¾¾ã€‚è¿™æ ·åº”è¯¥èƒ½å†™å‡ºç¬¦åˆç”¨æˆ·éœ€æ±‚çš„ç¬‘è¯ã€‚'))], created=1741760307, model='deepseek-reasoner', object='chat.completion', service_tier=None, system_fingerprint='fp_5417b77867_prod0311fp8', usage=CompletionUsage(completion_tokens=387, prompt_tokens=8, total_tokens=395, completion_tokens_details=CompletionTokensDetails(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=312, rejected_prediction_tokens=None), prompt_tokens_details=PromptTokensDetails(audio_tokens=None, cached_tokens=0), prompt_cache_hit_tokens=0, prompt_cache_miss_tokens=8)), logprobs=None, delta=None)"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from llama_index.llms.deepseek import DeepSeek\n",
    "\n",
    "llm = DeepSeek(model=\"deepseek-reasoner\", api_key=os.environ[\"DEEPSEEK_API_KEY\"])\n",
    "\n",
    "llm.complete(\"å†™ä¸ªç¬‘è¯\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f49f6da-d220-4c36-b697-e3b525902af7",
   "metadata": {},
   "source": [
    "#### è®¾ç½®å…¨å±€ä½¿ç”¨çš„è¯­è¨€æ¨¡å‹\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "94943bec-563c-44b5-80bb-9c0a7c05382c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import Settings\n",
    "\n",
    "Settings.llm = OpenAI(temperature=0, model=\"gpt-4o\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63dd7b76-2594-4c39-8e81-33e3c985df38",
   "metadata": {},
   "source": [
    "é™¤ OpenAI å¤–ï¼ŒLlamaIndex å·²é›†æˆå¤šä¸ªå¤§è¯­è¨€æ¨¡å‹ï¼ŒåŒ…æ‹¬äº‘æœåŠ¡ API å’Œæœ¬åœ°éƒ¨ç½² APIï¼Œè¯¦è§å®˜æ–¹æ–‡æ¡£ï¼š[Available LLM integrations](https://docs.llamaindex.ai/en/stable/module_guides/models/llms/modules/)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b110fc0b-ec6c-445b-a3c2-3ddef41d9589",
   "metadata": {},
   "source": [
    "### 7.3ã€Embedding æ¨¡å‹\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c5be1257-eb1a-4e2d-bad9-2cd002841093",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.embeddings.openai import OpenAIEmbedding\n",
    "from llama_index.core import Settings\n",
    "\n",
    "# å…¨å±€è®¾å®š\n",
    "Settings.embed_model = OpenAIEmbedding(model=\"text-embedding-3-small\", dimensions=512)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb025ce6-c065-4cf2-9e9d-5a01f04c26ee",
   "metadata": {},
   "source": [
    "LlamaIndex åŒæ ·é›†æˆäº†å¤šç§ Embedding æ¨¡å‹ï¼ŒåŒ…æ‹¬äº‘æœåŠ¡ API å’Œå¼€æºæ¨¡å‹ï¼ˆHuggingFaceï¼‰ç­‰ï¼Œè¯¦è§[å®˜æ–¹æ–‡æ¡£](https://docs.llamaindex.ai/en/stable/module_guides/models/embeddings/)ã€‚\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be91aa7d-6f3a-4387-859c-c7357d7c5d15",
   "metadata": {},
   "source": [
    "## 8ã€åŸºäº LlamaIndex å®ç°ä¸€ä¸ªåŠŸèƒ½è¾ƒå®Œæ•´çš„ RAG ç³»ç»Ÿ\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9a1b844-605e-4be2-9cae-2e9e56e46b40",
   "metadata": {},
   "source": [
    "åŠŸèƒ½è¦æ±‚ï¼š\n",
    "\n",
    "- åŠ è½½æŒ‡å®šç›®å½•çš„æ–‡ä»¶\n",
    "- æ”¯æŒ RAG-Fusion\n",
    "- ä½¿ç”¨ Qdrant å‘é‡æ•°æ®åº“ï¼Œå¹¶æŒä¹…åŒ–åˆ°æœ¬åœ°\n",
    "- æ”¯æŒæ£€ç´¢åæ’åº\n",
    "- æ”¯æŒå¤šè½®å¯¹è¯\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "90348baf-3a5e-4cc4-968d-e90f9d315495",
   "metadata": {},
   "outputs": [],
   "source": [
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.models import VectorParams, Distance\n",
    "\n",
    "EMBEDDING_DIM = 512\n",
    "COLLECTION_NAME = \"full_demo\"\n",
    "PATH = \"./qdrant_db\"\n",
    "\n",
    "client = QdrantClient(path=PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "cf485f1b-7da4-412e-a121-a7e6b5f3ece9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import VectorStoreIndex, KeywordTableIndex, SimpleDirectoryReader\n",
    "from llama_index.vector_stores.qdrant import QdrantVectorStore\n",
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "from llama_index.core.ingestion import IngestionPipeline\n",
    "from llama_index.core import Settings\n",
    "from llama_index.core import StorageContext\n",
    "from llama_index.core.postprocessor import LLMRerank\n",
    "from llama_index.core.retrievers import QueryFusionRetriever\n",
    "from llama_index.core.query_engine import RetrieverQueryEngine\n",
    "from llama_index.core.chat_engine import CondenseQuestionChatEngine\n",
    "from llama_index.llms.openai import OpenAI\n",
    "from llama_index.embeddings.openai import OpenAIEmbedding\n",
    "import time\n",
    "\n",
    "# 1. æŒ‡å®šå…¨å±€llmä¸embeddingæ¨¡å‹\n",
    "Settings.llm = OpenAI(temperature=0, model=\"gpt-4o\")\n",
    "Settings.embed_model = OpenAIEmbedding(model=\"text-embedding-3-small\", dimensions=EMBEDDING_DIM)\n",
    "# 2. æŒ‡å®šå…¨å±€æ–‡æ¡£å¤„ç†çš„ Ingestion Pipeline\n",
    "Settings.transformations = [SentenceSplitter(chunk_size=500, chunk_overlap=250)]\n",
    "\n",
    "# 3. åŠ è½½æœ¬åœ°æ–‡æ¡£\n",
    "documents = SimpleDirectoryReader(\"./data\").load_data()\n",
    "\n",
    "if client.collection_exists(collection_name=COLLECTION_NAME):\n",
    "    client.delete_collection(collection_name=COLLECTION_NAME)\n",
    "\n",
    "# 4. åˆ›å»º collection\n",
    "client.create_collection(\n",
    "    collection_name=COLLECTION_NAME,\n",
    "    vectors_config=VectorParams(size=EMBEDDING_DIM, distance=Distance.COSINE)\n",
    ")\n",
    "\n",
    "# 5. åˆ›å»º Vector Store\n",
    "vector_store = QdrantVectorStore(client=client, collection_name=COLLECTION_NAME)\n",
    "\n",
    "# 6. æŒ‡å®š Vector Store çš„ Storage ç”¨äº index\n",
    "storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
    "index = VectorStoreIndex.from_documents(\n",
    "    documents, storage_context=storage_context\n",
    ")\n",
    "\n",
    "# 7. å®šä¹‰æ£€ç´¢åæ’åºæ¨¡å‹\n",
    "reranker = LLMRerank(top_n=2)\n",
    "\n",
    "# 8. å®šä¹‰ RAG Fusion æ£€ç´¢å™¨\n",
    "fusion_retriever = QueryFusionRetriever(\n",
    "    [index.as_retriever()],\n",
    "    similarity_top_k=5, # æ£€ç´¢å¬å› top k ç»“æœ\n",
    "    num_queries=3,  # ç”Ÿæˆ query æ•°\n",
    "    use_async=False,\n",
    "    # query_gen_prompt=\"...\",  # å¯ä»¥è‡ªå®šä¹‰ query ç”Ÿæˆçš„ prompt æ¨¡æ¿\n",
    ")\n",
    "\n",
    "# 9. æ„å»ºå•è½® query engine\n",
    "query_engine = RetrieverQueryEngine.from_args(\n",
    "    fusion_retriever,\n",
    "    node_postprocessors=[reranker]\n",
    ")\n",
    "\n",
    "# 10. å¯¹è¯å¼•æ“\n",
    "chat_engine = CondenseQuestionChatEngine.from_defaults(\n",
    "    query_engine=query_engine, \n",
    "    # condense_question_prompt=... # å¯ä»¥è‡ªå®šä¹‰ chat message prompt æ¨¡æ¿\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "0124ef05-eb4b-417b-aa56-7128460162dd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "User: deepseek v3æœ‰å¤šå°‘å‚æ•°\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AI: DeepSeek-V3æ€»å…±æœ‰6710äº¿ä¸ªå‚æ•°ã€‚\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "User: æ¿€æ´»å¤šå°‘\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AI: DeepSeek-V3çš„å‚æ•°ä¸­æœ‰37Bæ˜¯è¢«æ¿€æ´»çš„ã€‚\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "User: å®ƒæ•°å­¦èƒ½åŠ›æ€ä¹ˆæ ·\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AI: DeepSeek-V3åœ¨æ•°å­¦ç›¸å…³åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºè‰²ï¼Œåœ¨æ‰€æœ‰éé•¿é“¾æ¨ç†çš„å¼€æºå’Œé—­æºæ¨¡å‹ä¸­è¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚å®ƒç”šè‡³åœ¨æŸäº›åŸºå‡†æµ‹è¯•ä¸Šè¶…è¶Šäº†o1-previewï¼Œå±•ç¤ºäº†å…¶å¼ºå¤§çš„æ•°å­¦æ¨ç†èƒ½åŠ›ã€‚\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "User: \n"
     ]
    }
   ],
   "source": [
    "while True:\n",
    "    question=input(\"User:\")\n",
    "    if question.strip() == \"\":\n",
    "        break\n",
    "    response = chat_engine.chat(question)\n",
    "    print(f\"AI: {response}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "741c79d5-5990-42ec-9c89-3e2eaa9b31ff",
   "metadata": {},
   "source": [
    "## 9ã€å·¥ä½œæµï¼ˆWorkflowï¼‰ç®€ä»‹"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "071629c4-1cbd-456c-bc55-ab38e4bedc7c",
   "metadata": {},
   "source": [
    "å·¥ä½œæµé¡¾åæ€ä¹‰æ˜¯å¯¹ä¸€äº›åˆ—å·¥ä½œæ­¥éª¤çš„æŠ½è±¡ã€‚\n",
    "\n",
    "LlamaIndex çš„å·¥ä½œæµæ˜¯äº‹ä»¶ï¼ˆ`event`ï¼‰é©±åŠ¨çš„ï¼š\n",
    "\n",
    "- å·¥ä½œæµç”± `step` ç»„æˆ\n",
    "- æ¯ä¸ª `step` å¤„ç†ç‰¹å®šçš„äº‹ä»¶\n",
    "- `step` ä¹Ÿä¼šäº§ç”Ÿæ–°çš„äº‹ä»¶ï¼ˆäº¤ç”±åç»§çš„ `step` è¿›è¡Œå¤„ç†ï¼‰\n",
    "- ç›´åˆ°äº§ç”Ÿ `StopEvent` æ•´ä¸ªå·¥ä½œæµç»“æŸ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f99b8edc-2764-4f07-ad2e-1e8c549b8157",
   "metadata": {},
   "source": [
    "ä¸¾ä¸ªå…·ä½“çš„ä¾‹å­ï¼šç”¨è‡ªç„¶è¯­è¨€æŸ¥è¯¢æ•°æ®åº“ï¼Œæ•°æ®åº“ä¸­åŒ…å«å¤šå¼ è¡¨\n",
    "\n",
    "å·¥ä½œæµï¼š\n",
    "\n",
    "<img src=\"workflow.png\" alt=\"å·¥ä½œæµ\" width=\"600\"/>\n",
    "\n",
    "åˆ†æ­¥è¯´æ˜ï¼š\n",
    "\n",
    "1. ç”¨æˆ·è¾“å…¥è‡ªç„¶è¯­è¨€æŸ¥è¯¢\n",
    "2. ç³»ç»Ÿå…ˆå»æ£€ç´¢è·ŸæŸ¥è¯¢ç›¸å…³çš„è¡¨\n",
    "3. æ ¹æ®è¡¨çš„ Schema è®©å¤§æ¨¡å‹ç”Ÿæˆ SQL\n",
    "4. ç”¨ç”Ÿæˆçš„ SQL æŸ¥è¯¢æ•°æ®åº“\n",
    "5. æ ¹æ®æŸ¥è¯¢ç»“æœï¼Œè°ƒç”¨å¤§æ¨¡å‹ç”Ÿæˆè‡ªç„¶è¯­è¨€å›å¤"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2be28368-98b7-4dcf-8cca-a05f443cb66b",
   "metadata": {},
   "source": [
    "### 9.1 æ•°æ®å‡†å¤‡"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb7487c6-f529-4987-b069-1e96d20303ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ä¸‹è½½ WikiTableQuestions\n",
    "# WikiTableQuestions æ˜¯ä¸€ä¸ªä¸ºè¡¨æ ¼é—®ç­”è®¾è®¡çš„æ•°æ®é›†ã€‚å…¶ä¸­åŒ…å« 2,108 ä¸ªä»ç»´åŸºç™¾ç§‘æå–çš„ HTML è¡¨æ ¼\n",
    "\n",
    "# !wget \"https://github.com/ppasupat/WikiTableQuestions/releases/download/v1.0.2/WikiTableQuestions-1.0.2-compact.zip\" -O wiki_data.zip\n",
    "# !unzip wiki_data.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b7dfdbf-c064-4575-b59e-6562e86aff21",
   "metadata": {},
   "source": [
    "1. éå†ç›®å½•åŠ è½½è¡¨æ ¼"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "dd4661dd-856a-4476-b771-1164fecadf47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing file: WikiTableQuestions/csv/200-csv/0.csv\n",
      "processing file: WikiTableQuestions/csv/200-csv/1.csv\n",
      "processing file: WikiTableQuestions/csv/200-csv/10.csv\n",
      "processing file: WikiTableQuestions/csv/200-csv/11.csv\n",
      "processing file: WikiTableQuestions/csv/200-csv/12.csv\n",
      "processing file: WikiTableQuestions/csv/200-csv/14.csv\n",
      "processing file: WikiTableQuestions/csv/200-csv/15.csv\n",
      "Error parsing WikiTableQuestions/csv/200-csv/15.csv: Error tokenizing data. C error: Expected 4 fields in line 16, saw 5\n",
      "\n",
      "processing file: WikiTableQuestions/csv/200-csv/17.csv\n",
      "Error parsing WikiTableQuestions/csv/200-csv/17.csv: Error tokenizing data. C error: Expected 6 fields in line 5, saw 7\n",
      "\n",
      "processing file: WikiTableQuestions/csv/200-csv/18.csv\n",
      "processing file: WikiTableQuestions/csv/200-csv/20.csv\n",
      "processing file: WikiTableQuestions/csv/200-csv/22.csv\n",
      "processing file: WikiTableQuestions/csv/200-csv/24.csv\n",
      "processing file: WikiTableQuestions/csv/200-csv/25.csv\n",
      "processing file: WikiTableQuestions/csv/200-csv/26.csv\n",
      "processing file: WikiTableQuestions/csv/200-csv/28.csv\n",
      "processing file: WikiTableQuestions/csv/200-csv/29.csv\n",
      "processing file: WikiTableQuestions/csv/200-csv/3.csv\n",
      "processing file: WikiTableQuestions/csv/200-csv/30.csv\n",
      "processing file: WikiTableQuestions/csv/200-csv/31.csv\n",
      "processing file: WikiTableQuestions/csv/200-csv/32.csv\n",
      "processing file: WikiTableQuestions/csv/200-csv/33.csv\n",
      "processing file: WikiTableQuestions/csv/200-csv/34.csv\n",
      "Error parsing WikiTableQuestions/csv/200-csv/34.csv: Error tokenizing data. C error: Expected 4 fields in line 6, saw 13\n",
      "\n",
      "processing file: WikiTableQuestions/csv/200-csv/35.csv\n",
      "processing file: WikiTableQuestions/csv/200-csv/36.csv\n",
      "processing file: WikiTableQuestions/csv/200-csv/37.csv\n",
      "processing file: WikiTableQuestions/csv/200-csv/38.csv\n",
      "processing file: WikiTableQuestions/csv/200-csv/4.csv\n",
      "processing file: WikiTableQuestions/csv/200-csv/41.csv\n",
      "processing file: WikiTableQuestions/csv/200-csv/42.csv\n",
      "processing file: WikiTableQuestions/csv/200-csv/44.csv\n",
      "processing file: WikiTableQuestions/csv/200-csv/45.csv\n",
      "processing file: WikiTableQuestions/csv/200-csv/46.csv\n",
      "processing file: WikiTableQuestions/csv/200-csv/47.csv\n",
      "processing file: WikiTableQuestions/csv/200-csv/48.csv\n",
      "processing file: WikiTableQuestions/csv/200-csv/7.csv\n",
      "processing file: WikiTableQuestions/csv/200-csv/8.csv\n",
      "processing file: WikiTableQuestions/csv/200-csv/9.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "data_dir = Path(\"./WikiTableQuestions/csv/200-csv\")\n",
    "csv_files = sorted([f for f in data_dir.glob(\"*.csv\")])\n",
    "dfs = []\n",
    "for csv_file in csv_files:\n",
    "    print(f\"processing file: {csv_file}\")\n",
    "    try:\n",
    "        df = pd.read_csv(csv_file)\n",
    "        dfs.append(df)\n",
    "    except Exception as e:\n",
    "        print(f\"Error parsing {csv_file}: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f7e7eef-d81a-4dd2-bcfd-1fb5dd89603a",
   "metadata": {},
   "source": [
    "2. ä¸ºæ¯ä¸ªè¡¨ç”Ÿæˆä¸€æ®µæ–‡å­—è¡¨è¿°ï¼ˆç”¨äºæ£€ç´¢ï¼‰ï¼Œä¿å­˜åœ¨ `WikiTableQuestions_TableInfo` ç›®å½•"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "25c1a118-6f5e-470a-bb02-54fd5f1497ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.prompts import ChatPromptTemplate\n",
    "from llama_index.core.bridge.pydantic import BaseModel, Field\n",
    "from llama_index.llms.openai import OpenAI\n",
    "from llama_index.core.llms import ChatMessage\n",
    "\n",
    "\n",
    "class TableInfo(BaseModel):\n",
    "    \"\"\"Information regarding a structured table.\"\"\"\n",
    "\n",
    "    table_name: str = Field(\n",
    "        ..., description=\"table name (must be underscores and NO spaces)\"\n",
    "    )\n",
    "    table_summary: str = Field(\n",
    "        ..., description=\"short, concise summary/caption of the table\"\n",
    "    )\n",
    "\n",
    "\n",
    "prompt_str = \"\"\"\\\n",
    "Give me a summary of the table with the following JSON format.\n",
    "\n",
    "- The table name must be unique to the table and describe it while being concise. \n",
    "- Do NOT output a generic table name (e.g. table, my_table).\n",
    "\n",
    "Do NOT make the table name one of the following: {exclude_table_name_list}\n",
    "\n",
    "Table:\n",
    "{table_str}\n",
    "\n",
    "Summary: \"\"\"\n",
    "prompt_tmpl = ChatPromptTemplate(\n",
    "    message_templates=[ChatMessage.from_str(prompt_str, role=\"user\")]\n",
    ")\n",
    "\n",
    "llm = OpenAI(model=\"gpt-4o-mini\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "9aaded9f-4af5-4ab9-b994-374ac420bf58",
   "metadata": {},
   "outputs": [],
   "source": [
    "tableinfo_dir = \"WikiTableQuestions_TableInfo\"\n",
    "!mkdir {tableinfo_dir}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "8e4bf4ce-f969-45a2-b705-adc23a0c1cf1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "\n",
    "def _get_tableinfo_with_index(idx: int) -> str:\n",
    "    results_gen = Path(tableinfo_dir).glob(f\"{idx}_*\")\n",
    "    results_list = list(results_gen)\n",
    "    if len(results_list) == 0:\n",
    "        return None\n",
    "    elif len(results_list) == 1:\n",
    "        path = results_list[0]\n",
    "        with open(path, 'r') as file:\n",
    "            data = json.load(file) \n",
    "            return TableInfo.model_validate(data)\n",
    "    else:\n",
    "        raise ValueError(\n",
    "            f\"More than one file matching index: {list(results_gen)}\"\n",
    "        )\n",
    "\n",
    "\n",
    "table_names = set()\n",
    "table_infos = []\n",
    "for idx, df in enumerate(dfs):\n",
    "    table_info = _get_tableinfo_with_index(idx)\n",
    "    if table_info:\n",
    "        table_infos.append(table_info)\n",
    "    else:\n",
    "        while True:\n",
    "            df_str = df.head(10).to_csv()\n",
    "            table_info = llm.structured_predict(\n",
    "                TableInfo,\n",
    "                prompt_tmpl,\n",
    "                table_str=df_str,\n",
    "                exclude_table_name_list=str(list(table_names)),\n",
    "            )\n",
    "            table_name = table_info.table_name\n",
    "            print(f\"Processed table: {table_name}\")\n",
    "            if table_name not in table_names:\n",
    "                table_names.add(table_name)\n",
    "                break\n",
    "            else:\n",
    "                # try again\n",
    "                print(f\"Table name {table_name} already exists, trying again.\")\n",
    "                pass\n",
    "\n",
    "        out_file = f\"{tableinfo_dir}/{idx}_{table_name}.json\"\n",
    "        json.dump(table_info.dict(), open(out_file, \"w\"))\n",
    "    table_infos.append(table_info)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73345b81-07ff-49b0-9ccd-ed553c195f93",
   "metadata": {},
   "source": [
    "3. å°†ä¸Šè¿°è¡¨æ ¼å­˜å…¥ SQLite æ•°æ®åº“"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "09b6fd32-3255-4f2d-9962-eb9449f56565",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating table: progressive_rock_album_chart_positions\n",
      "Creating table: filmography_of_diane\n",
      "Creating table: annual_fatalities_and_accidents_statistics\n",
      "Creating table: academy_awards_1972_results\n",
      "Creating table: theatrical_award_nominations_and_wins\n",
      "Creating table: bad_boy_artists_album_release_summary\n",
      "Creating table: south_dakota_radio_stations\n",
      "Creating table: missing_persons_case_summary_1982\n",
      "Creating table: chart_performance_of_singles\n",
      "Creating table: kodachrome_film_types_and_dates\n",
      "Creating table: bbc_radio_service_costs_2012_2013\n",
      "Creating table: french_airports_usage_summary\n",
      "Creating table: voter_registration_summary_by_party\n",
      "Creating table: norwegian_club_performance_statistics\n",
      "Creating table: triple_crown_winners_history\n",
      "Creating table: grammy_awards_summary_for_artist\n",
      "Creating table: boxing_fight_results_history\n",
      "Creating table: historical_sports_team_performance\n",
      "Creating table: yamato_population_density_summary\n",
      "Creating table: voter_registration_summary_by_party_distribution\n",
      "Creating table: best_actress_award_nominations_and_wins\n",
      "Creating table: uk_ministerial_positions_and_titles_history\n",
      "Creating table: municipality_merger_summary\n",
      "Creating table: euro_2020_group_stage_results\n",
      "Creating table: binary_encoding_probabilities\n",
      "Creating table: monthly_climate_statistics\n",
      "Creating table: italian_government_term_history\n",
      "Creating table: new_mexico_government_officials\n",
      "Creating table: monthly_climate_statistics_summary\n",
      "Creating table: historical_rainfall_experiment_drops\n",
      "Creating table: monthly_weather_statistics\n",
      "Creating table: multilingual_greetings_and_phrases\n",
      "Creating table: ohio_private_schools_summary\n",
      "Creating table: cancer_related_genetic_factors\n"
     ]
    }
   ],
   "source": [
    "# put data into sqlite db\n",
    "from sqlalchemy import (\n",
    "    create_engine,\n",
    "    MetaData,\n",
    "    Table,\n",
    "    Column,\n",
    "    String,\n",
    "    Integer,\n",
    ")\n",
    "import re\n",
    "\n",
    "\n",
    "# Function to create a sanitized column name\n",
    "def sanitize_column_name(col_name):\n",
    "    # Remove special characters and replace spaces with underscores\n",
    "    return re.sub(r\"\\W+\", \"_\", col_name)\n",
    "\n",
    "\n",
    "# Function to create a table from a DataFrame using SQLAlchemy\n",
    "def create_table_from_dataframe(\n",
    "    df: pd.DataFrame, table_name: str, engine, metadata_obj\n",
    "):\n",
    "    # Sanitize column names\n",
    "    sanitized_columns = {col: sanitize_column_name(col) for col in df.columns}\n",
    "    df = df.rename(columns=sanitized_columns)\n",
    "\n",
    "    # Dynamically create columns based on DataFrame columns and data types\n",
    "    columns = [\n",
    "        Column(col, String if dtype == \"object\" else Integer)\n",
    "        for col, dtype in zip(df.columns, df.dtypes)\n",
    "    ]\n",
    "\n",
    "    # Create a table with the defined columns\n",
    "    table = Table(table_name, metadata_obj, *columns)\n",
    "\n",
    "    # Create the table in the database\n",
    "    metadata_obj.create_all(engine)\n",
    "\n",
    "    # Insert data from DataFrame into the table\n",
    "    with engine.connect() as conn:\n",
    "        for _, row in df.iterrows():\n",
    "            insert_stmt = table.insert().values(**row.to_dict())\n",
    "            conn.execute(insert_stmt)\n",
    "        conn.commit()\n",
    "\n",
    "\n",
    "# engine = create_engine(\"sqlite:///:memory:\")\n",
    "engine = create_engine(\"sqlite:///wiki_table_questions.db\")\n",
    "metadata_obj = MetaData()\n",
    "for idx, df in enumerate(dfs):\n",
    "    tableinfo = _get_tableinfo_with_index(idx)\n",
    "    print(f\"Creating table: {tableinfo.table_name}\")\n",
    "    create_table_from_dataframe(df, tableinfo.table_name, engine, metadata_obj)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18b6ab4f-7db7-47ef-ad8a-898b19540d56",
   "metadata": {},
   "source": [
    "### 9.2 æ„å»ºåŸºç¡€å·¥å…·"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c2ebdd1-88e4-4e3e-b025-000890ac300e",
   "metadata": {},
   "source": [
    "1. åˆ›å»ºåŸºäºè¡¨çš„æè¿°çš„å‘é‡ç´¢å¼•\n",
    "\n",
    "- `ObjectIndex` æ˜¯ä¸€ä¸ª LlamaIndex å†…ç½®çš„æ¨¡å—ï¼Œé€šè¿‡ç´¢å¼• (Indexï¼‰æ£€ç´¢ä»»æ„ Python å¯¹è±¡\n",
    "- è¿™é‡Œæˆ‘ä»¬ä½¿ç”¨ `VectorStoreIndex` ä¹Ÿå°±æ˜¯å‘é‡æ£€ç´¢ï¼Œå¹¶é€šè¿‡ `SQLTableNodeMapping` å°†æ–‡æœ¬æè¿°çš„ `node` å’Œæ•°æ®åº“çš„è¡¨å½¢æˆæ˜ å°„\n",
    "- ç›¸å…³æ–‡æ¡£ï¼šhttps://docs.llamaindex.ai/en/stable/examples/objects/object_index/#the-objectindex-class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "49e12988-cc89-4618-b3f6-a6a038c3c8dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.objects import (\n",
    "    SQLTableNodeMapping,\n",
    "    ObjectIndex,\n",
    "    SQLTableSchema,\n",
    ")\n",
    "from llama_index.core import SQLDatabase, VectorStoreIndex\n",
    "\n",
    "sql_database = SQLDatabase(engine)\n",
    "\n",
    "table_node_mapping = SQLTableNodeMapping(sql_database)\n",
    "table_schema_objs = [\n",
    "    SQLTableSchema(table_name=t.table_name, context_str=t.table_summary)\n",
    "    for t in table_infos\n",
    "]  # add a SQLTableSchema for each table\n",
    "\n",
    "obj_index = ObjectIndex.from_objects(\n",
    "    table_schema_objs,\n",
    "    table_node_mapping,\n",
    "    VectorStoreIndex,\n",
    ")\n",
    "obj_retriever = obj_index.as_retriever(similarity_top_k=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef92b78b-0530-468d-90ec-309d1614d390",
   "metadata": {},
   "source": [
    "2. åˆ›å»º SQL æŸ¥è¯¢å™¨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "948b8bee-2af6-4487-8b27-a2b2a5d7420b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.retrievers import SQLRetriever\n",
    "from typing import List\n",
    "\n",
    "sql_retriever = SQLRetriever(sql_database)\n",
    "\n",
    "\n",
    "def get_table_context_str(table_schema_objs: List[SQLTableSchema]):\n",
    "    \"\"\"Get table context string.\"\"\"\n",
    "    context_strs = []\n",
    "    for table_schema_obj in table_schema_objs:\n",
    "        table_info = sql_database.get_single_table_info(\n",
    "            table_schema_obj.table_name\n",
    "        )\n",
    "        if table_schema_obj.context_str:\n",
    "            table_opt_context = \" The table description is: \"\n",
    "            table_opt_context += table_schema_obj.context_str\n",
    "            table_info += table_opt_context\n",
    "\n",
    "        context_strs.append(table_info)\n",
    "    return \"\\n\\n\".join(context_strs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39d5bac2-4c10-4411-bcd1-d825727df41f",
   "metadata": {},
   "source": [
    "3. åˆ›å»º Text2SQL çš„æç¤ºè¯ï¼ˆç³»ç»Ÿé»˜è®¤æ¨¡æ¿ï¼‰ï¼Œå’Œè¾“å‡ºç»“æœè§£æå™¨ï¼ˆä»ç”Ÿæˆçš„æ–‡æœ¬ä¸­æŠ½å–SQLï¼‰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "da0c9ea7-5aa5-4069-8baf-9ca62c9e278a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Given an input question, first create a syntactically correct {dialect} query to run, then look at the results of the query and return the answer. You can order the results by a relevant column to return the most interesting examples in the database.\n",
      "\n",
      "Never query for all the columns from a specific table, only ask for a few relevant columns given the question.\n",
      "\n",
      "Pay attention to use only the column names that you can see in the schema description. Be careful to not query for columns that do not exist. Pay attention to which column is in which table. Also, qualify column names with the table name when needed. You are required to use the following format, each taking one line:\n",
      "\n",
      "Question: Question here\n",
      "SQLQuery: SQL Query to run\n",
      "SQLResult: Result of the SQLQuery\n",
      "Answer: Final answer here\n",
      "\n",
      "Only use tables listed below.\n",
      "{schema}\n",
      "\n",
      "Question: {query_str}\n",
      "SQLQuery: \n"
     ]
    }
   ],
   "source": [
    "from llama_index.core.prompts.default_prompts import DEFAULT_TEXT_TO_SQL_PROMPT\n",
    "from llama_index.core import PromptTemplate\n",
    "from llama_index.core.llms import ChatResponse\n",
    "\n",
    "def parse_response_to_sql(chat_response: ChatResponse) -> str:\n",
    "    \"\"\"Parse response to SQL.\"\"\"\n",
    "    response = chat_response.message.content\n",
    "    sql_query_start = response.find(\"SQLQuery:\")\n",
    "    if sql_query_start != -1:\n",
    "        response = response[sql_query_start:]\n",
    "        # TODO: move to removeprefix after Python 3.9+\n",
    "        if response.startswith(\"SQLQuery:\"):\n",
    "            response = response[len(\"SQLQuery:\") :]\n",
    "    sql_result_start = response.find(\"SQLResult:\")\n",
    "    if sql_result_start != -1:\n",
    "        response = response[:sql_result_start]\n",
    "    return response.strip().strip(\"```\").strip()\n",
    "\n",
    "\n",
    "text2sql_prompt = DEFAULT_TEXT_TO_SQL_PROMPT.partial_format(\n",
    "    dialect=engine.dialect.name\n",
    ")\n",
    "print(text2sql_prompt.template)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "691c7cf0-1217-4d16-8196-2937411bbbed",
   "metadata": {},
   "source": [
    "4. åˆ›å»ºè‡ªç„¶è¯­è¨€å›å¤ç”Ÿæˆæ¨¡æ¿"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "f1fbf506-9c46-44bd-9148-0712e4e835a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "response_synthesis_prompt_str = (\n",
    "    \"Given an input question, synthesize a response from the query results.\\n\"\n",
    "    \"Query: {query_str}\\n\"\n",
    "    \"SQL: {sql_query}\\n\"\n",
    "    \"SQL Response: {context_str}\\n\"\n",
    "    \"Response: \"\n",
    ")\n",
    "response_synthesis_prompt = PromptTemplate(\n",
    "    response_synthesis_prompt_str,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "9f4b9121-7da6-496c-8506-48fee7d4ac5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = OpenAI(model=\"gpt-4o-mini\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "258cc672-bed9-45b6-a9cd-80d162f96fd1",
   "metadata": {},
   "source": [
    "### 9.3 å®šä¹‰å·¥ä½œæµ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "8ac5f9db-6dc7-4828-bd2b-1ff7d1e16abd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.workflow import (\n",
    "    Workflow,\n",
    "    StartEvent,\n",
    "    StopEvent,\n",
    "    step,\n",
    "    Context,\n",
    "    Event,\n",
    ")\n",
    "\n",
    "# äº‹ä»¶ï¼šæ‰¾åˆ°äº†æ•°æ®åº“ä¸­ç›¸å…³çš„è¡¨\n",
    "class TableRetrieveEvent(Event):\n",
    "    \"\"\"Result of running table retrieval.\"\"\"\n",
    "\n",
    "    table_context_str: str\n",
    "    query: str\n",
    "\n",
    "# äº‹ä»¶ï¼šæ–‡æœ¬è½¬ä¸ºäº† SQL\n",
    "class TextToSQLEvent(Event):\n",
    "    \"\"\"Text-to-SQL event.\"\"\"\n",
    "\n",
    "    sql: str\n",
    "    query: str\n",
    "\n",
    "\n",
    "class TextToSQLWorkflow1(Workflow):\n",
    "    \"\"\"Text-to-SQL Workflow that does query-time table retrieval.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        obj_retriever,\n",
    "        text2sql_prompt,\n",
    "        sql_retriever,\n",
    "        response_synthesis_prompt,\n",
    "        llm,\n",
    "        *args,\n",
    "        **kwargs\n",
    "    ) -> None:\n",
    "        \"\"\"Init params.\"\"\"\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.obj_retriever = obj_retriever\n",
    "        self.text2sql_prompt = text2sql_prompt\n",
    "        self.sql_retriever = sql_retriever\n",
    "        self.response_synthesis_prompt = response_synthesis_prompt\n",
    "        self.llm = llm\n",
    "\n",
    "    @step\n",
    "    def retrieve_tables(\n",
    "        self, ctx: Context, ev: StartEvent\n",
    "    ) -> TableRetrieveEvent:\n",
    "        \"\"\"Retrieve tables.\"\"\"\n",
    "        table_schema_objs = self.obj_retriever.retrieve(ev.query)\n",
    "        table_context_str = get_table_context_str(table_schema_objs)\n",
    "        print(\"====\\n\"+table_context_str+\"\\n====\")\n",
    "        return TableRetrieveEvent(\n",
    "            table_context_str=table_context_str, query=ev.query\n",
    "        )\n",
    "\n",
    "    @step\n",
    "    def generate_sql(\n",
    "        self, ctx: Context, ev: TableRetrieveEvent\n",
    "    ) -> TextToSQLEvent:\n",
    "        \"\"\"Generate SQL statement.\"\"\"\n",
    "        fmt_messages = self.text2sql_prompt.format_messages(\n",
    "            query_str=ev.query, schema=ev.table_context_str\n",
    "        )\n",
    "        chat_response = self.llm.chat(fmt_messages)\n",
    "        sql = parse_response_to_sql(chat_response)\n",
    "        print(\"====\\n\"+sql+\"\\n====\")\n",
    "        return TextToSQLEvent(sql=sql, query=ev.query)\n",
    "\n",
    "    @step\n",
    "    def generate_response(self, ctx: Context, ev: TextToSQLEvent) -> StopEvent:\n",
    "        \"\"\"Run SQL retrieval and generate response.\"\"\"\n",
    "        retrieved_rows = self.sql_retriever.retrieve(ev.sql)\n",
    "        print(\"====\\n\"+str(retrieved_rows)+\"\\n====\")\n",
    "        fmt_messages = self.response_synthesis_prompt.format_messages(\n",
    "            sql_query=ev.sql,\n",
    "            context_str=str(retrieved_rows),\n",
    "            query_str=ev.query,\n",
    "        )\n",
    "        chat_response = llm.chat(fmt_messages)\n",
    "        return StopEvent(result=chat_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "9b3060a2-20e4-4153-b3f9-6b91ed4d7020",
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow = TextToSQLWorkflow1(\n",
    "    obj_retriever,\n",
    "    text2sql_prompt,\n",
    "    sql_retriever,\n",
    "    response_synthesis_prompt,\n",
    "    llm,\n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "ead701cd-c79d-4b48-8a20-9c2322140712",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running step retrieve_tables\n",
      "====\n",
      "Table 'bad_boy_artists_album_release_summary' has columns: Act (VARCHAR), Year_signed (INTEGER), _Albums_released_under_Bad_Boy (VARCHAR), . The table description is: Summary of artists signed to Bad Boy Records and their album releases.\n",
      "\n",
      "Table 'chart_performance_of_singles' has columns: Year (INTEGER), Single (VARCHAR), Peak_chart_positions_GER (VARCHAR), Peak_chart_positions_IRE (VARCHAR), Peak_chart_positions_UK (VARCHAR), Peak_chart_positions_US (VARCHAR), Peak_chart_positions_US_Main (VARCHAR), Peak_chart_positions_US_Dance (VARCHAR), Certifications_sales_thresholds_ (INTEGER), Album (VARCHAR), . The table description is: A summary of chart positions and certifications for singles released by an artist across various countries.\n",
      "\n",
      "Table 'progressive_rock_album_chart_positions' has columns: Year (INTEGER), Title (VARCHAR), Chart_Positions_UK (VARCHAR), Chart_Positions_US (VARCHAR), Chart_Positions_NL (VARCHAR), Comments (VARCHAR), . The table description is: Chart positions of progressive rock albums in the UK, US, and NL from 1969 to 1981.\n",
      "====\n",
      "Step retrieve_tables produced event TableRetrieveEvent\n",
      "Running step generate_sql\n",
      "====\n",
      "SELECT Year_signed FROM bad_boy_artists_album_release_summary WHERE Act = 'The Notorious B.I.G';\n",
      "====\n",
      "Step generate_sql produced event TextToSQLEvent\n",
      "Running step generate_response\n",
      "====\n",
      "[NodeWithScore(node=TextNode(id_='e064d2c0-9c9c-481f-bddf-08c6cd63cfa0', embedding=None, metadata={'sql_query': \"SELECT Year_signed FROM bad_boy_artists_album_release_summary WHERE Act = 'The Notorious B.I.G';\", 'result': [(1993,), (1993,)], 'col_keys': ['Year_signed']}, excluded_embed_metadata_keys=['sql_query', 'result', 'col_keys'], excluded_llm_metadata_keys=['sql_query', 'result', 'col_keys'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text='[(1993,), (1993,)]', mimetype='text/plain', start_char_idx=None, end_char_idx=None, metadata_seperator='\\n', text_template='{metadata_str}\\n\\n{content}'), score=None)]\n",
      "====\n",
      "Step generate_response produced event StopEvent\n",
      "assistant: The Notorious B.I.G was signed to Bad Boy in 1993.\n"
     ]
    }
   ],
   "source": [
    "response = await workflow.run(\n",
    "    query=\"What was the year that The Notorious B.I.G was signed to Bad Boy?\"\n",
    ")\n",
    "print(str(response))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09b64fac-b8b8-4fad-8ecc-5a9431469706",
   "metadata": {},
   "source": [
    "### 9.4 å¯è§†åŒ–å·¥ä½œæµ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b370ab8-9c87-4baa-92ba-3e729a481857",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install llama-index-utils-workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "933d00bd-a602-4958-bed3-d1b530192895",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'NoneType'>\n",
      "<class 'llama_index.core.workflow.events.StopEvent'>\n",
      "<class '__main__.TextToSQLEvent'>\n",
      "<class '__main__.TableRetrieveEvent'>\n",
      "text_to_sql_table_retrieval.html\n"
     ]
    }
   ],
   "source": [
    "from llama_index.utils.workflow import draw_all_possible_flows\n",
    "\n",
    "draw_all_possible_flows(\n",
    "    TextToSQLWorkflow1, filename=\"text_to_sql_table_retrieval.html\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78fc8317-0cac-458b-9ee5-969e4b46442c",
   "metadata": {},
   "source": [
    "### 9.5 å·¥ä½œæµç®¡ç†æ¡†æ¶æ„ä¹‰æ˜¯ä»€ä¹ˆ\n",
    "\n",
    "æ€è€ƒä»¥ä¸‹æƒ…å†µï¼š\n",
    "\n",
    "- `step` çš„æ‰§è¡Œé¡ºåºæœ‰é€»è¾‘åˆ†æ”¯\n",
    "- `step` çš„æ‰§è¡Œæœ‰å¾ªç¯\n",
    "- `step` çš„æ‰§è¡Œå¯ä»¥å¹¶è¡Œ\n",
    "- ä¸€ä¸ª `step` çš„è§¦å‘æ¡ä»¶ä¾èµ–å‰é¢è‹¥å¹² `step` çš„ç»“æœï¼Œä¸”å®ƒä»¬ä¹‹é—´å¯èƒ½æœ‰å¾ªç¯æˆ–è€…å¹¶è¡Œ\n",
    "\n",
    "<img src=\"workflow2.png\" alt=\"å·¥ä½œæµä¸¾ä¾‹\" width=\"800\"/>\n",
    "\n",
    "æ‰€ä»¥ï¼Œå·¥ä½œæµç®¡ç†æ¡†æ¶çš„æ„æ€æ˜¯ä¾¿äºå°†å•ä¸ªäº‹ä»¶çš„å¤„ç†é€»è¾‘å’Œäº‹ä»¶ä¹‹é—´çš„æ‰§è¡Œé¡ºåºç‹¬ç«‹å¼€\n",
    "\n",
    "å…³äº LlamaIndex å·¥ä½œæµçš„æ›´è¯¦ç»†æ–‡æ¡£ï¼šhttps://docs.llamaindex.ai/en/stable/examples/workflow/workflows_cookbook/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b98c98c0-6636-4d71-9156-731e7ca28b6f",
   "metadata": {},
   "source": [
    "## LlamaIndex çš„æ›´å¤šåŠŸèƒ½\n",
    "\n",
    "- æ™ºèƒ½ä½“ï¼ˆAgentï¼‰å¼€å‘æ¡†æ¶ï¼šhttps://docs.llamaindex.ai/en/stable/module_guides/deploying/agents/\n",
    "- RAG çš„è¯„æµ‹ï¼šhttps://docs.llamaindex.ai/en/stable/module_guides/evaluating/\n",
    "- è¿‡ç¨‹ç›‘æ§ï¼šhttps://docs.llamaindex.ai/en/stable/module_guides/observability/\n",
    "\n",
    "ä»¥ä¸Šå†…å®¹æ¶‰åŠè¾ƒå¤šèƒŒæ™¯çŸ¥è¯†ï¼Œæš‚æ—¶ä¸åœ¨æœ¬è¯¾å±•å¼€ï¼Œç›¸å…³çŸ¥è¯†ä¼šåœ¨åé¢è¯¾ç¨‹ä¸­é€ä¸€è¯¦ç»†è®²è§£ã€‚\n",
    "\n",
    "æ­¤å¤–ï¼ŒLlamaIndex é’ˆå¯¹ç”Ÿäº§çº§çš„ RAG ç³»ç»Ÿä¸­é‡åˆ°çš„å„ä¸ªæ–¹é¢çš„ç»†èŠ‚é—®é¢˜ï¼Œæ€»ç»“äº†å¾ˆå¤šé«˜ç«¯æŠ€å·§ï¼ˆ[Advanced Topics](https://docs.llamaindex.ai/en/stable/optimizing/production_rag/)ï¼‰ï¼Œå¯¹å®æˆ˜å¾ˆæœ‰å‚è€ƒä»·å€¼ï¼Œéå¸¸æ¨èæœ‰èƒ½åŠ›çš„åŒå­¦é˜…è¯»ã€‚\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87418cf5-44bf-4004-8757-75638b134c96",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8d1c1bc-daa2-4f40-a9af-bcbd1b092320",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "054aab08-e330-4df7-b0ee-40fc181b03e7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2018ae3-94c6-4e6f-8b5e-d1068cd54390",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

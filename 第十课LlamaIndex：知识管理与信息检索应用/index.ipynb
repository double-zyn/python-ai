{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "35d0faef-0ef0-4a1b-a7d8-99f59cd72a09",
   "metadata": {},
   "source": [
    "# 💡 这节课会带给你\n",
    "\n",
    "1. LlamaIndex 的特点和基本用法\n",
    "2. 了解 LlamaIndex 内置的工具\n",
    "3. 如何用好 SDK 简化基于 LLM 的应用开发\n",
    "\n",
    "开始上课！\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2207fa56-c6c4-446a-9576-5949d0e6e881",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 🎓 这节课怎么学\n",
    "\n",
    "代码能力要求：**中高**，AI/数学基础要求：**无**\n",
    "\n",
    "1. 有编程与软件工程基础的同学\n",
    "   - 关注接口与实现细节、高级技巧、可扩展性\n",
    "2. 没有编程或软件工程基础的同学\n",
    "   - 尽量理解 SDK 的概念和价值，尝试体会使用 SDK 前后的差别与意义\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "194427bf-5807-49d4-ab25-fa0556f835f3",
   "metadata": {},
   "source": [
    "## 1、大语言模型开发框架的价值是什么？\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb60e6e5-8f2a-4cd5-a4b5-824259afc229",
   "metadata": {},
   "source": [
    "_SDK：Software Development Kit，它是一组软件工具和资源的集合，旨在帮助开发者创建、测试、部署和维护应用程序或软件。_\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77d08668-4981-4b84-ae07-f74cfa191309",
   "metadata": {},
   "source": [
    "所有开发框架（SDK）的核心价值，都是降低开发、维护成本。\n",
    "\n",
    "大语言模型开发框架的价值，是让开发者可以更方便地开发基于大语言模型的应用。主要提供两类帮助：\n",
    "\n",
    "1. 第三方能力抽象。比如 LLM、向量数据库、搜索接口等\n",
    "2. 常用工具、方案封装\n",
    "3. 底层实现封装。比如流式接口、超时重连、异步与并行等\n",
    "\n",
    "好的开发框架，需要具备以下特点：\n",
    "\n",
    "1. 可靠性、鲁棒性高\n",
    "2. 可维护性高\n",
    "3. 可扩展性高\n",
    "4. 学习成本低\n",
    "\n",
    "举些通俗的例子：\n",
    "\n",
    "- 与外部功能解依赖\n",
    "  - 比如可以随意更换 LLM 而不用大量重构代码\n",
    "  - 更换三方工具也同理\n",
    "- 经常变的部分要在外部维护而不是放在代码里\n",
    "  - 比如 Prompt 模板\n",
    "- 各种环境下都适用\n",
    "  - 比如线程安全\n",
    "- 方便调试和测试\n",
    "  - 至少要能感觉到用了比不用方便吧\n",
    "  - 合法的输入不会引发框架内部的报错\n",
    "\n",
    "<div class=\"alert alert-success\">\n",
    "<b>划重点：</b>选对了框架，事半功倍；反之，事倍功半。\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "579b589b-5a4e-451e-baf0-d2a409a9cb4b",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "    \n",
    "<b>什么是 SDK?</b> https://aws.amazon.com/cn/what-is/sdk/\n",
    "<br/>\n",
    "<b>SDK 和 API 的区别是什么?</b> https://aws.amazon.com/cn/compare/the-difference-between-sdk-and-api/\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30361cf7-ec31-4c45-871e-28ac4f0db1a9",
   "metadata": {},
   "source": [
    "#### 🌰 举个例子：使用 SDK，4 行代码实现一个简易的 RAG 系统\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b734d50-a6eb-43f1-879d-fa00c84d1941",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "运行本课代码前，请先重启一下 kernel，以重置所有配置。<br>\n",
    "    \n",
    "<img src=\"tips.png\" width=400px/>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb03dc82-649d-4fe8-8ba5-023220c8cb22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install --upgrade llama-index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dea32ce7-c7a7-4692-a217-45cf632281ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import VectorStoreIndex, SimpleDirectoryReader\n",
    "\n",
    "documents = SimpleDirectoryReader(\"./data\").load_data()\n",
    "index = VectorStoreIndex.from_documents(documents)\n",
    "\n",
    "query_engine = index.as_query_engine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "565af66f-30b4-41b4-a49f-88b7bbfc6fd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DeepSeek-V3有671B总参数，每个token激活了37B参数。\n"
     ]
    }
   ],
   "source": [
    "response = query_engine.query(\"deepseek v3有多少参数\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24e4c617-2fb8-489d-a0d0-34d5ca196f1c",
   "metadata": {},
   "source": [
    "## 2、LlamaIndex 介绍\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "313487e1-a2c7-4d4b-ad41-e3a9f0a0b07e",
   "metadata": {},
   "source": [
    "官网标题：_「 Build AI Knowledge Assistants over your enterprise data 」_\n",
    "\n",
    "LlamaIndex 是一个为开发「知识增强」的大语言模型应用的框架（也就是 SDK）。**知识增强**，泛指任何在私有或特定领域数据基础上应用大语言模型的情况。例如：\n",
    "\n",
    "<img src=\"basic_rag.png\" width=600px>\n",
    "\n",
    "- Question-Answering Chatbots (也就是 RAG)\n",
    "- Document Understanding and Extraction （文档理解与信息抽取）\n",
    "\n",
    "- Autonomous Agents that can perform research and take actions （智能体应用）\n",
    "- Workflow orchestrating single and multi-agent (编排单个或多个智能体形成工作流）\n",
    "\n",
    "LlamaIndex 有 Python 和 Typescript 两个版本，Python 版的文档相对更完善。\n",
    "\n",
    "- Python 文档地址：https://docs.llamaindex.ai/en/stable/\n",
    "- Python API 接口文档：https://docs.llamaindex.ai/en/stable/api_reference/\n",
    "\n",
    "- TS 文档地址：https://ts.llamaindex.ai/\n",
    "\n",
    "LlamaIndex 是一个开源框架，Github 链接：https://github.com/run-llama\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee4822b2-52d5-411c-8244-3432f8733da2",
   "metadata": {},
   "source": [
    "### LlamaIndex 的核心模块\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c474b43-018a-4687-a51e-54b391a6bbca",
   "metadata": {},
   "source": [
    "<img src=\"llamaindex.png\" alt=\"LlamaIndex 核心模块\" width=\"800\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca66bf37-48e4-45eb-9e7e-a24e86108ac1",
   "metadata": {},
   "source": [
    "### 安装 LlamaIndex\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79027d8d-106f-42c8-b7b6-944a46182bda",
   "metadata": {},
   "source": [
    "1. Python\n",
    "\n",
    "```\n",
    "pip install llama-index\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b59179e-c8c8-4730-aa29-01c728c00df4",
   "metadata": {},
   "source": [
    "2. Typescript\n",
    "\n",
    "```\n",
    "# 通过 npm 安装\n",
    "npm install llamaindex\n",
    "\n",
    "# 通过 yarn 安装\n",
    "yarn add llamaindex\n",
    "\n",
    "# 通过 pnpm 安装\n",
    "pnpm add llamaindex\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9a882ce-b03f-43dc-aa4c-b269ff49adfd",
   "metadata": {},
   "source": [
    "本课程以 Python 版为例进行讲解。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74341a28-fb7f-4e5b-9342-cf5cd850654a",
   "metadata": {},
   "source": [
    "## 3、数据加载（Loading）\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d134a56-ae9a-41a5-a703-ce9dfb3fc600",
   "metadata": {},
   "source": [
    "### 3.1、加载本地数据\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d35bfa25-9a09-4c40-afa1-9f9156e56dc2",
   "metadata": {},
   "source": [
    "`SimpleDirectoryReader` 是一个简单的本地文件加载器。它会遍历指定目录，并根据文件扩展名自动加载文件（**文本内容**）。\n",
    "\n",
    "支持的文件类型：\n",
    "\n",
    "- `.csv` - comma-separated values\n",
    "- `.docx` - Microsoft Word\n",
    "- `.epub` - EPUB ebook format\n",
    "- `.hwp` - Hangul Word Processor\n",
    "- `.ipynb` - Jupyter Notebook\n",
    "- `.jpeg`, `.jpg` - JPEG image\n",
    "- `.mbox` - MBOX email archive\n",
    "- `.md` - Markdown\n",
    "- `.mp3`, `.mp4` - audio and video\n",
    "- `.pdf` - Portable Document Format\n",
    "- `.png` - Portable Network Graphics\n",
    "- `.ppt`, `.pptm`, `.pptx` - Microsoft PowerPoint\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "05b6a279-3eca-4685-af91-80fb443fca1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pydantic.v1 import BaseModel\n",
    "\n",
    "def show_json(data):\n",
    "    \"\"\"用于展示json数据\"\"\"\n",
    "    if isinstance(data, str):\n",
    "        obj = json.loads(data)\n",
    "        print(json.dumps(obj, indent=4, ensure_ascii=False))\n",
    "    elif isinstance(data, dict) or isinstance(data, list):\n",
    "        print(json.dumps(data, indent=4, ensure_ascii=False))\n",
    "    elif issubclass(type(data), BaseModel):\n",
    "        print(json.dumps(data.dict(), indent=4, ensure_ascii=False))\n",
    "\n",
    "def show_list_obj(data):\n",
    "    \"\"\"用于展示一组对象\"\"\"\n",
    "    if isinstance(data, list):\n",
    "        for item in data:\n",
    "            show_json(item)\n",
    "    else:\n",
    "        raise ValueError(\"Input is not a list\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "198ebc20-fab6-46a9-8cca-4b114fe2640f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import SimpleDirectoryReader\n",
    "\n",
    "reader = SimpleDirectoryReader(\n",
    "        input_dir=\"./data\", # 目标目录\n",
    "        recursive=False, # 是否递归遍历子目录\n",
    "        required_exts=[\".pdf\"] # (可选)只读取指定后缀的文件\n",
    "    )\n",
    "documents = reader.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dfb3f1ac-75cb-4c5e-b0a1-53621d8d325e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DeepSeek-V3 Technical Report\n",
      "DeepSeek-AI\n",
      "research@deepseek.com\n",
      "Abstract\n",
      "We present DeepSeek-V3, a strong Mixture-of-Experts (MoE) language model with 671B total\n",
      "parameters with 37B activated for each token. To achieve efficient inference and cost-effective\n",
      "training, DeepSeek-V3 adopts Multi-head Latent Attention (MLA) and DeepSeekMoE architec-\n",
      "tures, which were thoroughly validated in DeepSeek-V2. Furthermore, DeepSeek-V3 pioneers\n",
      "an auxiliary-loss-free strategy for load balancing and sets a multi-token prediction training\n",
      "objective for stronger performance. We pre-train DeepSeek-V3 on 14.8 trillion diverse and\n",
      "high-quality tokens, followed by Supervised Fine-Tuning and Reinforcement Learning stages to\n",
      "fully harness its capabilities. Comprehensive evaluations reveal that DeepSeek-V3 outperforms\n",
      "other open-source models and achieves performance comparable to leading closed-source\n",
      "models. Despite its excellent performance, DeepSeek-V3 requires only 2.788M H800 GPU hours\n",
      "for its full training. In addition, its training process is remarkably stable. Throughout the entire\n",
      "training process, we did not experience any irrecoverable loss spikes or perform any rollbacks.\n",
      "The model checkpoints are available at https://github.com/deepseek-ai/DeepSeek-V3.\n",
      "MMLU-Pro\n",
      "(EM)\n",
      "GPQA-Diamond\n",
      "(Pass@1)\n",
      "MATH 500\n",
      "(EM)\n",
      "AIME 2024\n",
      "(Pass@1)\n",
      "Codeforces\n",
      "(Percentile)\n",
      "SWE-bench Verified\n",
      "(Resolved)\n",
      "0\n",
      "20\n",
      "40\n",
      "60\n",
      "80\n",
      "100Accuracy / Percentile (%)\n",
      "75.9\n",
      "59.1\n",
      "90.2\n",
      "39.2\n",
      "51.6\n",
      "42.0\n",
      "66.2\n",
      "41.3\n",
      "74.7\n",
      "16.7\n",
      "35.6\n",
      "22.6\n",
      "71.6\n",
      "49.0\n",
      "80.0\n",
      "23.3 24.8 23.8\n",
      "73.3\n",
      "51.1\n",
      "73.8\n",
      "23.3\n",
      "25.3 24.5\n",
      "72.6\n",
      "49.9\n",
      "74.6\n",
      "9.3\n",
      "23.6\n",
      "38.8\n",
      "78.0\n",
      "65.0\n",
      "78.3\n",
      "16.0\n",
      "20.3\n",
      "50.8\n",
      "DeepSeek-V3 DeepSeek-V2.5 Qwen2.5-72B-Inst Llama-3.1-405B-Inst GPT-4o-0513 Claude-3.5-Sonnet-1022\n",
      "Figure 1 |Benchmark performance of DeepSeek-V3 and its counterparts.\n",
      "arXiv:2412.19437v2  [cs.CL]  18 Feb 2025\n",
      "{\n",
      "    \"id_\": \"268f9401-4266-45fd-9ccf-b1dcb17daaef\",\n",
      "    \"embedding\": null,\n",
      "    \"metadata\": {\n",
      "        \"page_label\": \"1\",\n",
      "        \"file_name\": \"deepseek-v3-1-4.pdf\",\n",
      "        \"file_path\": \"/home/jovyan/dev-notes/10-llamaindex/data/deepseek-v3-1-4.pdf\",\n",
      "        \"file_type\": \"application/pdf\",\n",
      "        \"file_size\": 192218,\n",
      "        \"creation_date\": \"2025-03-12\",\n",
      "        \"last_modified_date\": \"2025-03-12\"\n",
      "    },\n",
      "    \"excluded_embed_metadata_keys\": [\n",
      "        \"file_name\",\n",
      "        \"file_type\",\n",
      "        \"file_size\",\n",
      "        \"creation_date\",\n",
      "        \"last_modified_date\",\n",
      "        \"last_accessed_date\"\n",
      "    ],\n",
      "    \"excluded_llm_metadata_keys\": [\n",
      "        \"file_name\",\n",
      "        \"file_type\",\n",
      "        \"file_size\",\n",
      "        \"creation_date\",\n",
      "        \"last_modified_date\",\n",
      "        \"last_accessed_date\"\n",
      "    ],\n",
      "    \"relationships\": {},\n",
      "    \"metadata_template\": \"{key}: {value}\",\n",
      "    \"metadata_separator\": \"\\n\",\n",
      "    \"text_resource\": {\n",
      "        \"embeddings\": null,\n",
      "        \"text\": \"DeepSeek-V3 Technical Report\\nDeepSeek-AI\\nresearch@deepseek.com\\nAbstract\\nWe present DeepSeek-V3, a strong Mixture-of-Experts (MoE) language model with 671B total\\nparameters with 37B activated for each token. To achieve efficient inference and cost-effective\\ntraining, DeepSeek-V3 adopts Multi-head Latent Attention (MLA) and DeepSeekMoE architec-\\ntures, which were thoroughly validated in DeepSeek-V2. Furthermore, DeepSeek-V3 pioneers\\nan auxiliary-loss-free strategy for load balancing and sets a multi-token prediction training\\nobjective for stronger performance. We pre-train DeepSeek-V3 on 14.8 trillion diverse and\\nhigh-quality tokens, followed by Supervised Fine-Tuning and Reinforcement Learning stages to\\nfully harness its capabilities. Comprehensive evaluations reveal that DeepSeek-V3 outperforms\\nother open-source models and achieves performance comparable to leading closed-source\\nmodels. Despite its excellent performance, DeepSeek-V3 requires only 2.788M H800 GPU hours\\nfor its full training. In addition, its training process is remarkably stable. Throughout the entire\\ntraining process, we did not experience any irrecoverable loss spikes or perform any rollbacks.\\nThe model checkpoints are available at https://github.com/deepseek-ai/DeepSeek-V3.\\nMMLU-Pro\\n(EM)\\nGPQA-Diamond\\n(Pass@1)\\nMATH 500\\n(EM)\\nAIME 2024\\n(Pass@1)\\nCodeforces\\n(Percentile)\\nSWE-bench Verified\\n(Resolved)\\n0\\n20\\n40\\n60\\n80\\n100Accuracy / Percentile (%)\\n75.9\\n59.1\\n90.2\\n39.2\\n51.6\\n42.0\\n66.2\\n41.3\\n74.7\\n16.7\\n35.6\\n22.6\\n71.6\\n49.0\\n80.0\\n23.3 24.8 23.8\\n73.3\\n51.1\\n73.8\\n23.3\\n25.3 24.5\\n72.6\\n49.9\\n74.6\\n9.3\\n23.6\\n38.8\\n78.0\\n65.0\\n78.3\\n16.0\\n20.3\\n50.8\\nDeepSeek-V3 DeepSeek-V2.5 Qwen2.5-72B-Inst Llama-3.1-405B-Inst GPT-4o-0513 Claude-3.5-Sonnet-1022\\nFigure 1 |Benchmark performance of DeepSeek-V3 and its counterparts.\\narXiv:2412.19437v2  [cs.CL]  18 Feb 2025\",\n",
      "        \"path\": null,\n",
      "        \"url\": null,\n",
      "        \"mimetype\": null\n",
      "    },\n",
      "    \"image_resource\": null,\n",
      "    \"audio_resource\": null,\n",
      "    \"video_resource\": null,\n",
      "    \"text_template\": \"{metadata_str}\\n\\n{content}\",\n",
      "    \"class_name\": \"Document\",\n",
      "    \"text\": \"DeepSeek-V3 Technical Report\\nDeepSeek-AI\\nresearch@deepseek.com\\nAbstract\\nWe present DeepSeek-V3, a strong Mixture-of-Experts (MoE) language model with 671B total\\nparameters with 37B activated for each token. To achieve efficient inference and cost-effective\\ntraining, DeepSeek-V3 adopts Multi-head Latent Attention (MLA) and DeepSeekMoE architec-\\ntures, which were thoroughly validated in DeepSeek-V2. Furthermore, DeepSeek-V3 pioneers\\nan auxiliary-loss-free strategy for load balancing and sets a multi-token prediction training\\nobjective for stronger performance. We pre-train DeepSeek-V3 on 14.8 trillion diverse and\\nhigh-quality tokens, followed by Supervised Fine-Tuning and Reinforcement Learning stages to\\nfully harness its capabilities. Comprehensive evaluations reveal that DeepSeek-V3 outperforms\\nother open-source models and achieves performance comparable to leading closed-source\\nmodels. Despite its excellent performance, DeepSeek-V3 requires only 2.788M H800 GPU hours\\nfor its full training. In addition, its training process is remarkably stable. Throughout the entire\\ntraining process, we did not experience any irrecoverable loss spikes or perform any rollbacks.\\nThe model checkpoints are available at https://github.com/deepseek-ai/DeepSeek-V3.\\nMMLU-Pro\\n(EM)\\nGPQA-Diamond\\n(Pass@1)\\nMATH 500\\n(EM)\\nAIME 2024\\n(Pass@1)\\nCodeforces\\n(Percentile)\\nSWE-bench Verified\\n(Resolved)\\n0\\n20\\n40\\n60\\n80\\n100Accuracy / Percentile (%)\\n75.9\\n59.1\\n90.2\\n39.2\\n51.6\\n42.0\\n66.2\\n41.3\\n74.7\\n16.7\\n35.6\\n22.6\\n71.6\\n49.0\\n80.0\\n23.3 24.8 23.8\\n73.3\\n51.1\\n73.8\\n23.3\\n25.3 24.5\\n72.6\\n49.9\\n74.6\\n9.3\\n23.6\\n38.8\\n78.0\\n65.0\\n78.3\\n16.0\\n20.3\\n50.8\\nDeepSeek-V3 DeepSeek-V2.5 Qwen2.5-72B-Inst Llama-3.1-405B-Inst GPT-4o-0513 Claude-3.5-Sonnet-1022\\nFigure 1 |Benchmark performance of DeepSeek-V3 and its counterparts.\\narXiv:2412.19437v2  [cs.CL]  18 Feb 2025\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "print(documents[0].text)\n",
    "show_json(documents[0].json())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "277be7f4-1993-4b74-bd3e-f9d8cb5a827d",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "<b>注意：</b>对图像、视频、语音类文件，默认不会自动提取其中文字。如需提取，参考下面介绍的 <code>Data Connectors</code>。\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2c98851-0858-4215-858d-bea70e310d5f",
   "metadata": {},
   "source": [
    "默认的 `PDFReader` 效果并不理想，我们可以更换文件加载器\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b093a88b-5d4a-423b-bfe7-fa28b34744e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started parsing the file under job_id 45c4b2cf-963a-4abc-99be-38b82ce8eef3\n",
      "# DeepSeek-V3 Technical Report\n",
      "\n",
      "# DeepSeek-AI\n",
      "\n",
      "research@deepseek.com\n",
      "\n",
      "# Abstract\n",
      "\n",
      "We present DeepSeek-V3, a strong Mixture-of-Experts (MoE) language model with 671B total parameters with 37B activated for each token. To achieve efficient inference and cost-effective training, DeepSeek-V3 adopts Multi-head Latent Attention (MLA) and DeepSeekMoE architectures, which were thoroughly validated in DeepSeek-V2. Furthermore, DeepSeek-V3 pioneers an auxiliary-loss-free strategy for load balancing and sets a multi-token prediction training objective for stronger performance. We pre-train DeepSeek-V3 on 14.8 trillion diverse and high-quality tokens, followed by Supervised Fine-Tuning and Reinforcement Learning stages to fully harness its capabilities. Comprehensive evaluations reveal that DeepSeek-V3 outperforms other open-source models and achieves performance comparable to leading closed-source models. Despite its excellent performance, DeepSeek-V3 requires only 2.788M H800 GPU hours for its full training. In addition, its training process is remarkably stable. Throughout the entire training process, we did not experience any irrecoverable loss spikes or perform any rollbacks. The model checkpoints are available at https://github.com/deepseek-ai/DeepSeek-V3.\n",
      "\n",
      "|DeepSeek-V3|DeepSeek-V2.5|Qwen2.5-72B-Inst|Llama-3.1-405B-Inst|GPT-4o-0513|Claude-3.5-Sonnet-1022| |\n",
      "|---|---|---|---|---|---|---|\n",
      "|100|90.2| | | | | |\n",
      "|80|75.9|71.6|73.3|72.6|78.0| |\n",
      "| |74.7|80.0|73.8|74.6|78.3| |\n",
      "|66.2| |65.0| | | | |\n",
      "|60| |59.1| | | | |\n",
      "|49.0|51.1|49.9| |51.6|50.8| |\n",
      "|40|41.3|39.2| |42.0|38.8| |\n",
      "| | | |35.6| | | |\n",
      "|20| | | |20.3| | |\n",
      "| |16.7|16.0| |9.3| | |\n",
      "|0|MMLU-Pro|GPQA-Diamond|MATH 500|AIME 2024|Codeforces|SWE-bench Verified|\n",
      "\n",
      "Figure 1 | Benchmark performance of DeepSeek-V3 and its counterparts.\n"
     ]
    }
   ],
   "source": [
    "# 在环境变量中配置 LLAMA_CLOUD_API_KEY=llx-8uobXJdi12aGhJduv2iTpbpXnZPbMFEMvDoKzj1JLV4ZjKDP\n",
    "import os\n",
    "os.environ[\"LLAMA_CLOUD_API_KEY\"] = \"llx-8uobXJdi12aGhJduv2iTpbpXnZPbMFEMvDoKzj1JLV4ZjKDP\"\n",
    "\n",
    "from llama_cloud_services import LlamaParse\n",
    "from llama_index.core import SimpleDirectoryReader\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply() # 只在Jupyter笔记环境中需要此操作，否则会报错\n",
    "\n",
    "# set up parser\n",
    "parser = LlamaParse(\n",
    "    result_type=\"markdown\"  # \"markdown\" and \"text\" are available\n",
    ")\n",
    "file_extractor = {\".pdf\": parser}\n",
    "\n",
    "documents = SimpleDirectoryReader(input_dir=\"./data\", required_exts=[\".pdf\"], file_extractor=file_extractor).load_data()\n",
    "print(documents[0].text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a116d296-e4a4-4710-a41a-318bbab8ec24",
   "metadata": {},
   "source": [
    "### 3.2、Data Connectors\n",
    "\n",
    "用于处理更丰富的数据类型，并将其读取为 `Document` 的形式。\n",
    "\n",
    "例如：直接读取网页\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19471210-6cb3-4822-8bad-dc701b6ab335",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install llama-index-readers-web"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2d73abf3-91df-488c-a5f3-c7de0c6d4c44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[网页](https://www.baidu.com/s?ie=utf-8&fr=bks0000&wd=知乎知学堂)[新闻](http://news.baidu.com/ns?tn=news&cl=2&rn=20&ct=1&fr=bks0000&ie=utf-8&word=知乎知学堂)[贴吧](https://tieba.baidu.com/f?ie=utf-8&fr=bks0000&kw=知乎知学堂)[知道](https://zhidao.baidu.com/search?pn=0&&rn=10&lm=0&fr=bks0000&word=知乎知学堂)[网盘](https://pan.baidu.com/disk/home#/search?from=1027327l&key=知乎知学堂)[图片](http://image.baidu.com/search/index?tn=baiduimage&ct=201326592&lm=-1&cl=2&nc=1&ie=utf-8&word=知乎知学堂)[视频](https://www.baidu.com/sf/vsearch?pd=video&tn=vsearch&ie=utf-8&rsv_spt=17&wd=知乎知学堂)[地图](http://map.baidu.com/m?ie=utf-8&fr=bks0000&word=知乎知学堂)[文库](https://wenku.baidu.com/search?lm=0&od=0&ie=utf-8&fr=bks0000&word=知乎知学堂)[资讯](https://www.baidu.com/s?rtt=1&bsst=1&cl=2&tn=news&fr=baike&word=知乎知学堂)[采购](https://b2b.baidu.com/s?fr=bks0000&q=知乎知学堂)百科\n",
      "\n",
      "[百度首页](http://www.baidu.com)\n",
      "\n",
      "登录\n",
      "\n",
      "[注册](https://passport.baidu.com/v2/?reg&regType=1&tpl=wk)\n",
      "\n",
      "[![百度百科](https://baikebcs.bdimg.com/baike-react/common/logo-\n",
      "baike.svg)](/#home)\n",
      "\n",
      "进入词条全站搜索[帮助](/help)\n",
      "\n",
      "[首页](/)\n",
      "\n",
      "    \n",
      "\n",
      "秒懂百科\n",
      "\n",
      "    \n",
      "\n",
      "特色百科\n",
      "\n",
      "    \n",
      "\n",
      "知识专题\n",
      "\n",
      "    \n",
      "\n",
      "加入百科\n",
      "\n",
      "    \n",
      "\n",
      "百科团队\n",
      "\n",
      "    \n",
      "\n",
      "权威合作\n",
      "\n",
      "    \n",
      "\n",
      "[个人中心](/usercenter)\n",
      "\n",
      "收藏\n",
      "\n",
      "查看[我的收藏](/uc/favolemma)\n",
      "\n",
      "0有用+1\n",
      "\n",
      "0\n",
      "\n",
      "# 知乎知学堂\n",
      "\n",
      "播报[讨论](/planet/talk?lemmaId=64028760&fromModule=lemma_right-issue-btn)上传视频\n",
      "\n",
      "知乎旗下职业教育品牌\n",
      "\n",
      "知乎知学堂是知乎旗下的职业教育品牌，于2022年12月30日正式发布。知乎知学堂依托自身科技实力，专注于用户职业发展，聚集各行业优质教育资源，打造了一站式在线职业教育平台。\n",
      "[1]__截至2023年底\n",
      "，知乎知学堂集合了品职教育、趴趴教育、MBA大师、一起公考、一起考教师、AGI课堂6个子品牌，完成了6大领域30多个品类课程的覆盖，累计为超过2000万学员提供职业培训服务。\n",
      "[2]__\n",
      "\n",
      "2024年1月11日，知乎知学堂宣布正式独立运营，并公布了未来两大核心战略：第一，通过数字化赋能，改善职业教育服务体系和交付标准；第二，通过开放协作和产教融合，助力职业教育实现高质量就业。\n",
      "[2]__\n",
      "\n",
      "中文名\n",
      "\n",
      "     知乎知学堂\n",
      "\n",
      "所属行业\n",
      "\n",
      "    职业教育\n",
      "\n",
      "创立时间\n",
      "\n",
      "    2022年12月30日\n",
      "\n",
      "所属公司\n",
      "\n",
      "    知乎\n",
      "\n",
      "品牌口号\n",
      "\n",
      "    未来职业之路\n",
      "\n",
      "商标注册号\n",
      "\n",
      "    59736985 [3]__\n",
      "\n",
      "## 目录\n",
      "\n",
      "  1. 1发展历程\n",
      "  2. 2业务矩阵\n",
      "  3. 3旗下品牌\n",
      "  4. 4部分师资\n",
      "  5. 5品牌动态\n",
      "\n",
      "## 发展历程\n",
      "\n",
      "播报\n",
      "\n",
      "编辑\n",
      "\n",
      "2022年6月29日，探组求求嘱乎凶知乎推出“学习”专区，上线数百门免费正版授权课程。 [4]__\n",
      "\n",
      "2022年11月1日，知乎上线AI大师免费课程，吴恩达、沈向洋领衔揭秘人工智能。 [5]__\n",
      "\n",
      "2022年12月30日，知乎发布职业教育品牌——在线职业教育平台「知乎知学堂」。 [6]__\n",
      "\n",
      "2023年2月21日，知乎联合中国教催签寒育在线发布考研禁讲厚数据报告，站内考研格页府兴趣用户1嘱戒594万。 [7]__\n",
      "\n",
      "2023年7月15日，知乎年度活动「2023年新知青年大会」在北京举行。知乎创始人、董事长兼 CEO\n",
      "周源出席并发表演婆旋讲将职业教育作为第二增长曲线，同时也宣布「知学计划 」已进入内测阶段，即将对更多有志于提供专业职业教育服务的机构和个人开放。 [8]__\n",
      "\n",
      "2023年11月29日，知乎发布2023年Q3财碑旬照报，职业教育业务营收达到1.45亿元，实现同比增长85.6%。 [9]__\n",
      "\n",
      "2024年1月11日消息，知乎在北京举办「2024知乎教育大会」，宣布职业教育品牌「知乎知学堂」正式独立运营。 [2]__\n",
      "\n",
      "## 业务矩阵\n",
      "\n",
      "播报\n",
      "\n",
      "编辑\n",
      "\n",
      "知乎知学堂已经集合了品职教育、趴趴教育、MBA大师、一起公考、一起考教师、AGI课堂6个子品牌，完成了6大领域30多个品类的覆盖，累计为超过2000万学员提供职业培训服务，成为国内覆盖品类最全的职业教育内容体系之一。在考研、公考、教资、CPA、CFA、MBA、职场技能等诸多课程赛道中，知乎都形成了一定的用户口碑和内容影响力。\n",
      "[2]__\n",
      "\n",
      "[![](https://bkimg.cdn.bcebos.com/pic/d31b0ef41bd5ad6eddc4d047bd932edbb6fd5266965c?x-bce-\n",
      "process=image/format,f_auto/resize,m_lfit,limit_1,h_268)](/pic/%E7%9F%A5%E4%B9%8E%E7%9F%A5%E5%AD%A6%E5%A0%82/64028760/0/d31b0ef41bd5ad6eddc4d047bd932edbb6fd5266965c?fr=lemma&fromModule=lemma_content-\n",
      "image \"知乎知学堂学科品类\")知乎知学堂学科品类 [1]__\n",
      "\n",
      "## 旗下品牌\n",
      "\n",
      "播报\n",
      "\n",
      "编辑\n",
      "\n",
      "趴趴教育：知乎知学堂旗下专注留学考试培训以及留学申请的一站式服务机构，开设有雅思、托福、GRE、GMAT 等课程产品，累计为百万学员提供过留学语培服务。\n",
      "[1]__\n",
      "\n",
      "品职教育：知乎知学堂旗下专注金融职业培训的子品牌，提供包括 CFA、ESG、FRM、CPA、金融考研等多款财经教育类课程，已累计服务考生\n",
      "13000+、CFA 持证学员 2000+，并为 ESG 领域输送人才 4000+。 [1]__\n",
      "\n",
      "MBA大师：知乎知学堂旗下专注管理类联考培训的在线教育品牌，为考生提供精品课程、专属教材及一站式备考服务。「MBA大师」始终致力于为用户提供智能化、个性化、系统化的在线学习体验，累计已服务管理类联考考生超过\n",
      "300 万人。 [1]__\n",
      "\n",
      "一起考教师：知乎知学堂旗下专注教师资格证和教师招聘考试培训的教育品牌，累计为超过 1500 万备考教师资格证和教师招聘的考生提供过服务。 [1]__\n",
      "\n",
      "一起公考：知乎知学堂旗下专注于公务员和事业单位考试培训的教育品牌，累计为超过 200 万备考公务员及事业单位考试的用户提供过服务。 [1]__\n",
      "\n",
      "AGI课堂：知乎知学堂旗下专注人工智能技术培训的教育品牌，提供大模型算法原理、大模型技术应用、大模型业务解决方案等前沿 AI\n",
      "课程。「AGI课堂」始终致力于结合时下热门技术路线及高频应用场景，为用户提供一站式人工智能教育解决方案。 [1]__\n",
      "\n",
      "[![](https://bkimg.cdn.bcebos.com/pic/dcc451da81cb39dbb6fd2dd3924e1e24ab18962b9aa4?x-bce-\n",
      "process=image/format,f_auto/resize,m_lfit,limit_1,h_240)](/pic/%E7%9F%A5%E4%B9%8E%E7%9F%A5%E5%AD%A6%E5%A0%82/64028760/0/dcc451da81cb39dbb6fd2dd3924e1e24ab18962b9aa4?fr=lemma&fromModule=lemma_content-\n",
      "image \"知乎知学堂旗下品牌\")知乎知学堂旗下品牌 [2]__\n",
      "\n",
      "## 部分师资\n",
      "\n",
      "播报\n",
      "\n",
      "编辑\n",
      "\n",
      "贾岚 考研英语资深名师、中国人民大学英语语言文学硕士。 [1]__\n",
      "\n",
      "王瑞恩 中美两国法律职业资格证持证人、本科毕业于清华大学。 [1]__\n",
      "\n",
      "蔡金龙 公考资深名师、17年执教行业经验、清华大学本硕。 [1]__\n",
      "\n",
      "[李斯克](/item/%E6%9D%8E%E6%96%AF%E5%85%8B/58536549?fromModule=lemma_inlink)\n",
      "CFA/FRM资深名师、上海交通大学本硕。 [1]__\n",
      "\n",
      "Paul Winterbottom 前雅思口语写作英籍考官、牛津大学本硕。 [1]__\n",
      "\n",
      "[薛睿](/item/%E8%96%9B%E7%9D%BF/54369978?fromModule=lemma_inlink)\n",
      "管理类联考资深名师、西安交大本科及MBA、利兹硕士。 [1]__\n",
      "\n",
      "张倩 前红圈所律师、毕业于北京大学法学院。 [1]__\n",
      "\n",
      "## 品牌动态\n",
      "\n",
      "播报\n",
      "\n",
      "编辑\n",
      "\n",
      "2024年1月11日，知乎在北京举办“2024知乎教育大会”。会上，知乎创始人、CEO[周源](/item/%E5%91%A8%E6%BA%90/15897920?fromModule=lemma_inlink)表示，今天的知乎，不止于社区，正转变为一个为新职人搭建平台、提供服务的公司，满足新职人包含职业教育在内的各类需求；知乎高级副总裁、知乎知学堂CEO张荣乐表示，知乎知学堂已经成为国内覆盖品类最全的职业教育内容体系之一，未来将持续推动数字化赋能，用科技重塑职业教育行业，并通过开放协作和产教融合，助力职业教育实现高质量就业。\n",
      "[2]__\n",
      "\n",
      "[![](https://bkimg.cdn.bcebos.com/pic/8ad4b31c8701a18b87d6e56ba777100828381f3050c4?x-bce-\n",
      "process=image/format,f_auto/resize,m_lfit,limit_1,w_440)大会嘉宾观点(5张)](/pic/%E7%9F%A5%E4%B9%8E%E7%9F%A5%E5%AD%A6%E5%A0%82/64028760/5168228231/8ad4b31c8701a18b87d6e56ba777100828381f3050c4?fr=lemma&fromModule=lemma_content-\n",
      "image \"大会嘉宾观点\")\n",
      "\n",
      "新手上路\n",
      "\n",
      "[成长任务](/usercenter/tasks#guide)[编辑入门](/help#main01)[编辑规则](/help#main06)[本人编辑![new](https://baikebcs.bdimg.com/baike-\n",
      "react/common/new.png)](/item/百度百科：本人词条编辑服务/22442459?bk_fr=pcFooter)\n",
      "\n",
      "我有疑问\n",
      "\n",
      "内容质疑[在线客服](https://ufosdk.baidu.com/bailingPC/getEntryPath/aKo-\n",
      "PBP84zlnBbgvj2STxzZNwlrmWT8XVd-\n",
      "PzIQ3C-c=)[官方贴吧](http://tieba.baidu.com/f?ie=utf-8&fr=bks0000&kw=%E7%99%BE%E5%BA%A6%E7%99%BE%E7%A7%91)意见反馈\n",
      "\n",
      "投诉建议\n",
      "\n",
      "[举报不良信息](http://help.baidu.com/newadd?prod_id=10&category=1)[未通过词条申诉](http://help.baidu.com/newadd?prod_id=10&category=2)[投诉侵权信息](http://help.baidu.com/newadd?prod_id=10&category=6)[封禁查询与解封](http://help.baidu.com/newadd?prod_id=10&category=5)\n",
      "\n",
      "©2025 Baidu [使用百度前必读](http://www.baidu.com/duty/) | [百科协议](http://help.baidu.com/question?prod_en=baike&class=89&id=1637) | [隐私政策](http://help.baidu.com/question?prod_id=10&class=690&id=1001779) | [百度百科合作平台](/operation/cooperation) | [京ICP证030173号 ](https://beian.miit.gov.cn/)![](https://ss0.bdstatic.com/5aV1bjqh_Q23odCf/static/superman/img/copy_rignt_24.png)\n",
      "\n",
      "[\n",
      "__京公网安备11000002000001号](http://www.beian.gov.cn/portal/registerSystemInfo?recordcode=11000002000001)\n"
     ]
    }
   ],
   "source": [
    "from llama_index.readers.web import SimpleWebPageReader\n",
    "\n",
    "documents = SimpleWebPageReader(html_to_text=True).load_data(\n",
    "    [\"https://baike.baidu.com/item/%E7%9F%A5%E4%B9%8E%E7%9F%A5%E5%AD%A6%E5%A0%82/64028760\"]\n",
    ")\n",
    "\n",
    "print(documents[0].text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02a88893-01b8-44a1-9bed-e93e60cec328",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "<b>更多 Data Connectors</b>\n",
    "    <ul>\n",
    "        <li>内置的<a href=\"https://llamahub.ai/l/readers/llama-index-readers-file\">文件加载器</a></li>\n",
    "        <li>连接三方服务的<a href=\"https://docs.llamaindex.ai/en/stable/module_guides/loading/connector/modules/\">数据加载器</a>，例如数据库</li>\n",
    "        <li>更多加载器可以在 <a href=\"https://llamahub.ai/\">LlamaHub</a> 上找到</li>\n",
    "    </ul>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36ed8a42-f6fc-430f-9e92-07736e7f359c",
   "metadata": {},
   "source": [
    "## 4、文本切分与解析（Chunking）\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39ad7ebd-9e56-47f9-8136-2e1098139c01",
   "metadata": {},
   "source": [
    "为方便检索，我们通常把 `Document` 切分为 `Node`。\n",
    "\n",
    "在 LlamaIndex 中，`Node` 被定义为一个文本的「chunk」。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4881c7d9-9107-4704-b322-d3bc34af96f2",
   "metadata": {},
   "source": [
    "### 4.1、使用 TextSplitters 对文本做切分\n",
    "\n",
    "例如：`TokenTextSplitter` 按指定 token 数切分文本\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "69a083f7-cda9-45d9-be3e-397ce866e440",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import Document\n",
    "from llama_index.core.node_parser import TokenTextSplitter\n",
    "\n",
    "node_parser = TokenTextSplitter(\n",
    "    chunk_size=1000,  # 每个 chunk 的最大长度\n",
    "    chunk_overlap=500  # chunk 之间重叠长度 \n",
    ")\n",
    "\n",
    "nodes = node_parser.get_nodes_from_documents(\n",
    "    documents, show_progress=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "81b363de-c5ef-4e20-ad4d-39bccabb3369",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"id_\": \"33ac9624-6d4b-4894-afbf-67c7bc1d0257\",\n",
      "    \"embedding\": null,\n",
      "    \"metadata\": {},\n",
      "    \"excluded_embed_metadata_keys\": [],\n",
      "    \"excluded_llm_metadata_keys\": [],\n",
      "    \"relationships\": {\n",
      "        \"1\": {\n",
      "            \"node_id\": \"https://baike.baidu.com/item/%E7%9F%A5%E4%B9%8E%E7%9F%A5%E5%AD%A6%E5%A0%82/64028760\",\n",
      "            \"node_type\": \"4\",\n",
      "            \"metadata\": {},\n",
      "            \"hash\": \"eacbba43d1c8799964f5b2ac4b82de6fdc0571d0bf2e0c21fe0bf27649ada42c\",\n",
      "            \"class_name\": \"RelatedNodeInfo\"\n",
      "        },\n",
      "        \"2\": {\n",
      "            \"node_id\": \"7704db4e-fa23-4289-a1a5-5ea1fb024fe4\",\n",
      "            \"node_type\": \"1\",\n",
      "            \"metadata\": {},\n",
      "            \"hash\": \"facf5469a06bfabcff6e70288290aee614bf116a9eb4e33b85bfc70e14fcb2bd\",\n",
      "            \"class_name\": \"RelatedNodeInfo\"\n",
      "        },\n",
      "        \"3\": {\n",
      "            \"node_id\": \"d9a63cad-8b9e-442b-819e-df1a25155ef3\",\n",
      "            \"node_type\": \"1\",\n",
      "            \"metadata\": {},\n",
      "            \"hash\": \"90219c2d62580d8e83fcb60370b640246a46d6e4817ffe622c1289dbfe1e97ae\",\n",
      "            \"class_name\": \"RelatedNodeInfo\"\n",
      "        }\n",
      "    },\n",
      "    \"metadata_template\": \"{key}: {value}\",\n",
      "    \"metadata_separator\": \"\\n\",\n",
      "    \"text\": \"秒懂百科\\n\\n    \\n\\n特色百科\\n\\n    \\n\\n知识专题\\n\\n    \\n\\n加入百科\\n\\n    \\n\\n百科团队\\n\\n    \\n\\n权威合作\\n\\n    \\n\\n[个人中心](/usercenter)\\n\\n收藏\\n\\n查看[我的收藏](/uc/favolemma)\\n\\n0有用+1\\n\\n0\\n\\n# 知乎知学堂\\n\\n播报[讨论](/planet/talk?lemmaId=64028760&fromModule=lemma_right-issue-btn)上传视频\\n\\n知乎旗下职业教育品牌\\n\\n知乎知学堂是知乎旗下的职业教育品牌，于2022年12月30日正式发布。知乎知学堂依托自身科技实力，专注于用户职业发展，聚集各行业优质教育资源，打造了一站式在线职业教育平台。\\n[1]__截至2023年底\\n，知乎知学堂集合了品职教育、趴趴教育、MBA大师、一起公考、一起考教师、AGI课堂6个子品牌，完成了6大领域30多个品类课程的覆盖，累计为超过2000万学员提供职业培训服务。\\n[2]__\\n\\n2024年1月11日，知乎知学堂宣布正式独立运营，并公布了未来两大核心战略：第一，通过数字化赋能，改善职业教育服务体系和交付标准；第二，通过开放协作和产教融合，助力职业教育实现高质量就业。\\n[2]__\\n\\n中文名\\n\\n     知乎知学堂\\n\\n所属行业\\n\\n    职业教育\\n\\n创立时间\\n\\n    2022年12月30日\\n\\n所属公司\\n\\n    知乎\\n\\n品牌口号\\n\\n    未来职业之路\\n\\n商标注册号\\n\\n    59736985 [3]__\\n\\n## 目录\\n\\n  1. 1发展历程\\n  2. 2业务矩阵\\n  3. 3旗下品牌\\n  4. 4部分师资\\n  5. 5品牌动态\\n\\n## 发展历程\\n\\n播报\\n\\n编辑\\n\\n2022年6月29日，知乎推出“学习”专区，上线数百门免精罪墓费正版授权课程。 [4]__\\n\\n巴达捉2022年11月1日，知乎上线AI大师免费课程，吴恩达、沈向婆旋洋领衔揭秘人工智能。 [5]__\\n\\n2022年12月30日，知乎发布职业教育品牌——在线职业教育平台「知乎知学堂」。 [6]__\\n\\n2023年2月21日，知乎厦糊联合中国教育在线发布考研数据报告，站内考研兴趣用户1594万。 [7]__\\n\\n2023年7月15日，知乎年度活动「2023年新知青年大会」在北京举行。知乎创始人、董事长兼 CEO\\n周源出席并发表演讲将职业教育作碑旬照为第二增长曲线，同时也宣布「知学计划\",\n",
      "    \"mimetype\": \"text/plain\",\n",
      "    \"start_char_idx\": 1013,\n",
      "    \"end_char_idx\": 2024,\n",
      "    \"metadata_seperator\": \"\\n\",\n",
      "    \"text_template\": \"{metadata_str}\\n\\n{content}\",\n",
      "    \"class_name\": \"TextNode\"\n",
      "}\n",
      "{\n",
      "    \"id_\": \"d9a63cad-8b9e-442b-819e-df1a25155ef3\",\n",
      "    \"embedding\": null,\n",
      "    \"metadata\": {},\n",
      "    \"excluded_embed_metadata_keys\": [],\n",
      "    \"excluded_llm_metadata_keys\": [],\n",
      "    \"relationships\": {\n",
      "        \"1\": {\n",
      "            \"node_id\": \"https://baike.baidu.com/item/%E7%9F%A5%E4%B9%8E%E7%9F%A5%E5%AD%A6%E5%A0%82/64028760\",\n",
      "            \"node_type\": \"4\",\n",
      "            \"metadata\": {},\n",
      "            \"hash\": \"eacbba43d1c8799964f5b2ac4b82de6fdc0571d0bf2e0c21fe0bf27649ada42c\",\n",
      "            \"class_name\": \"RelatedNodeInfo\"\n",
      "        },\n",
      "        \"2\": {\n",
      "            \"node_id\": \"33ac9624-6d4b-4894-afbf-67c7bc1d0257\",\n",
      "            \"node_type\": \"1\",\n",
      "            \"metadata\": {},\n",
      "            \"hash\": \"3d70a560f1015648e7840183cc71f72e028dffed6a22f33e0b3cd52c9a27febd\",\n",
      "            \"class_name\": \"RelatedNodeInfo\"\n",
      "        },\n",
      "        \"3\": {\n",
      "            \"node_id\": \"d46243fb-c175-4f0c-a6f6-ab38723a14f7\",\n",
      "            \"node_type\": \"1\",\n",
      "            \"metadata\": {},\n",
      "            \"hash\": \"54e597ca35d3611ff0085643b5a8f41aa88be906e08b05b8da443c0d9086eae3\",\n",
      "            \"class_name\": \"RelatedNodeInfo\"\n",
      "        }\n",
      "    },\n",
      "    \"metadata_template\": \"{key}: {value}\",\n",
      "    \"metadata_separator\": \"\\n\",\n",
      "    \"text\": \"知乎知学堂\\n\\n所属行业\\n\\n    职业教育\\n\\n创立时间\\n\\n    2022年12月30日\\n\\n所属公司\\n\\n    知乎\\n\\n品牌口号\\n\\n    未来职业之路\\n\\n商标注册号\\n\\n    59736985 [3]__\\n\\n## 目录\\n\\n  1. 1发展历程\\n  2. 2业务矩阵\\n  3. 3旗下品牌\\n  4. 4部分师资\\n  5. 5品牌动态\\n\\n## 发展历程\\n\\n播报\\n\\n编辑\\n\\n2022年6月29日，知乎推出“学习”专区，上线数百门免精罪墓费正版授权课程。 [4]__\\n\\n巴达捉2022年11月1日，知乎上线AI大师免费课程，吴恩达、沈向婆旋洋领衔揭秘人工智能。 [5]__\\n\\n2022年12月30日，知乎发布职业教育品牌——在线职业教育平台「知乎知学堂」。 [6]__\\n\\n2023年2月21日，知乎厦糊联合中国教育在线发布考研数据报告，站内考研兴趣用户1594万。 [7]__\\n\\n2023年7月15日，知乎年度活动「2023年新知青年大会」在北京举行。知乎创始人、董事长兼 CEO\\n周源出席并发表演讲将职业教育作碑旬照为第二增长曲线，同时也宣布「知学计划 」已进入内测阶段，即将对更多有志于提供专业职业教育服务的机构和个人犁轿辣开放。\\n[8]__\\n\\n2023年11月29日，知乎发布2023年Q3财报，职业教育业务营收达到1.探组求45亿元，实现同比增长85.6%。 [9]__\\n\\n2024年1月11日消息，知乎在北京举办「2024知乎教育大会」，宣布职业教育品牌「知乎知学堂腿页求乌」正式独立运营。 [2]__\\n\\n##\",\n",
      "    \"mimetype\": \"text/plain\",\n",
      "    \"start_char_idx\": 1550,\n",
      "    \"end_char_idx\": 2207,\n",
      "    \"metadata_seperator\": \"\\n\",\n",
      "    \"text_template\": \"{metadata_str}\\n\\n{content}\",\n",
      "    \"class_name\": \"TextNode\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "show_json(nodes[1].json())\n",
    "show_json(nodes[2].json())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1f56dbf-4247-4a06-a127-2fa1929455f0",
   "metadata": {},
   "source": [
    "LlamaIndex 提供了丰富的 `TextSplitter`，例如：\n",
    "\n",
    "- [`SentenceSplitter`](https://docs.llamaindex.ai/en/stable/api_reference/node_parsers/sentence_splitter/)：在切分指定长度的 chunk 同时尽量保证句子边界不被切断；\n",
    "- [`CodeSplitter`](https://docs.llamaindex.ai/en/stable/api_reference/node_parsers/code/)：根据 AST（编译器的抽象句法树）切分代码，保证代码功能片段完整；\n",
    "- [`SemanticSplitterNodeParser`](https://docs.llamaindex.ai/en/stable/api_reference/node_parsers/semantic_splitter/)：根据语义相关性对将文本切分为片段。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b692f9fb-af99-4bf5-9d4e-c745438173d6",
   "metadata": {},
   "source": [
    "### 4.2、使用 NodeParsers 对有结构的文档做解析\n",
    "\n",
    "例如：`HTMLNodeParser`解析 HTML 文档\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b7c6f533-a203-42a2-b312-3d89e0cbf22a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "收藏\n",
      "\n",
      "0\n",
      "有用+1\n",
      "\n",
      "\n",
      "播报\n",
      "知乎知学堂是知乎旗下的职业教育品牌，于2022年12月30日正式发布。知乎知学堂依托自身科技实力，专注于用户职业发展，聚集各行业优质教育资源，打造了一站式在线职业教育平台。\n",
      "[1]\n",
      "截至2023年底\n",
      "，知乎知学堂集合了品职教育、趴趴教育、MBA大师、一起公考、一起考教师、AGI课堂6个子品牌，完成了6大领域30多个品类课程的覆盖，累计为超过2000万学员提供职业培训服务。\n",
      "[2]\n",
      "2024年1月11日，知乎知学堂宣布正式独立运营，并公布了未来两大核心战略：第一，通过数字化赋能，改善职业教育服务体系和交付标准；第二，通过开放协作和产教融合，助力职业教育实现高质量就业。\n",
      "[2]\n",
      "知乎知学堂\n",
      "职业教育\n",
      "2022年12月30日\n",
      "知乎\n",
      "未来职业之路\n",
      "59736985\n",
      "[3]\n",
      "1\n",
      "发展历程\n",
      "2\n",
      "业务矩阵\n",
      "3\n",
      "旗下品牌\n",
      "4\n",
      "部分师资\n",
      "5\n",
      "品牌动态\n",
      "\n",
      "\n",
      "播报\n",
      "编辑\n",
      "2022年6月29日，知乎推出“学习”专区，上线数百碑旬照门免费正版授权课程。\n",
      "[4]\n",
      "2022年11月1日，知乎上线AI大师免费课程，吴恩达婆旋、沈向洋领衔揭秘人工智能。\n",
      "[5]\n",
      "2022年12月30日，知乎发布职业教育品牌——在线职业教育平台「知挨罪乎知学堂」。\n",
      "[6]\n",
      "2023年2月21日，知乎联合中国教育在线发布考研数据报告，站内考研兴趣用户1594万。\n",
      "[7]\n",
      "2023年7月15日，知乎年度活动「2023年新知青年大会故兆民」在北京举行。知乎创始人、董事长兼 CEO 周源出席并发表演讲将职业教育作为第二探组求增长曲线项船腊，同时也宣布「知学计划 」已进入内测阶段，即将对更多有志于提供专业职业教育服务的机构和个人开放。\n",
      "[8]\n",
      "2023年11月29日，知乎发布2023年Q3财报，职业教育业务营收达篮埋腊到1.45亿元，实现同比增长85.6%。\n",
      "[9]\n",
      "2024年1月11日消息，知乎在北京举办「2024知乎教育大会」，宣布职业教育品牌「知乎知学堂」正式独立运照尝艰乎营。\n",
      "[2]\n",
      "\n",
      "\n",
      "播报\n",
      "编辑\n",
      "知乎知学堂已经集合了品职教育、趴趴教育、MBA大师、一起公考、一起考教师、AGI课堂6个子品牌，完成了6大领域30多个品类的覆盖，累计为超过2000万学员提供职业培训服务，成为国内覆盖品类最全的职业教育内容体系之一。在考研、公考、教资、CPA、CFA、MBA、职场技能等诸多课程赛道中，知乎都形成了一定的用户口碑和内容影响力。\n",
      "[2]\n",
      "\n",
      "知乎知学堂学科品类\n",
      "[1]\n",
      "\n",
      "\n",
      "播报\n",
      "编辑\n",
      "趴趴教育：知乎知学堂旗下专注留学考试培训以及留学申请的一站式服务机构，开设有雅思、托福、GRE、GMAT 等课程产品，累计为百万学员提供过留学语培服务。\n",
      "[1]\n",
      "品职教育：知乎知学堂旗下专注金融职业培训的子品牌，提供包括 CFA、ESG、FRM、CPA、金融考研等多款财经教育类课程，已累计服务考生 13000+、CFA 持证学员 2000+，并为 ESG 领域输送人才 4000+。\n",
      "[1]\n",
      "MBA大师：知乎知学堂旗下专注管理类联考培训的在线教育品牌，为考生提供精品课程、专属教材及一站式备考服务。「MBA大师」始终致力于为用户提供智能化、个性化、系统化的在线学习体验，累计已服务管理类联考考生超过 300 万人。\n",
      "[1]\n",
      "一起考教师：知乎知学堂旗下专注教师资格证和教师招聘考试培训的教育品牌，累计为超过 1500 万备考教师资格证和教师招聘的考生提供过服务。\n",
      "[1]\n",
      "一起公考：知乎知学堂旗下专注于公务员和事业单位考试培训的教育品牌，累计为超过 200 万备考公务员及事业单位考试的用户提供过服务。\n",
      "[1]\n",
      "AGI课堂：知乎知学堂旗下专注人工智能技术培训的教育品牌，提供大模型算法原理、大模型技术应用、大模型业务解决方案等前沿 AI 课程。「AGI课堂」始终致力于结合时下热门技术路线及高频应用场景，为用户提供一站式人工智能教育解决方案。\n",
      "[1]\n",
      "\n",
      "知乎知学堂旗下品牌\n",
      "[2]\n",
      "\n",
      "\n",
      "播报\n",
      "编辑\n",
      "贾岚     考研英语资深名师、中国人民大学英语语言文学硕士。\n",
      "[1]\n",
      "王瑞恩  中美两国法律职业资格证持证人、本科毕业于清华大学。\n",
      "[1]\n",
      "蔡金龙  公考资深名师、17年执教行业经验、清华大学本硕。\n",
      "[1]\n",
      "李斯克\n",
      "CFA/FRM资深名师、上海交通大学本硕。\n",
      "[1]\n",
      "Paul Winterbottom   前雅思口语写作英籍考官、牛津大学本硕。\n",
      "[1]\n",
      "薛睿\n",
      "管理类联考资深名师、西安交大本科及MBA、利兹硕士。\n",
      "[1]\n",
      "张倩     前红圈所律师、毕业于北京大学法学院。\n",
      "[1]\n",
      "\n",
      "\n",
      "播报\n",
      "编辑\n",
      "2024年1月11日，知乎在北京举办“2024知乎教育大会”。会上，知乎创始人、CEO\n",
      "周源\n",
      "表示，今天的知乎，不止于社区，正转变为一个为新职人搭建平台、提供服务的公司，满足新职人包含职业教育在内的各类需求；知乎高级副总裁、知乎知学堂CEO张荣乐表示，知乎知学堂已经成为国内覆盖品类最全的职业教育内容体系之一，未来将持续推动数字化赋能，用科技重塑职业教育行业，并通过开放协作和产教融合，助力职业教育实现高质量就业。\n",
      "[2]\n",
      "大会嘉宾观点\n",
      "(5张)\n",
      "新手上路\n",
      "我有疑问\n",
      "投诉建议\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core.node_parser import HTMLNodeParser\n",
    "from llama_index.readers.web import SimpleWebPageReader\n",
    "\n",
    "documents = SimpleWebPageReader(html_to_text=False).load_data(\n",
    "    [\"https://baike.baidu.com/item/%E7%9F%A5%E4%B9%8E%E7%9F%A5%E5%AD%A6%E5%A0%82/64028760\"]\n",
    ")\n",
    "\n",
    "# 默认解析 [\"p\", \"h1\", \"h2\", \"h3\", \"h4\", \"h5\", \"h6\", \"li\", \"b\", \"i\", \"u\", \"section\"]\n",
    "parser = HTMLNodeParser(tags=[\"span\"])  # 可以自定义解析哪些标签\n",
    "nodes = parser.get_nodes_from_documents(documents)\n",
    "\n",
    "for node in nodes:\n",
    "    print(node.text+\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bdc613c-36a8-4afb-871b-097496703eaf",
   "metadata": {},
   "source": [
    "更多的 `NodeParser` 包括 [`MarkdownNodeParser`](https://docs.llamaindex.ai/en/stable/api_reference/node_parsers/markdown/)，[`JSONNodeParser`](https://docs.llamaindex.ai/en/stable/api_reference/node_parsers/json/)等等。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8b8557f-20af-477d-918d-14761a9c986d",
   "metadata": {},
   "source": [
    "## 5、索引（Indexing）与检索（Retrieval）\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df338b16-df37-412d-a385-2d1f4b681112",
   "metadata": {},
   "source": [
    "**基础概念**：在「检索」相关的上下文中，「索引」即`index`， 通常是指为了实现快速检索而设计的特定「数据结构」。\n",
    "\n",
    "索引的具体原理与实现不是本课程的教学重点，感兴趣的同学可以参考：[传统索引](https://en.wikipedia.org/wiki/Search_engine_indexing)、[向量索引](https://medium.com/kx-systems/vector-indexing-a-roadmap-for-vector-databases-65866f07daf5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dd397fe-8932-49de-ac37-bed0b585205c",
   "metadata": {},
   "source": [
    "### 5.1、向量检索\n",
    "\n",
    "1. `SimpleVectorStore` 直接在内存中构建一个 Vector Store 并建索引\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "4ea17e80-d25c-43ac-b9b5-983c6acb3adb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "it surpasses these models in Chinese factual knowledge (Chinese\n",
      "SimpleQA), highlighting its strength in Chinese factual knowledge.\n",
      "• Code, Math, and Reasoning: (1) DeepSeek-V3 achieves state-of-the-art performance on\n",
      "math-related benchmarks among all non-long-CoT open-source and closed-source models.\n",
      "Notably, it even outperforms o1-preview on specific benchmarks, such as MATH-500,\n",
      "demonstrating its robust mathematical reasoning capabilities. (2) On coding-related tasks,\n",
      "DeepSeek-V3 emerges as the top-performing model for coding competition benchmarks,\n",
      "such as LiveCodeBench, solidifying its position as the leading model in this domain. For\n",
      "engineering-related tasks, while DeepSeek-V3 performs slightly below Claude-Sonnet-3.5,\n",
      "it still outpaces all other models by a significant margin, demonstrating its competitiveness\n",
      "across diverse technical benchmarks.\n",
      "In the remainder of this paper, we first present a detailed exposition of our DeepSeek-V3\n",
      "model architecture (Section 2). Subsequently, we introduce our infrastructures, encompassing\n",
      "our compute clusters, the training framework, the support for FP8 training, the inference\n",
      "deployment strategy, and our suggestions on future hardware design. Next, we describe our\n",
      "pre-training process, including the construction of training data, hyper-parameter settings, long-\n",
      "context extension techniques, the associated evaluations, as well as some discussions (Section 4).\n",
      "Thereafter, we discuss our efforts on post-training, which include Supervised Fine-Tuning (SFT),\n",
      "Reinforcement Learning (RL), the corresponding evaluations, and discussions (Section 5). Lastly,\n",
      "we conclude this work, discuss existing limitations of DeepSeek-V3, and propose potential\n",
      "directions for future research (Section 6).\n",
      "2. Architecture\n",
      "We first introduce the basic architecture of DeepSeek-V3, featured by Multi-head Latent Atten-\n",
      "tion (MLA) (DeepSeek-AI, 2024c) for efficient inference and DeepSeekMoE (Dai et al., 2024)\n",
      "for economical training. Then, we present a Multi-Token Prediction (MTP) training objective,\n",
      "which we have observed to enhance the overall performance on evaluation benchmarks. For\n",
      "other minor details not explicitly mentioned,\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core import VectorStoreIndex, SimpleDirectoryReader\n",
    "from llama_index.core.node_parser import TokenTextSplitter\n",
    "\n",
    "# 加载 pdf 文档\n",
    "documents = SimpleDirectoryReader(\n",
    "    \"./data\", \n",
    "    required_exts=[\".pdf\"],\n",
    ").load_data()\n",
    "\n",
    "# 定义 Node Parser\n",
    "node_parser = TokenTextSplitter(chunk_size=500, chunk_overlap=250)\n",
    "\n",
    "# 切分文档\n",
    "nodes = node_parser.get_nodes_from_documents(documents)\n",
    "\n",
    "# 构建 index\n",
    "index = VectorStoreIndex(nodes)\n",
    "\n",
    "# 获取 retriever\n",
    "vector_retriever = index.as_retriever(\n",
    "    similarity_top_k=2 # 返回2个结果\n",
    ")\n",
    "\n",
    "# 检索\n",
    "results = vector_retriever.retrieve(\"deepseek v3数学能力怎么样\")\n",
    "\n",
    "print(results[0].text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaeca7fd-5606-45a4-8443-ddab6b9e413f",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "<p>LlamaIndex 默认的 Embedding 模型是 <code>OpenAIEmbedding(model=\"text-embedding-ada-002\")</code>。</p>\n",
    "<p>如何替换指定的 Embedding 模型见后面章节详解。</p>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90ebec20-7c12-4d2d-a4f4-5cb2abcc5f32",
   "metadata": {},
   "source": [
    "2. 使用自定义的 Vector Store，以 `Qdrant` 为例：\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b5e7648-0598-4df7-923f-42fe2f172da3",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install llama-index-vector-stores-qdrant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "a65a38da-3e71-4fa8-872d-eff6b4f3856b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Node ID: c6f4bdf6-ee82-4a19-b238-ab43cc8f53f3\n",
      "Text: it surpasses these models in Chinese factual knowledge (Chinese\n",
      "SimpleQA), highlighting its strength in Chinese factual knowledge. •\n",
      "Code, Math, and Reasoning: (1) DeepSeek-V3 achieves state-of-the-art\n",
      "performance on math-related benchmarks among all non-long-CoT open-\n",
      "source and closed-source models. Notably, it even outperforms\n",
      "o1-preview on sp...\n",
      "Score:  0.834\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core.indices.vector_store.base import VectorStoreIndex\n",
    "from llama_index.vector_stores.qdrant import QdrantVectorStore\n",
    "from llama_index.core import StorageContext\n",
    "\n",
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.models import VectorParams, Distance\n",
    "\n",
    "client = QdrantClient(location=\":memory:\")\n",
    "collection_name = \"demo\"\n",
    "collection = client.create_collection(\n",
    "    collection_name=collection_name,\n",
    "    vectors_config=VectorParams(size=1536, distance=Distance.COSINE)\n",
    ")\n",
    "\n",
    "vector_store = QdrantVectorStore(client=client, collection_name=collection_name)\n",
    "# storage: 指定存储空间\n",
    "storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
    "\n",
    "# 创建 index：通过 Storage Context 关联到自定义的 Vector Store\n",
    "index = VectorStoreIndex(nodes, storage_context=storage_context)\n",
    "\n",
    "# 获取 retriever\n",
    "vector_retriever = index.as_retriever(similarity_top_k=1)\n",
    "\n",
    "# 检索\n",
    "results = vector_retriever.retrieve(\"deepseek v3数学能力怎么样\")\n",
    "\n",
    "print(results[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9913e5d5-7c84-4833-b21d-2fe440749a63",
   "metadata": {},
   "source": [
    "### 5.2、更多索引与检索方式\n",
    "\n",
    "LlamaIndex 内置了丰富的检索机制，例如：\n",
    "\n",
    "- 关键字检索\n",
    "\n",
    "  - [`BM25Retriever`](https://docs.llamaindex.ai/en/stable/api_reference/retrievers/bm25/)：基于 tokenizer 实现的 BM25 经典检索算法\n",
    "  - [`KeywordTableGPTRetriever`](https://docs.llamaindex.ai/en/stable/api_reference/retrievers/keyword/#llama_index.core.indices.keyword_table.retrievers.KeywordTableGPTRetriever)：使用 GPT 提取检索关键字\n",
    "  - [`KeywordTableSimpleRetriever`](https://docs.llamaindex.ai/en/stable/api_reference/retrievers/keyword/#llama_index.core.indices.keyword_table.retrievers.KeywordTableSimpleRetriever)：使用正则表达式提取检索关键字\n",
    "  - [`KeywordTableRAKERetriever`](https://docs.llamaindex.ai/en/stable/api_reference/retrievers/keyword/#llama_index.core.indices.keyword_table.retrievers.KeywordTableRAKERetriever)：使用[`RAKE`](https://pypi.org/project/rake-nltk/)算法提取检索关键字（有语言限制）\n",
    "\n",
    "- RAG-Fusion [`QueryFusionRetriever`](https://docs.llamaindex.ai/en/stable/api_reference/retrievers/query_fusion/)\n",
    "\n",
    "- 还支持 [KnowledgeGraph](https://docs.llamaindex.ai/en/stable/api_reference/retrievers/knowledge_graph/)、[SQL](https://docs.llamaindex.ai/en/stable/api_reference/retrievers/sql/#llama_index.core.retrievers.SQLRetriever)、[Text-to-SQL](https://docs.llamaindex.ai/en/stable/api_reference/retrievers/sql/#llama_index.core.retrievers.NLSQLRetriever) 等等\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af53e99b-2ab5-4520-ba96-4a9979a94480",
   "metadata": {},
   "source": [
    "### 5.3、检索后处理\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13c53c7c-9077-42bc-a5c0-832688b352b8",
   "metadata": {},
   "source": [
    "LlamaIndex 的 `Node Postprocessors` 提供了一系列检索后处理模块。\n",
    "\n",
    "例如：我们可以用不同模型对检索后的 `Nodes` 做重排序\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "7176f29c-be6b-491c-b2e8-6e604ad201b4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0] pipeline bubbles and hides most of the communication during training through\n",
      "computation-communication overlap. This overlap ensures that, as the model further scales up,\n",
      "as long as we maintain a constant computation-to-communication ratio, we can still employ\n",
      "fine-grained experts across nodes while achieving a near-zero all-to-all communication overhead.\n",
      "In addition, we also develop efficient cross-node all-to-all communication kernels to fully utilize\n",
      "InfiniBand (IB) and NVLink bandwidths. Furthermore, we meticulously optimize the memory\n",
      "footprint, making it possible to train DeepSeek-V3 without using costly tensor parallelism.\n",
      "Combining these efforts, we achieve high training efficiency.\n",
      "During pre-training, we train DeepSeek-V3 on 14.8T high-quality and diverse tokens. The\n",
      "pre-training process is remarkably stable. Throughout the entire training process, we did not\n",
      "encounter any irrecoverable loss spikes or have to roll back. Next, we conduct a two-stage\n",
      "context length extension for DeepSeek-V3. In the first stage, the maximum context length is\n",
      "extended to 32K, and in the second stage, it is further extended to 128K. Following this, we\n",
      "conduct post-training, including Supervised Fine-Tuning (SFT) and Reinforcement Learning (RL)\n",
      "on the base model of DeepSeek-V3, to align it with human preferences and further unlock its\n",
      "potential. During the post-training stage, we distill the reasoning capability from the DeepSeek-\n",
      "R1 series of models, and meanwhile carefully maintain the balance between model accuracy\n",
      "4\n",
      "\n",
      "[1] Training Costs Pre-Training Context Extension Post-Training Total\n",
      "in H800 GPU Hours 2664K 119K 5K 2788K\n",
      "in USD $5.328M $0.238M $0.01M $5.576M\n",
      "Table 1 |Training costs of DeepSeek-V3, assuming the rental price of H800 is $2 per GPU hour.\n",
      "and generation length.\n",
      "We evaluate DeepSeek-V3 on a comprehensive array of benchmarks. Despite its economical\n",
      "training costs, comprehensive evaluations reveal that DeepSeek-V3-Base has emerged as the\n",
      "strongest open-source base model currently available, especially in code and math. Its chat\n",
      "version also outperforms other open-source models and achieves performance comparable to\n",
      "leading closed-source models, including GPT-4o and Claude-3.5-Sonnet, on a series of standard\n",
      "and open-ended benchmarks.\n",
      "Lastly, we emphasize again the economical training costs of DeepSeek-V3, summarized in\n",
      "Table 1, achieved through our optimized co-design of algorithms, frameworks, and hardware.\n",
      "During the pre-training stage, training DeepSeek-V3 on each trillion tokens requires only 180K\n",
      "H800 GPU hours, i.e., 3.7 days on our cluster with 2048 H800 GPUs. Consequently, our pre-\n",
      "training stage is completed in less than two months and costs 2664K GPU hours. Combined\n",
      "with 119K GPU hours for the context length extension and 5K GPU hours for post-training,\n",
      "DeepSeek-V3 costs only 2.788M GPU hours for its full training. Assuming the rental price of\n",
      "the H800 GPU is $2 per GPU hour, our total training costs amount to only $5.576M. Note that\n",
      "the aforementioned costs include only the official training of DeepSeek-V3, excluding the costs\n",
      "associated with prior research and ablation experiments on architectures, algorithms, or data.\n",
      "Our main contribution includes:\n",
      "Architecture: Innovative Load Balancing Strategy and Training Objective\n",
      "• On top of the efficient architecture of DeepSeek-V2, we pioneer an auxiliary-loss-free\n",
      "strategy for load balancing, which minimizes the performance degradation that arises\n",
      "from encouraging load balancing.\n",
      "• We investigate a Multi-Token Prediction (MTP) objective\n",
      "\n",
      "[2] algorithms, frameworks, and hardware.\n",
      "During the pre-training stage, training DeepSeek-V3 on each trillion tokens requires only 180K\n",
      "H800 GPU hours, i.e., 3.7 days on our cluster with 2048 H800 GPUs. Consequently, our pre-\n",
      "training stage is completed in less than two months and costs 2664K GPU hours. Combined\n",
      "with 119K GPU hours for the context length extension and 5K GPU hours for post-training,\n",
      "DeepSeek-V3 costs only 2.788M GPU hours for its full training. Assuming the rental price of\n",
      "the H800 GPU is $2 per GPU hour, our total training costs amount to only $5.576M. Note that\n",
      "the aforementioned costs include only the official training of DeepSeek-V3, excluding the costs\n",
      "associated with prior research and ablation experiments on architectures, algorithms, or data.\n",
      "Our main contribution includes:\n",
      "Architecture: Innovative Load Balancing Strategy and Training Objective\n",
      "• On top of the efficient architecture of DeepSeek-V2, we pioneer an auxiliary-loss-free\n",
      "strategy for load balancing, which minimizes the performance degradation that arises\n",
      "from encouraging load balancing.\n",
      "• We investigate a Multi-Token Prediction (MTP) objective and prove it beneficial to model\n",
      "performance. It can also be used for speculative decoding for inference acceleration.\n",
      "Pre-Training: Towards Ultimate Training Efficiency\n",
      "• We design an FP8 mixed precision training framework and, for the first time, validate the\n",
      "feasibility and effectiveness of FP8 training on an extremely large-scale model.\n",
      "• Through the co-design of algorithms, frameworks, and hardware, we overcome the\n",
      "communication bottleneck in cross-node MoE training, achieving near-full computation-\n",
      "communication overlap. This significantly enhances our training efficiency and reduces the\n",
      "training costs, enabling us to further scale up the model size without additional overhead.\n",
      "• At an economical cost of only 2.664M H800 GPU hours, we complete the pre-training of\n",
      "DeepSeek-V3 on 14.8T tokens, producing the currently strongest open-source base model.\n",
      "The subsequent training stages after pre-training require only 0.1M GPU hours.\n",
      "Post-Training: Knowledge Distillation from DeepSeek-R1\n",
      "• We introduce an innovative methodology to distill reasoning capabilities from the\n",
      "\n",
      "[3] it surpasses these models in Chinese factual knowledge (Chinese\n",
      "SimpleQA), highlighting its strength in Chinese factual knowledge.\n",
      "• Code, Math, and Reasoning: (1) DeepSeek-V3 achieves state-of-the-art performance on\n",
      "math-related benchmarks among all non-long-CoT open-source and closed-source models.\n",
      "Notably, it even outperforms o1-preview on specific benchmarks, such as MATH-500,\n",
      "demonstrating its robust mathematical reasoning capabilities. (2) On coding-related tasks,\n",
      "DeepSeek-V3 emerges as the top-performing model for coding competition benchmarks,\n",
      "such as LiveCodeBench, solidifying its position as the leading model in this domain. For\n",
      "engineering-related tasks, while DeepSeek-V3 performs slightly below Claude-Sonnet-3.5,\n",
      "it still outpaces all other models by a significant margin, demonstrating its competitiveness\n",
      "across diverse technical benchmarks.\n",
      "In the remainder of this paper, we first present a detailed exposition of our DeepSeek-V3\n",
      "model architecture (Section 2). Subsequently, we introduce our infrastructures, encompassing\n",
      "our compute clusters, the training framework, the support for FP8 training, the inference\n",
      "deployment strategy, and our suggestions on future hardware design. Next, we describe our\n",
      "pre-training process, including the construction of training data, hyper-parameter settings, long-\n",
      "context extension techniques, the associated evaluations, as well as some discussions (Section 4).\n",
      "Thereafter, we discuss our efforts on post-training, which include Supervised Fine-Tuning (SFT),\n",
      "Reinforcement Learning (RL), the corresponding evaluations, and discussions (Section 5). Lastly,\n",
      "we conclude this work, discuss existing limitations of DeepSeek-V3, and propose potential\n",
      "directions for future research (Section 6).\n",
      "2. Architecture\n",
      "We first introduce the basic architecture of DeepSeek-V3, featured by Multi-head Latent Atten-\n",
      "tion (MLA) (DeepSeek-AI, 2024c) for efficient inference and DeepSeekMoE (Dai et al., 2024)\n",
      "for economical training. Then, we present a Multi-Token Prediction (MTP) training objective,\n",
      "which we have observed to enhance the overall performance on evaluation benchmarks. For\n",
      "other minor details not explicitly mentioned,\n",
      "\n",
      "[4] performance comparable to leading closed-source\n",
      "models. Despite its excellent performance, DeepSeek-V3 requires only 2.788M H800 GPU hours\n",
      "for its full training. In addition, its training process is remarkably stable. Throughout the entire\n",
      "training process, we did not experience any irrecoverable loss spikes or perform any rollbacks.\n",
      "The model checkpoints are available at https://github.com/deepseek-ai/DeepSeek-V3.\n",
      "MMLU-Pro\n",
      "(EM)\n",
      "GPQA-Diamond\n",
      "(Pass@1)\n",
      "MATH 500\n",
      "(EM)\n",
      "AIME 2024\n",
      "(Pass@1)\n",
      "Codeforces\n",
      "(Percentile)\n",
      "SWE-bench Verified\n",
      "(Resolved)\n",
      "0\n",
      "20\n",
      "40\n",
      "60\n",
      "80\n",
      "100Accuracy / Percentile (%)\n",
      "75.9\n",
      "59.1\n",
      "90.2\n",
      "39.2\n",
      "51.6\n",
      "42.0\n",
      "66.2\n",
      "41.3\n",
      "74.7\n",
      "16.7\n",
      "35.6\n",
      "22.6\n",
      "71.6\n",
      "49.0\n",
      "80.0\n",
      "23.3 24.8 23.8\n",
      "73.3\n",
      "51.1\n",
      "73.8\n",
      "23.3\n",
      "25.3 24.5\n",
      "72.6\n",
      "49.9\n",
      "74.6\n",
      "9.3\n",
      "23.6\n",
      "38.8\n",
      "78.0\n",
      "65.0\n",
      "78.3\n",
      "16.0\n",
      "20.3\n",
      "50.8\n",
      "DeepSeek-V3 DeepSeek-V2.5 Qwen2.5-72B-Inst Llama-3.1-405B-Inst GPT-4o-0513 Claude-3.5-Sonnet-1022\n",
      "Figure 1 |Benchmark performance of DeepSeek-V3 and its counterparts.\n",
      "arXiv:2412.19437v2  [cs.CL]  18 Feb 2025\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 获取 retriever\n",
    "vector_retriever = index.as_retriever(similarity_top_k=5)\n",
    "\n",
    "# 检索\n",
    "nodes = vector_retriever.retrieve(\"deepseek v3需要多少训练资源?\")\n",
    "\n",
    "for i, node in enumerate(nodes):\n",
    "    print(f\"[{i}] {node.text}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "65bae2b4-7c36-44fa-9566-0d1b3f34972a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0] Training Costs Pre-Training Context Extension Post-Training Total\n",
      "in H800 GPU Hours 2664K 119K 5K 2788K\n",
      "in USD $5.328M $0.238M $0.01M $5.576M\n",
      "Table 1 |Training costs of DeepSeek-V3, assuming the rental price of H800 is $2 per GPU hour.\n",
      "and generation length.\n",
      "We evaluate DeepSeek-V3 on a comprehensive array of benchmarks. Despite its economical\n",
      "training costs, comprehensive evaluations reveal that DeepSeek-V3-Base has emerged as the\n",
      "strongest open-source base model currently available, especially in code and math. Its chat\n",
      "version also outperforms other open-source models and achieves performance comparable to\n",
      "leading closed-source models, including GPT-4o and Claude-3.5-Sonnet, on a series of standard\n",
      "and open-ended benchmarks.\n",
      "Lastly, we emphasize again the economical training costs of DeepSeek-V3, summarized in\n",
      "Table 1, achieved through our optimized co-design of algorithms, frameworks, and hardware.\n",
      "During the pre-training stage, training DeepSeek-V3 on each trillion tokens requires only 180K\n",
      "H800 GPU hours, i.e., 3.7 days on our cluster with 2048 H800 GPUs. Consequently, our pre-\n",
      "training stage is completed in less than two months and costs 2664K GPU hours. Combined\n",
      "with 119K GPU hours for the context length extension and 5K GPU hours for post-training,\n",
      "DeepSeek-V3 costs only 2.788M GPU hours for its full training. Assuming the rental price of\n",
      "the H800 GPU is $2 per GPU hour, our total training costs amount to only $5.576M. Note that\n",
      "the aforementioned costs include only the official training of DeepSeek-V3, excluding the costs\n",
      "associated with prior research and ablation experiments on architectures, algorithms, or data.\n",
      "Our main contribution includes:\n",
      "Architecture: Innovative Load Balancing Strategy and Training Objective\n",
      "• On top of the efficient architecture of DeepSeek-V2, we pioneer an auxiliary-loss-free\n",
      "strategy for load balancing, which minimizes the performance degradation that arises\n",
      "from encouraging load balancing.\n",
      "• We investigate a Multi-Token Prediction (MTP) objective\n",
      "[1] algorithms, frameworks, and hardware.\n",
      "During the pre-training stage, training DeepSeek-V3 on each trillion tokens requires only 180K\n",
      "H800 GPU hours, i.e., 3.7 days on our cluster with 2048 H800 GPUs. Consequently, our pre-\n",
      "training stage is completed in less than two months and costs 2664K GPU hours. Combined\n",
      "with 119K GPU hours for the context length extension and 5K GPU hours for post-training,\n",
      "DeepSeek-V3 costs only 2.788M GPU hours for its full training. Assuming the rental price of\n",
      "the H800 GPU is $2 per GPU hour, our total training costs amount to only $5.576M. Note that\n",
      "the aforementioned costs include only the official training of DeepSeek-V3, excluding the costs\n",
      "associated with prior research and ablation experiments on architectures, algorithms, or data.\n",
      "Our main contribution includes:\n",
      "Architecture: Innovative Load Balancing Strategy and Training Objective\n",
      "• On top of the efficient architecture of DeepSeek-V2, we pioneer an auxiliary-loss-free\n",
      "strategy for load balancing, which minimizes the performance degradation that arises\n",
      "from encouraging load balancing.\n",
      "• We investigate a Multi-Token Prediction (MTP) objective and prove it beneficial to model\n",
      "performance. It can also be used for speculative decoding for inference acceleration.\n",
      "Pre-Training: Towards Ultimate Training Efficiency\n",
      "• We design an FP8 mixed precision training framework and, for the first time, validate the\n",
      "feasibility and effectiveness of FP8 training on an extremely large-scale model.\n",
      "• Through the co-design of algorithms, frameworks, and hardware, we overcome the\n",
      "communication bottleneck in cross-node MoE training, achieving near-full computation-\n",
      "communication overlap. This significantly enhances our training efficiency and reduces the\n",
      "training costs, enabling us to further scale up the model size without additional overhead.\n",
      "• At an economical cost of only 2.664M H800 GPU hours, we complete the pre-training of\n",
      "DeepSeek-V3 on 14.8T tokens, producing the currently strongest open-source base model.\n",
      "The subsequent training stages after pre-training require only 0.1M GPU hours.\n",
      "Post-Training: Knowledge Distillation from DeepSeek-R1\n",
      "• We introduce an innovative methodology to distill reasoning capabilities from the\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core.postprocessor import LLMRerank\n",
    "\n",
    "postprocessor = LLMRerank(top_n=2)\n",
    "\n",
    "nodes = postprocessor.postprocess_nodes(nodes, query_str=\"deepseek v3需要多少训练资源?\")\n",
    "\n",
    "for i, node in enumerate(nodes):\n",
    "    print(f\"[{i}] {node.text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69ca41ea-3112-491a-a1a1-5ec8db295b61",
   "metadata": {},
   "source": [
    "更多的 Rerank 及其它后处理方法，参考官方文档：[Node Postprocessor Modules](https://docs.llamaindex.ai/en/stable/module_guides/querying/node_postprocessors/node_postprocessors/)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "392d8d69-cd42-445c-a0cd-0dd7c66d07bc",
   "metadata": {},
   "source": [
    "## 6、生成回复（QA & Chat）\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d4248c1-602c-4df6-bd6a-72e994faf1ed",
   "metadata": {},
   "source": [
    "### 6.1、单轮问答（Query Engine）\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "261d35c3-8b54-4840-ab88-c04b348b0fb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DeepSeek-V3有671B总参数，每个token激活了37B参数。\n"
     ]
    }
   ],
   "source": [
    "qa_engine = index.as_query_engine()\n",
    "response = qa_engine.query(\"deepseek v3有多少参数?\")\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44d55521-5b67-4c26-a8de-440d2e1046a7",
   "metadata": {},
   "source": [
    "#### 流式输出\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "1817400e-9311-4161-89e4-507267862524",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DeepSeek-V3有671B总参数，每个token激活了37B参数。"
     ]
    }
   ],
   "source": [
    "qa_engine = index.as_query_engine(streaming=True)\n",
    "response = qa_engine.query(\"deepseek v3有多少参数?\")\n",
    "response.print_response_stream()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cf53205-beb2-4571-81aa-c5f8e12142f5",
   "metadata": {},
   "source": [
    "### 6.2、多轮对话（Chat Engine）\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "81a73a48-469e-4d40-adf3-e0e5ec44305a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DeepSeek-V3 has a total of 671 billion parameters, with 37 billion parameters activated for each token.\n"
     ]
    }
   ],
   "source": [
    "chat_engine = index.as_chat_engine()\n",
    "response = chat_engine.chat(\"deepseek v3有多少参数?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "712c8c71-6420-4c6f-821c-1bd1c31dc8bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "每个令牌激活了37亿个参数。\n"
     ]
    }
   ],
   "source": [
    "response = chat_engine.chat(\"每次激活多少?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3094d188-2241-4995-9704-1d2aeb878e1e",
   "metadata": {},
   "source": [
    "#### 流式输出\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "bf33effa-ad36-4eea-b1d1-926ca5cc8dff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DeepSeek-V3有6710亿个总参数，每个标记激活了370亿个参数。"
     ]
    }
   ],
   "source": [
    "chat_engine = index.as_chat_engine()\n",
    "streaming_response = chat_engine.stream_chat(\"deepseek v3有多少参数?\")\n",
    "# streaming_response.print_response_stream()\n",
    "for token in streaming_response.response_gen:\n",
    "    print(token, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7eb76fa-e4ba-4c47-9785-53eadddf478e",
   "metadata": {},
   "source": [
    "## 7、底层接口：Prompt、LLM 与 Embedding\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da0cce84-9f79-41ef-a6c4-caaeb70d029e",
   "metadata": {},
   "source": [
    "### 7.1、Prompt 模板\n",
    "\n",
    "#### `PromptTemplate` 定义提示词模板\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "41358d83-4fcb-430c-9c2e-c1ac0839cbfe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'写一个关于小明的笑话'"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from llama_index.core import PromptTemplate\n",
    "\n",
    "prompt = PromptTemplate(\"写一个关于{topic}的笑话\")\n",
    "\n",
    "prompt.format(topic=\"小明\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4a94232-e8d5-4bf2-8751-8851d924fcee",
   "metadata": {},
   "source": [
    "#### `ChatPromptTemplate` 定义多轮消息模板\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "edb05592-835f-4cb2-bfa0-862796c36fdf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "system: 你叫小明，你必须根据用户提供的上下文回答问题。\n",
      "user: 已知上下文：\n",
      "这是一个测试\n",
      "\n",
      "问题：这是什么\n",
      "assistant: \n"
     ]
    }
   ],
   "source": [
    "from llama_index.core.llms import ChatMessage, MessageRole\n",
    "from llama_index.core import ChatPromptTemplate\n",
    "\n",
    "chat_text_qa_msgs = [\n",
    "    ChatMessage(\n",
    "        role=MessageRole.SYSTEM,\n",
    "        content=\"你叫{name}，你必须根据用户提供的上下文回答问题。\",\n",
    "    ),\n",
    "    ChatMessage(\n",
    "        role=MessageRole.USER, \n",
    "        content=(\n",
    "            \"已知上下文：\\n\" \\\n",
    "            \"{context}\\n\\n\" \\\n",
    "            \"问题：{question}\"\n",
    "        )\n",
    "    ),\n",
    "]\n",
    "text_qa_template = ChatPromptTemplate(chat_text_qa_msgs)\n",
    "\n",
    "print(\n",
    "    text_qa_template.format(\n",
    "        name=\"小明\",\n",
    "        context=\"这是一个测试\",\n",
    "        question=\"这是什么\"\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c98931d9-f411-4433-9900-649f74a40b6e",
   "metadata": {},
   "source": [
    "### 7.2、语言模型\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "1febbfc2-06c1-4692-9ac1-8843e5554098",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.llms.openai import OpenAI\n",
    "\n",
    "llm = OpenAI(temperature=0, model=\"gpt-4o\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "0c0d3842-32ff-4510-9853-47e7282b56e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "小明有一天去参加一个智力竞赛，主持人问他：“小明，请用‘因为’和‘所以’造一个句子。”\n",
      "\n",
      "小明想了想，说：“因为今天我没带作业，所以我只能站在这里回答问题。”\n"
     ]
    }
   ],
   "source": [
    "response = llm.complete(prompt.format(topic=\"小明\"))\n",
    "\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d0cbe1cc-9dfc-4274-8cdd-b4dd6d3eb5bb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "我是瓜瓜，你的智能助手。根据你提供的上下文，我们正在进行一个测试。请让我知道如果你有其他问题或需要进一步的帮助！\n"
     ]
    }
   ],
   "source": [
    "response = llm.complete(\n",
    "    text_qa_template.format(\n",
    "        name=\"小明\",\n",
    "        context=\"这是一个测试\",\n",
    "        question=\"你是谁，我们在干嘛\"\n",
    "    )\n",
    ")\n",
    "\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4234c43f-d46c-4755-afb3-ade10c308bf8",
   "metadata": {},
   "source": [
    "#### 连接DeepSeek"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad92e176-6153-4059-a01b-e2957a052a6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install llama-index-llms-deepseek"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "511a86bd-c5eb-45e7-8e0a-7e56b2fe7f7d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CompletionResponse(text='😂 熊猫去手机店买手机，店员热情推荐最新款，熊猫却摆摆手说：“算了，这手机不适合我。”  \\n店员不解：“为啥呀？功能超强的！”  \\n熊猫指了指黑眼圈：“美颜开到最大都救不了我的黑眼圈，买了有啥用？”  \\n\\n（店员默默把美白滤镜广告牌藏到了身后）', additional_kwargs={}, raw=ChatCompletion(id='375b81e2-08c2-4601-9ae4-eea7d1bb2a45', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='😂 熊猫去手机店买手机，店员热情推荐最新款，熊猫却摆摆手说：“算了，这手机不适合我。”  \\n店员不解：“为啥呀？功能超强的！”  \\n熊猫指了指黑眼圈：“美颜开到最大都救不了我的黑眼圈，买了有啥用？”  \\n\\n（店员默默把美白滤镜广告牌藏到了身后）', refusal=None, role='assistant', audio=None, function_call=None, tool_calls=None, reasoning_content='好的，用户让我写个笑话。首先，我需要确定用户的需求是什么。他们可能只是想轻松一下，或者需要用在某个场合，比如演讲、聚会，或者只是日常娱乐。接下来，我得考虑笑话的类型，是冷笑话、双关语、还是情景喜剧类型的。\\n\\n用户可能没有说明具体类型，所以我要选一个普遍受欢迎的类型。双关语或者日常生活相关的笑话通常比较安全，容易引起共鸣。然后，我需要确保笑话不会涉及敏感话题，比如种族、宗教、政治等，避免冒犯任何人。\\n\\n接下来，构思笑话的结构。通常笑话有设定和笑点两部分。设定需要简洁明了，让听众能迅速进入情境。笑点要出乎意料，但又在情理之中。比如，利用谐音字或者反转。\\n\\n比如，可以想到一个关于动物的笑话，因为它们比较中性，容易接受。比如熊猫和北极熊的对比。熊猫为什么不用智能手机？然后反转答案，可能利用它们的颜色特点，比如黑眼圈和白毛，导致自拍问题。这样既有趣又无害。\\n\\n然后检查笑话是否原创，有没有听过类似的。虽然很多笑话结构相似，但内容上要尽量创新。同时，语言要口语化，避免复杂句子，让笑话更容易理解。最后，确认笑点是否明显，有没有可能让人听不懂，可能需要调整用词或结构，确保笑点清晰。\\n\\n总结一下，先确定类型，构思情境，制造反转，检查 appropriateness 和原创性，调整语言表达。这样应该能写出符合用户需求的笑话。'))], created=1741760307, model='deepseek-reasoner', object='chat.completion', service_tier=None, system_fingerprint='fp_5417b77867_prod0311fp8', usage=CompletionUsage(completion_tokens=387, prompt_tokens=8, total_tokens=395, completion_tokens_details=CompletionTokensDetails(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=312, rejected_prediction_tokens=None), prompt_tokens_details=PromptTokensDetails(audio_tokens=None, cached_tokens=0), prompt_cache_hit_tokens=0, prompt_cache_miss_tokens=8)), logprobs=None, delta=None)"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from llama_index.llms.deepseek import DeepSeek\n",
    "\n",
    "llm = DeepSeek(model=\"deepseek-reasoner\", api_key=os.environ[\"DEEPSEEK_API_KEY\"])\n",
    "\n",
    "llm.complete(\"写个笑话\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f49f6da-d220-4c36-b697-e3b525902af7",
   "metadata": {},
   "source": [
    "#### 设置全局使用的语言模型\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "94943bec-563c-44b5-80bb-9c0a7c05382c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import Settings\n",
    "\n",
    "Settings.llm = OpenAI(temperature=0, model=\"gpt-4o\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63dd7b76-2594-4c39-8e81-33e3c985df38",
   "metadata": {},
   "source": [
    "除 OpenAI 外，LlamaIndex 已集成多个大语言模型，包括云服务 API 和本地部署 API，详见官方文档：[Available LLM integrations](https://docs.llamaindex.ai/en/stable/module_guides/models/llms/modules/)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b110fc0b-ec6c-445b-a3c2-3ddef41d9589",
   "metadata": {},
   "source": [
    "### 7.3、Embedding 模型\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c5be1257-eb1a-4e2d-bad9-2cd002841093",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.embeddings.openai import OpenAIEmbedding\n",
    "from llama_index.core import Settings\n",
    "\n",
    "# 全局设定\n",
    "Settings.embed_model = OpenAIEmbedding(model=\"text-embedding-3-small\", dimensions=512)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb025ce6-c065-4cf2-9e9d-5a01f04c26ee",
   "metadata": {},
   "source": [
    "LlamaIndex 同样集成了多种 Embedding 模型，包括云服务 API 和开源模型（HuggingFace）等，详见[官方文档](https://docs.llamaindex.ai/en/stable/module_guides/models/embeddings/)。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be91aa7d-6f3a-4387-859c-c7357d7c5d15",
   "metadata": {},
   "source": [
    "## 8、基于 LlamaIndex 实现一个功能较完整的 RAG 系统\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9a1b844-605e-4be2-9cae-2e9e56e46b40",
   "metadata": {},
   "source": [
    "功能要求：\n",
    "\n",
    "- 加载指定目录的文件\n",
    "- 支持 RAG-Fusion\n",
    "- 使用 Qdrant 向量数据库，并持久化到本地\n",
    "- 支持检索后排序\n",
    "- 支持多轮对话\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "90348baf-3a5e-4cc4-968d-e90f9d315495",
   "metadata": {},
   "outputs": [],
   "source": [
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.models import VectorParams, Distance\n",
    "\n",
    "EMBEDDING_DIM = 512\n",
    "COLLECTION_NAME = \"full_demo\"\n",
    "PATH = \"./qdrant_db\"\n",
    "\n",
    "client = QdrantClient(path=PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "cf485f1b-7da4-412e-a121-a7e6b5f3ece9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import VectorStoreIndex, KeywordTableIndex, SimpleDirectoryReader\n",
    "from llama_index.vector_stores.qdrant import QdrantVectorStore\n",
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "from llama_index.core.ingestion import IngestionPipeline\n",
    "from llama_index.core import Settings\n",
    "from llama_index.core import StorageContext\n",
    "from llama_index.core.postprocessor import LLMRerank\n",
    "from llama_index.core.retrievers import QueryFusionRetriever\n",
    "from llama_index.core.query_engine import RetrieverQueryEngine\n",
    "from llama_index.core.chat_engine import CondenseQuestionChatEngine\n",
    "from llama_index.llms.openai import OpenAI\n",
    "from llama_index.embeddings.openai import OpenAIEmbedding\n",
    "import time\n",
    "\n",
    "# 1. 指定全局llm与embedding模型\n",
    "Settings.llm = OpenAI(temperature=0, model=\"gpt-4o\")\n",
    "Settings.embed_model = OpenAIEmbedding(model=\"text-embedding-3-small\", dimensions=EMBEDDING_DIM)\n",
    "# 2. 指定全局文档处理的 Ingestion Pipeline\n",
    "Settings.transformations = [SentenceSplitter(chunk_size=500, chunk_overlap=250)]\n",
    "\n",
    "# 3. 加载本地文档\n",
    "documents = SimpleDirectoryReader(\"./data\").load_data()\n",
    "\n",
    "if client.collection_exists(collection_name=COLLECTION_NAME):\n",
    "    client.delete_collection(collection_name=COLLECTION_NAME)\n",
    "\n",
    "# 4. 创建 collection\n",
    "client.create_collection(\n",
    "    collection_name=COLLECTION_NAME,\n",
    "    vectors_config=VectorParams(size=EMBEDDING_DIM, distance=Distance.COSINE)\n",
    ")\n",
    "\n",
    "# 5. 创建 Vector Store\n",
    "vector_store = QdrantVectorStore(client=client, collection_name=COLLECTION_NAME)\n",
    "\n",
    "# 6. 指定 Vector Store 的 Storage 用于 index\n",
    "storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
    "index = VectorStoreIndex.from_documents(\n",
    "    documents, storage_context=storage_context\n",
    ")\n",
    "\n",
    "# 7. 定义检索后排序模型\n",
    "reranker = LLMRerank(top_n=2)\n",
    "\n",
    "# 8. 定义 RAG Fusion 检索器\n",
    "fusion_retriever = QueryFusionRetriever(\n",
    "    [index.as_retriever()],\n",
    "    similarity_top_k=5, # 检索召回 top k 结果\n",
    "    num_queries=3,  # 生成 query 数\n",
    "    use_async=False,\n",
    "    # query_gen_prompt=\"...\",  # 可以自定义 query 生成的 prompt 模板\n",
    ")\n",
    "\n",
    "# 9. 构建单轮 query engine\n",
    "query_engine = RetrieverQueryEngine.from_args(\n",
    "    fusion_retriever,\n",
    "    node_postprocessors=[reranker]\n",
    ")\n",
    "\n",
    "# 10. 对话引擎\n",
    "chat_engine = CondenseQuestionChatEngine.from_defaults(\n",
    "    query_engine=query_engine, \n",
    "    # condense_question_prompt=... # 可以自定义 chat message prompt 模板\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "0124ef05-eb4b-417b-aa56-7128460162dd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "User: deepseek v3有多少参数\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AI: DeepSeek-V3总共有6710亿个参数。\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "User: 激活多少\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AI: DeepSeek-V3的参数中有37B是被激活的。\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "User: 它数学能力怎么样\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AI: DeepSeek-V3在数学相关基准测试中表现出色，在所有非长链推理的开源和闭源模型中达到了最先进的性能。它甚至在某些基准测试上超越了o1-preview，展示了其强大的数学推理能力。\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "User: \n"
     ]
    }
   ],
   "source": [
    "while True:\n",
    "    question=input(\"User:\")\n",
    "    if question.strip() == \"\":\n",
    "        break\n",
    "    response = chat_engine.chat(question)\n",
    "    print(f\"AI: {response}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "741c79d5-5990-42ec-9c89-3e2eaa9b31ff",
   "metadata": {},
   "source": [
    "## 9、工作流（Workflow）简介"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "071629c4-1cbd-456c-bc55-ab38e4bedc7c",
   "metadata": {},
   "source": [
    "工作流顾名思义是对一些列工作步骤的抽象。\n",
    "\n",
    "LlamaIndex 的工作流是事件（`event`）驱动的：\n",
    "\n",
    "- 工作流由 `step` 组成\n",
    "- 每个 `step` 处理特定的事件\n",
    "- `step` 也会产生新的事件（交由后继的 `step` 进行处理）\n",
    "- 直到产生 `StopEvent` 整个工作流结束"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f99b8edc-2764-4f07-ad2e-1e8c549b8157",
   "metadata": {},
   "source": [
    "举个具体的例子：用自然语言查询数据库，数据库中包含多张表\n",
    "\n",
    "工作流：\n",
    "\n",
    "<img src=\"workflow.png\" alt=\"工作流\" width=\"600\"/>\n",
    "\n",
    "分步说明：\n",
    "\n",
    "1. 用户输入自然语言查询\n",
    "2. 系统先去检索跟查询相关的表\n",
    "3. 根据表的 Schema 让大模型生成 SQL\n",
    "4. 用生成的 SQL 查询数据库\n",
    "5. 根据查询结果，调用大模型生成自然语言回复"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2be28368-98b7-4dcf-8cca-a05f443cb66b",
   "metadata": {},
   "source": [
    "### 9.1 数据准备"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb7487c6-f529-4987-b069-1e96d20303ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 下载 WikiTableQuestions\n",
    "# WikiTableQuestions 是一个为表格问答设计的数据集。其中包含 2,108 个从维基百科提取的 HTML 表格\n",
    "\n",
    "# !wget \"https://github.com/ppasupat/WikiTableQuestions/releases/download/v1.0.2/WikiTableQuestions-1.0.2-compact.zip\" -O wiki_data.zip\n",
    "# !unzip wiki_data.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b7dfdbf-c064-4575-b59e-6562e86aff21",
   "metadata": {},
   "source": [
    "1. 遍历目录加载表格"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "dd4661dd-856a-4476-b771-1164fecadf47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing file: WikiTableQuestions/csv/200-csv/0.csv\n",
      "processing file: WikiTableQuestions/csv/200-csv/1.csv\n",
      "processing file: WikiTableQuestions/csv/200-csv/10.csv\n",
      "processing file: WikiTableQuestions/csv/200-csv/11.csv\n",
      "processing file: WikiTableQuestions/csv/200-csv/12.csv\n",
      "processing file: WikiTableQuestions/csv/200-csv/14.csv\n",
      "processing file: WikiTableQuestions/csv/200-csv/15.csv\n",
      "Error parsing WikiTableQuestions/csv/200-csv/15.csv: Error tokenizing data. C error: Expected 4 fields in line 16, saw 5\n",
      "\n",
      "processing file: WikiTableQuestions/csv/200-csv/17.csv\n",
      "Error parsing WikiTableQuestions/csv/200-csv/17.csv: Error tokenizing data. C error: Expected 6 fields in line 5, saw 7\n",
      "\n",
      "processing file: WikiTableQuestions/csv/200-csv/18.csv\n",
      "processing file: WikiTableQuestions/csv/200-csv/20.csv\n",
      "processing file: WikiTableQuestions/csv/200-csv/22.csv\n",
      "processing file: WikiTableQuestions/csv/200-csv/24.csv\n",
      "processing file: WikiTableQuestions/csv/200-csv/25.csv\n",
      "processing file: WikiTableQuestions/csv/200-csv/26.csv\n",
      "processing file: WikiTableQuestions/csv/200-csv/28.csv\n",
      "processing file: WikiTableQuestions/csv/200-csv/29.csv\n",
      "processing file: WikiTableQuestions/csv/200-csv/3.csv\n",
      "processing file: WikiTableQuestions/csv/200-csv/30.csv\n",
      "processing file: WikiTableQuestions/csv/200-csv/31.csv\n",
      "processing file: WikiTableQuestions/csv/200-csv/32.csv\n",
      "processing file: WikiTableQuestions/csv/200-csv/33.csv\n",
      "processing file: WikiTableQuestions/csv/200-csv/34.csv\n",
      "Error parsing WikiTableQuestions/csv/200-csv/34.csv: Error tokenizing data. C error: Expected 4 fields in line 6, saw 13\n",
      "\n",
      "processing file: WikiTableQuestions/csv/200-csv/35.csv\n",
      "processing file: WikiTableQuestions/csv/200-csv/36.csv\n",
      "processing file: WikiTableQuestions/csv/200-csv/37.csv\n",
      "processing file: WikiTableQuestions/csv/200-csv/38.csv\n",
      "processing file: WikiTableQuestions/csv/200-csv/4.csv\n",
      "processing file: WikiTableQuestions/csv/200-csv/41.csv\n",
      "processing file: WikiTableQuestions/csv/200-csv/42.csv\n",
      "processing file: WikiTableQuestions/csv/200-csv/44.csv\n",
      "processing file: WikiTableQuestions/csv/200-csv/45.csv\n",
      "processing file: WikiTableQuestions/csv/200-csv/46.csv\n",
      "processing file: WikiTableQuestions/csv/200-csv/47.csv\n",
      "processing file: WikiTableQuestions/csv/200-csv/48.csv\n",
      "processing file: WikiTableQuestions/csv/200-csv/7.csv\n",
      "processing file: WikiTableQuestions/csv/200-csv/8.csv\n",
      "processing file: WikiTableQuestions/csv/200-csv/9.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "data_dir = Path(\"./WikiTableQuestions/csv/200-csv\")\n",
    "csv_files = sorted([f for f in data_dir.glob(\"*.csv\")])\n",
    "dfs = []\n",
    "for csv_file in csv_files:\n",
    "    print(f\"processing file: {csv_file}\")\n",
    "    try:\n",
    "        df = pd.read_csv(csv_file)\n",
    "        dfs.append(df)\n",
    "    except Exception as e:\n",
    "        print(f\"Error parsing {csv_file}: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f7e7eef-d81a-4dd2-bcfd-1fb5dd89603a",
   "metadata": {},
   "source": [
    "2. 为每个表生成一段文字表述（用于检索），保存在 `WikiTableQuestions_TableInfo` 目录"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "25c1a118-6f5e-470a-bb02-54fd5f1497ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.prompts import ChatPromptTemplate\n",
    "from llama_index.core.bridge.pydantic import BaseModel, Field\n",
    "from llama_index.llms.openai import OpenAI\n",
    "from llama_index.core.llms import ChatMessage\n",
    "\n",
    "\n",
    "class TableInfo(BaseModel):\n",
    "    \"\"\"Information regarding a structured table.\"\"\"\n",
    "\n",
    "    table_name: str = Field(\n",
    "        ..., description=\"table name (must be underscores and NO spaces)\"\n",
    "    )\n",
    "    table_summary: str = Field(\n",
    "        ..., description=\"short, concise summary/caption of the table\"\n",
    "    )\n",
    "\n",
    "\n",
    "prompt_str = \"\"\"\\\n",
    "Give me a summary of the table with the following JSON format.\n",
    "\n",
    "- The table name must be unique to the table and describe it while being concise. \n",
    "- Do NOT output a generic table name (e.g. table, my_table).\n",
    "\n",
    "Do NOT make the table name one of the following: {exclude_table_name_list}\n",
    "\n",
    "Table:\n",
    "{table_str}\n",
    "\n",
    "Summary: \"\"\"\n",
    "prompt_tmpl = ChatPromptTemplate(\n",
    "    message_templates=[ChatMessage.from_str(prompt_str, role=\"user\")]\n",
    ")\n",
    "\n",
    "llm = OpenAI(model=\"gpt-4o-mini\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "9aaded9f-4af5-4ab9-b994-374ac420bf58",
   "metadata": {},
   "outputs": [],
   "source": [
    "tableinfo_dir = \"WikiTableQuestions_TableInfo\"\n",
    "!mkdir {tableinfo_dir}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "8e4bf4ce-f969-45a2-b705-adc23a0c1cf1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "\n",
    "def _get_tableinfo_with_index(idx: int) -> str:\n",
    "    results_gen = Path(tableinfo_dir).glob(f\"{idx}_*\")\n",
    "    results_list = list(results_gen)\n",
    "    if len(results_list) == 0:\n",
    "        return None\n",
    "    elif len(results_list) == 1:\n",
    "        path = results_list[0]\n",
    "        with open(path, 'r') as file:\n",
    "            data = json.load(file) \n",
    "            return TableInfo.model_validate(data)\n",
    "    else:\n",
    "        raise ValueError(\n",
    "            f\"More than one file matching index: {list(results_gen)}\"\n",
    "        )\n",
    "\n",
    "\n",
    "table_names = set()\n",
    "table_infos = []\n",
    "for idx, df in enumerate(dfs):\n",
    "    table_info = _get_tableinfo_with_index(idx)\n",
    "    if table_info:\n",
    "        table_infos.append(table_info)\n",
    "    else:\n",
    "        while True:\n",
    "            df_str = df.head(10).to_csv()\n",
    "            table_info = llm.structured_predict(\n",
    "                TableInfo,\n",
    "                prompt_tmpl,\n",
    "                table_str=df_str,\n",
    "                exclude_table_name_list=str(list(table_names)),\n",
    "            )\n",
    "            table_name = table_info.table_name\n",
    "            print(f\"Processed table: {table_name}\")\n",
    "            if table_name not in table_names:\n",
    "                table_names.add(table_name)\n",
    "                break\n",
    "            else:\n",
    "                # try again\n",
    "                print(f\"Table name {table_name} already exists, trying again.\")\n",
    "                pass\n",
    "\n",
    "        out_file = f\"{tableinfo_dir}/{idx}_{table_name}.json\"\n",
    "        json.dump(table_info.dict(), open(out_file, \"w\"))\n",
    "    table_infos.append(table_info)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73345b81-07ff-49b0-9ccd-ed553c195f93",
   "metadata": {},
   "source": [
    "3. 将上述表格存入 SQLite 数据库"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "09b6fd32-3255-4f2d-9962-eb9449f56565",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating table: progressive_rock_album_chart_positions\n",
      "Creating table: filmography_of_diane\n",
      "Creating table: annual_fatalities_and_accidents_statistics\n",
      "Creating table: academy_awards_1972_results\n",
      "Creating table: theatrical_award_nominations_and_wins\n",
      "Creating table: bad_boy_artists_album_release_summary\n",
      "Creating table: south_dakota_radio_stations\n",
      "Creating table: missing_persons_case_summary_1982\n",
      "Creating table: chart_performance_of_singles\n",
      "Creating table: kodachrome_film_types_and_dates\n",
      "Creating table: bbc_radio_service_costs_2012_2013\n",
      "Creating table: french_airports_usage_summary\n",
      "Creating table: voter_registration_summary_by_party\n",
      "Creating table: norwegian_club_performance_statistics\n",
      "Creating table: triple_crown_winners_history\n",
      "Creating table: grammy_awards_summary_for_artist\n",
      "Creating table: boxing_fight_results_history\n",
      "Creating table: historical_sports_team_performance\n",
      "Creating table: yamato_population_density_summary\n",
      "Creating table: voter_registration_summary_by_party_distribution\n",
      "Creating table: best_actress_award_nominations_and_wins\n",
      "Creating table: uk_ministerial_positions_and_titles_history\n",
      "Creating table: municipality_merger_summary\n",
      "Creating table: euro_2020_group_stage_results\n",
      "Creating table: binary_encoding_probabilities\n",
      "Creating table: monthly_climate_statistics\n",
      "Creating table: italian_government_term_history\n",
      "Creating table: new_mexico_government_officials\n",
      "Creating table: monthly_climate_statistics_summary\n",
      "Creating table: historical_rainfall_experiment_drops\n",
      "Creating table: monthly_weather_statistics\n",
      "Creating table: multilingual_greetings_and_phrases\n",
      "Creating table: ohio_private_schools_summary\n",
      "Creating table: cancer_related_genetic_factors\n"
     ]
    }
   ],
   "source": [
    "# put data into sqlite db\n",
    "from sqlalchemy import (\n",
    "    create_engine,\n",
    "    MetaData,\n",
    "    Table,\n",
    "    Column,\n",
    "    String,\n",
    "    Integer,\n",
    ")\n",
    "import re\n",
    "\n",
    "\n",
    "# Function to create a sanitized column name\n",
    "def sanitize_column_name(col_name):\n",
    "    # Remove special characters and replace spaces with underscores\n",
    "    return re.sub(r\"\\W+\", \"_\", col_name)\n",
    "\n",
    "\n",
    "# Function to create a table from a DataFrame using SQLAlchemy\n",
    "def create_table_from_dataframe(\n",
    "    df: pd.DataFrame, table_name: str, engine, metadata_obj\n",
    "):\n",
    "    # Sanitize column names\n",
    "    sanitized_columns = {col: sanitize_column_name(col) for col in df.columns}\n",
    "    df = df.rename(columns=sanitized_columns)\n",
    "\n",
    "    # Dynamically create columns based on DataFrame columns and data types\n",
    "    columns = [\n",
    "        Column(col, String if dtype == \"object\" else Integer)\n",
    "        for col, dtype in zip(df.columns, df.dtypes)\n",
    "    ]\n",
    "\n",
    "    # Create a table with the defined columns\n",
    "    table = Table(table_name, metadata_obj, *columns)\n",
    "\n",
    "    # Create the table in the database\n",
    "    metadata_obj.create_all(engine)\n",
    "\n",
    "    # Insert data from DataFrame into the table\n",
    "    with engine.connect() as conn:\n",
    "        for _, row in df.iterrows():\n",
    "            insert_stmt = table.insert().values(**row.to_dict())\n",
    "            conn.execute(insert_stmt)\n",
    "        conn.commit()\n",
    "\n",
    "\n",
    "# engine = create_engine(\"sqlite:///:memory:\")\n",
    "engine = create_engine(\"sqlite:///wiki_table_questions.db\")\n",
    "metadata_obj = MetaData()\n",
    "for idx, df in enumerate(dfs):\n",
    "    tableinfo = _get_tableinfo_with_index(idx)\n",
    "    print(f\"Creating table: {tableinfo.table_name}\")\n",
    "    create_table_from_dataframe(df, tableinfo.table_name, engine, metadata_obj)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18b6ab4f-7db7-47ef-ad8a-898b19540d56",
   "metadata": {},
   "source": [
    "### 9.2 构建基础工具"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c2ebdd1-88e4-4e3e-b025-000890ac300e",
   "metadata": {},
   "source": [
    "1. 创建基于表的描述的向量索引\n",
    "\n",
    "- `ObjectIndex` 是一个 LlamaIndex 内置的模块，通过索引 (Index）检索任意 Python 对象\n",
    "- 这里我们使用 `VectorStoreIndex` 也就是向量检索，并通过 `SQLTableNodeMapping` 将文本描述的 `node` 和数据库的表形成映射\n",
    "- 相关文档：https://docs.llamaindex.ai/en/stable/examples/objects/object_index/#the-objectindex-class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "49e12988-cc89-4618-b3f6-a6a038c3c8dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.objects import (\n",
    "    SQLTableNodeMapping,\n",
    "    ObjectIndex,\n",
    "    SQLTableSchema,\n",
    ")\n",
    "from llama_index.core import SQLDatabase, VectorStoreIndex\n",
    "\n",
    "sql_database = SQLDatabase(engine)\n",
    "\n",
    "table_node_mapping = SQLTableNodeMapping(sql_database)\n",
    "table_schema_objs = [\n",
    "    SQLTableSchema(table_name=t.table_name, context_str=t.table_summary)\n",
    "    for t in table_infos\n",
    "]  # add a SQLTableSchema for each table\n",
    "\n",
    "obj_index = ObjectIndex.from_objects(\n",
    "    table_schema_objs,\n",
    "    table_node_mapping,\n",
    "    VectorStoreIndex,\n",
    ")\n",
    "obj_retriever = obj_index.as_retriever(similarity_top_k=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef92b78b-0530-468d-90ec-309d1614d390",
   "metadata": {},
   "source": [
    "2. 创建 SQL 查询器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "948b8bee-2af6-4487-8b27-a2b2a5d7420b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.retrievers import SQLRetriever\n",
    "from typing import List\n",
    "\n",
    "sql_retriever = SQLRetriever(sql_database)\n",
    "\n",
    "\n",
    "def get_table_context_str(table_schema_objs: List[SQLTableSchema]):\n",
    "    \"\"\"Get table context string.\"\"\"\n",
    "    context_strs = []\n",
    "    for table_schema_obj in table_schema_objs:\n",
    "        table_info = sql_database.get_single_table_info(\n",
    "            table_schema_obj.table_name\n",
    "        )\n",
    "        if table_schema_obj.context_str:\n",
    "            table_opt_context = \" The table description is: \"\n",
    "            table_opt_context += table_schema_obj.context_str\n",
    "            table_info += table_opt_context\n",
    "\n",
    "        context_strs.append(table_info)\n",
    "    return \"\\n\\n\".join(context_strs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39d5bac2-4c10-4411-bcd1-d825727df41f",
   "metadata": {},
   "source": [
    "3. 创建 Text2SQL 的提示词（系统默认模板），和输出结果解析器（从生成的文本中抽取SQL）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "da0c9ea7-5aa5-4069-8baf-9ca62c9e278a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Given an input question, first create a syntactically correct {dialect} query to run, then look at the results of the query and return the answer. You can order the results by a relevant column to return the most interesting examples in the database.\n",
      "\n",
      "Never query for all the columns from a specific table, only ask for a few relevant columns given the question.\n",
      "\n",
      "Pay attention to use only the column names that you can see in the schema description. Be careful to not query for columns that do not exist. Pay attention to which column is in which table. Also, qualify column names with the table name when needed. You are required to use the following format, each taking one line:\n",
      "\n",
      "Question: Question here\n",
      "SQLQuery: SQL Query to run\n",
      "SQLResult: Result of the SQLQuery\n",
      "Answer: Final answer here\n",
      "\n",
      "Only use tables listed below.\n",
      "{schema}\n",
      "\n",
      "Question: {query_str}\n",
      "SQLQuery: \n"
     ]
    }
   ],
   "source": [
    "from llama_index.core.prompts.default_prompts import DEFAULT_TEXT_TO_SQL_PROMPT\n",
    "from llama_index.core import PromptTemplate\n",
    "from llama_index.core.llms import ChatResponse\n",
    "\n",
    "def parse_response_to_sql(chat_response: ChatResponse) -> str:\n",
    "    \"\"\"Parse response to SQL.\"\"\"\n",
    "    response = chat_response.message.content\n",
    "    sql_query_start = response.find(\"SQLQuery:\")\n",
    "    if sql_query_start != -1:\n",
    "        response = response[sql_query_start:]\n",
    "        # TODO: move to removeprefix after Python 3.9+\n",
    "        if response.startswith(\"SQLQuery:\"):\n",
    "            response = response[len(\"SQLQuery:\") :]\n",
    "    sql_result_start = response.find(\"SQLResult:\")\n",
    "    if sql_result_start != -1:\n",
    "        response = response[:sql_result_start]\n",
    "    return response.strip().strip(\"```\").strip()\n",
    "\n",
    "\n",
    "text2sql_prompt = DEFAULT_TEXT_TO_SQL_PROMPT.partial_format(\n",
    "    dialect=engine.dialect.name\n",
    ")\n",
    "print(text2sql_prompt.template)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "691c7cf0-1217-4d16-8196-2937411bbbed",
   "metadata": {},
   "source": [
    "4. 创建自然语言回复生成模板"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "f1fbf506-9c46-44bd-9148-0712e4e835a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "response_synthesis_prompt_str = (\n",
    "    \"Given an input question, synthesize a response from the query results.\\n\"\n",
    "    \"Query: {query_str}\\n\"\n",
    "    \"SQL: {sql_query}\\n\"\n",
    "    \"SQL Response: {context_str}\\n\"\n",
    "    \"Response: \"\n",
    ")\n",
    "response_synthesis_prompt = PromptTemplate(\n",
    "    response_synthesis_prompt_str,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "9f4b9121-7da6-496c-8506-48fee7d4ac5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = OpenAI(model=\"gpt-4o-mini\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "258cc672-bed9-45b6-a9cd-80d162f96fd1",
   "metadata": {},
   "source": [
    "### 9.3 定义工作流"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "8ac5f9db-6dc7-4828-bd2b-1ff7d1e16abd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.workflow import (\n",
    "    Workflow,\n",
    "    StartEvent,\n",
    "    StopEvent,\n",
    "    step,\n",
    "    Context,\n",
    "    Event,\n",
    ")\n",
    "\n",
    "# 事件：找到了数据库中相关的表\n",
    "class TableRetrieveEvent(Event):\n",
    "    \"\"\"Result of running table retrieval.\"\"\"\n",
    "\n",
    "    table_context_str: str\n",
    "    query: str\n",
    "\n",
    "# 事件：文本转为了 SQL\n",
    "class TextToSQLEvent(Event):\n",
    "    \"\"\"Text-to-SQL event.\"\"\"\n",
    "\n",
    "    sql: str\n",
    "    query: str\n",
    "\n",
    "\n",
    "class TextToSQLWorkflow1(Workflow):\n",
    "    \"\"\"Text-to-SQL Workflow that does query-time table retrieval.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        obj_retriever,\n",
    "        text2sql_prompt,\n",
    "        sql_retriever,\n",
    "        response_synthesis_prompt,\n",
    "        llm,\n",
    "        *args,\n",
    "        **kwargs\n",
    "    ) -> None:\n",
    "        \"\"\"Init params.\"\"\"\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.obj_retriever = obj_retriever\n",
    "        self.text2sql_prompt = text2sql_prompt\n",
    "        self.sql_retriever = sql_retriever\n",
    "        self.response_synthesis_prompt = response_synthesis_prompt\n",
    "        self.llm = llm\n",
    "\n",
    "    @step\n",
    "    def retrieve_tables(\n",
    "        self, ctx: Context, ev: StartEvent\n",
    "    ) -> TableRetrieveEvent:\n",
    "        \"\"\"Retrieve tables.\"\"\"\n",
    "        table_schema_objs = self.obj_retriever.retrieve(ev.query)\n",
    "        table_context_str = get_table_context_str(table_schema_objs)\n",
    "        print(\"====\\n\"+table_context_str+\"\\n====\")\n",
    "        return TableRetrieveEvent(\n",
    "            table_context_str=table_context_str, query=ev.query\n",
    "        )\n",
    "\n",
    "    @step\n",
    "    def generate_sql(\n",
    "        self, ctx: Context, ev: TableRetrieveEvent\n",
    "    ) -> TextToSQLEvent:\n",
    "        \"\"\"Generate SQL statement.\"\"\"\n",
    "        fmt_messages = self.text2sql_prompt.format_messages(\n",
    "            query_str=ev.query, schema=ev.table_context_str\n",
    "        )\n",
    "        chat_response = self.llm.chat(fmt_messages)\n",
    "        sql = parse_response_to_sql(chat_response)\n",
    "        print(\"====\\n\"+sql+\"\\n====\")\n",
    "        return TextToSQLEvent(sql=sql, query=ev.query)\n",
    "\n",
    "    @step\n",
    "    def generate_response(self, ctx: Context, ev: TextToSQLEvent) -> StopEvent:\n",
    "        \"\"\"Run SQL retrieval and generate response.\"\"\"\n",
    "        retrieved_rows = self.sql_retriever.retrieve(ev.sql)\n",
    "        print(\"====\\n\"+str(retrieved_rows)+\"\\n====\")\n",
    "        fmt_messages = self.response_synthesis_prompt.format_messages(\n",
    "            sql_query=ev.sql,\n",
    "            context_str=str(retrieved_rows),\n",
    "            query_str=ev.query,\n",
    "        )\n",
    "        chat_response = llm.chat(fmt_messages)\n",
    "        return StopEvent(result=chat_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "9b3060a2-20e4-4153-b3f9-6b91ed4d7020",
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow = TextToSQLWorkflow1(\n",
    "    obj_retriever,\n",
    "    text2sql_prompt,\n",
    "    sql_retriever,\n",
    "    response_synthesis_prompt,\n",
    "    llm,\n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "ead701cd-c79d-4b48-8a20-9c2322140712",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running step retrieve_tables\n",
      "====\n",
      "Table 'bad_boy_artists_album_release_summary' has columns: Act (VARCHAR), Year_signed (INTEGER), _Albums_released_under_Bad_Boy (VARCHAR), . The table description is: Summary of artists signed to Bad Boy Records and their album releases.\n",
      "\n",
      "Table 'chart_performance_of_singles' has columns: Year (INTEGER), Single (VARCHAR), Peak_chart_positions_GER (VARCHAR), Peak_chart_positions_IRE (VARCHAR), Peak_chart_positions_UK (VARCHAR), Peak_chart_positions_US (VARCHAR), Peak_chart_positions_US_Main (VARCHAR), Peak_chart_positions_US_Dance (VARCHAR), Certifications_sales_thresholds_ (INTEGER), Album (VARCHAR), . The table description is: A summary of chart positions and certifications for singles released by an artist across various countries.\n",
      "\n",
      "Table 'progressive_rock_album_chart_positions' has columns: Year (INTEGER), Title (VARCHAR), Chart_Positions_UK (VARCHAR), Chart_Positions_US (VARCHAR), Chart_Positions_NL (VARCHAR), Comments (VARCHAR), . The table description is: Chart positions of progressive rock albums in the UK, US, and NL from 1969 to 1981.\n",
      "====\n",
      "Step retrieve_tables produced event TableRetrieveEvent\n",
      "Running step generate_sql\n",
      "====\n",
      "SELECT Year_signed FROM bad_boy_artists_album_release_summary WHERE Act = 'The Notorious B.I.G';\n",
      "====\n",
      "Step generate_sql produced event TextToSQLEvent\n",
      "Running step generate_response\n",
      "====\n",
      "[NodeWithScore(node=TextNode(id_='e064d2c0-9c9c-481f-bddf-08c6cd63cfa0', embedding=None, metadata={'sql_query': \"SELECT Year_signed FROM bad_boy_artists_album_release_summary WHERE Act = 'The Notorious B.I.G';\", 'result': [(1993,), (1993,)], 'col_keys': ['Year_signed']}, excluded_embed_metadata_keys=['sql_query', 'result', 'col_keys'], excluded_llm_metadata_keys=['sql_query', 'result', 'col_keys'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text='[(1993,), (1993,)]', mimetype='text/plain', start_char_idx=None, end_char_idx=None, metadata_seperator='\\n', text_template='{metadata_str}\\n\\n{content}'), score=None)]\n",
      "====\n",
      "Step generate_response produced event StopEvent\n",
      "assistant: The Notorious B.I.G was signed to Bad Boy in 1993.\n"
     ]
    }
   ],
   "source": [
    "response = await workflow.run(\n",
    "    query=\"What was the year that The Notorious B.I.G was signed to Bad Boy?\"\n",
    ")\n",
    "print(str(response))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09b64fac-b8b8-4fad-8ecc-5a9431469706",
   "metadata": {},
   "source": [
    "### 9.4 可视化工作流"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b370ab8-9c87-4baa-92ba-3e729a481857",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install llama-index-utils-workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "933d00bd-a602-4958-bed3-d1b530192895",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'NoneType'>\n",
      "<class 'llama_index.core.workflow.events.StopEvent'>\n",
      "<class '__main__.TextToSQLEvent'>\n",
      "<class '__main__.TableRetrieveEvent'>\n",
      "text_to_sql_table_retrieval.html\n"
     ]
    }
   ],
   "source": [
    "from llama_index.utils.workflow import draw_all_possible_flows\n",
    "\n",
    "draw_all_possible_flows(\n",
    "    TextToSQLWorkflow1, filename=\"text_to_sql_table_retrieval.html\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78fc8317-0cac-458b-9ee5-969e4b46442c",
   "metadata": {},
   "source": [
    "### 9.5 工作流管理框架意义是什么\n",
    "\n",
    "思考以下情况：\n",
    "\n",
    "- `step` 的执行顺序有逻辑分支\n",
    "- `step` 的执行有循环\n",
    "- `step` 的执行可以并行\n",
    "- 一个 `step` 的触发条件依赖前面若干 `step` 的结果，且它们之间可能有循环或者并行\n",
    "\n",
    "<img src=\"workflow2.png\" alt=\"工作流举例\" width=\"800\"/>\n",
    "\n",
    "所以，工作流管理框架的意思是便于将单个事件的处理逻辑和事件之间的执行顺序独立开\n",
    "\n",
    "关于 LlamaIndex 工作流的更详细文档：https://docs.llamaindex.ai/en/stable/examples/workflow/workflows_cookbook/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b98c98c0-6636-4d71-9156-731e7ca28b6f",
   "metadata": {},
   "source": [
    "## LlamaIndex 的更多功能\n",
    "\n",
    "- 智能体（Agent）开发框架：https://docs.llamaindex.ai/en/stable/module_guides/deploying/agents/\n",
    "- RAG 的评测：https://docs.llamaindex.ai/en/stable/module_guides/evaluating/\n",
    "- 过程监控：https://docs.llamaindex.ai/en/stable/module_guides/observability/\n",
    "\n",
    "以上内容涉及较多背景知识，暂时不在本课展开，相关知识会在后面课程中逐一详细讲解。\n",
    "\n",
    "此外，LlamaIndex 针对生产级的 RAG 系统中遇到的各个方面的细节问题，总结了很多高端技巧（[Advanced Topics](https://docs.llamaindex.ai/en/stable/optimizing/production_rag/)），对实战很有参考价值，非常推荐有能力的同学阅读。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87418cf5-44bf-4004-8757-75638b134c96",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8d1c1bc-daa2-4f40-a9af-bcbd1b092320",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "054aab08-e330-4df7-b0ee-40fc181b03e7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2018ae3-94c6-4e6f-8b5e-d1068cd54390",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
